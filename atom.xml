<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>XhinLiang</title>
  
  <subtitle>A place to talk trash</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xhinliang.github.io/"/>
  <updated>2026-02-25T07:09:26.607Z</updated>
  <id>https://xhinliang.github.io/</id>
  
  <author>
    <name>XhinLiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从 LLM 到 Agent（下）：工程、框架与交付实践</title>
    <link href="https://xhinliang.github.io/2026/02/backend/from-llm-to-agent-part2/"/>
    <id>https://xhinliang.github.io/2026/02/backend/from-llm-to-agent-part2/</id>
    <published>2026-02-25T11:39:16.000Z</published>
    <updated>2026-02-25T07:09:26.607Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上篇我们把“Agent 从哪来”讲清楚了：LLM/Chatbot 的结构性局限决定了它“会答”不等于“会做事”，而 Tool Calling / MCP / ReAct / DeepResearch 则让推理如何与证据、行动与产物形成闭环。</p><p>但当你真正开始落地时，会发现 Agent 的难点不在“让模型更聪明”，而在“让系统更可靠”：工具会失败、上下文会爆、成本与时延要控、权限与风险要收口、产物要可追溯、效果要能回归。也正是在这里，Agent 从概念走向工程。</p><p>本篇聚焦“工程与选型”：一方面复用上篇第 8/9 章的 checklist，把状态、持久化、上下文工程、评测与可观测性视作运行时能力；另一方面在此基础上对 LangChain/LangGraph 等框架与多种交付形态（Flow-based、CLI Agent）做对比拆解，给出更可落地的决策视角。</p><p>上篇</p><ul><li>理解 LLM/Chatbot 的结构性局限：为什么“会答”不等于“会做事”</li><li>搞清 Tool Calling / MCP / ReAct / DeepResearch 的演化脉络：推理如何与证据、行动与产物形成闭环</li></ul><p>本篇</p><ul><li>掌握 Agent 工程骨架：状态、持久化、上下文工程、评测与可观测性</li><li>能做框架与交付形态选型：LangChain/LangGraph、高层框架、Flow-based、CLI Agent</li></ul><p>阅读指引：如果只关心框架/形态对比，可以直接从第 10 章（本篇）开始；如果希望从概念开始理解，建议先回看上篇第 8/9 章的工程骨架，再来对照本篇的框架能力。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_1.mkdc68kf.webp" alt></p><p>本文 @coauthor: ChatGPT DeepResearch, Codex, Manus, AnyGen, Nano Banana </p><h2 id="10-Elemental-Agent-Frameworks：LangChain-LangGraph"><a href="#10-Elemental-Agent-Frameworks：LangChain-LangGraph" class="headerlink" title="10. Elemental Agent Frameworks：LangChain / LangGraph"></a>10. Elemental Agent Frameworks：LangChain / LangGraph</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_37.8z70lko3on.webp" alt></p><p>第 8 章讨论了把 Agent loop 变成可交付工程系统的三块骨架：<strong>状态、持久化、上下文工程</strong>；第 9 章则回答“跑起来之后怎么变好、怎么排障、怎么上线止血”：<strong>评测与可观测性</strong>。把这些要求写成 checklist 后，会发现真正需要的是一套能把“状态演进 + 工具调用 + 轨迹数据”组织成确定性系统的框架原语。</p><p>这一节聚焦 LangChain 与 LangGraph：它们更接近“基础积木层”（elemental frameworks），常作为高层 Agent 框架/产品的底座（下一节会展开）。可以用一句话区分：</p><ul><li><strong>LangChain</strong>：偏组件与集成层——解决“每一步怎么做、怎么接模型/工具/解析器、怎么挂 tracing/evals”。</li><li><strong>LangGraph</strong>：偏运行时与控制层——解决“多步怎么跑、状态如何演进、如何中断/恢复/回放、如何做人类介入”。</li></ul><p>典型组合方式是：用 LangChain 写节点内部逻辑（RAG、tool calling、输出解析），用 LangGraph 把节点组织成显式状态图，并接入 checkpointer 与可观测/评测闭环。</p><h3 id="10-1-LangChain：组件抽象层（可组合、可集成）"><a href="#10-1-LangChain：组件抽象层（可组合、可集成）" class="headerlink" title="10.1 LangChain：组件抽象层（可组合、可集成）"></a>10.1 LangChain：组件抽象层（可组合、可集成）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_38.51en4wd31h.webp" alt></p><p>LangChain 的核心价值不是“提供某一种 Agent”，而是把 LLM 应用里最常见的部件抽象成可替换的积木，并提供一致的组合与执行协议，从而在多模型、多工具、多数据源条件下保持链路的<strong>可组合、可测试、可观测</strong>。</p><p>从工程视角，LangChain 可以理解为两层：</p><ul><li><strong>原语层（Primitives）</strong>：model / prompt / output parser / tool / retriever / memory 等部件的标准接口；</li><li><strong>组合层（Composition）</strong>：以 <code>Runnable</code>（LCEL）为中心，把“串行、并行、分支、流式输出、批处理执行”做成一等能力。</li></ul><p>示意图（组件抽象 + 组合执行）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">                         +-----------------------------+</span><br><span class="line">                         |  config / tags / metadata   |</span><br><span class="line">                         |  callbacks / tracing / eval  |</span><br><span class="line">                         +--------------+--------------+</span><br><span class="line">                                        |</span><br><span class="line">input ----------------------------------v---------------------------+</span><br><span class="line">                                                                      |</span><br><span class="line">                     +----------------- Runnable (LCEL) -------------+-----&gt; output</span><br><span class="line">                     |                    (composition layer)        |</span><br><span class="line">                     |</span><br><span class="line">                     +--&gt; [PromptTemplate] -&gt; [LLM] -&gt; [OutputParser] --------+</span><br><span class="line">                     |                                                       |</span><br><span class="line">                     +--&gt; [Router/Branch] -----------------------------------+</span><br><span class="line">                              |                          |</span><br><span class="line">                              |                          +--&gt; (Tool path) -&gt; [Tools...] -&gt; [LLM] -&gt; ...</span><br><span class="line">                              |</span><br><span class="line">                              +--&gt; (RAG path)  -&gt; [Retriever] -&gt; [Context] -&gt; [LLM] -&gt; ...</span><br></pre></td></tr></table></figure><p>工程关键点：</p><ul><li><strong>统一执行协议（Runnables）</strong><ul><li>同一链路可用 <code>invoke</code>（单次）、<code>batch</code>（离线批量）、<code>stream</code>（流式）等形态运行，便于把线上请求与离线评测复用为同一套实现；</li><li>运行期 config 可携带 <code>tags/metadata</code>、超时、并发度、回调等参数，使观测与治理从“外部包裹”变成“随链路流动”的上下文。</li></ul></li><li><strong>提示词与结构化输出</strong><ul><li>Prompt templates 把“静态模板 + 变量填充 + system/user 分层”显式化，降低 prompt 漂移；</li><li>Output parser/结构化输出把“模型输出”收敛到 schema，使结果可直接进入后续步骤或写入状态，减少脆弱的正则解析。</li></ul></li><li><strong>工具与集成（Tooling）</strong><ul><li>工具以 schema 描述输入输出，与模型的 tool calling 对齐；工具集合可封装为 toolkits，形成可移植能力包；</li><li>Router/选择器用于把输入分流到不同链路（不同模型、不同检索策略、不同工具集合），便于实现路由、降级与 A/B。</li></ul></li><li><strong>RAG 生态</strong><ul><li>文档加载、切分、向量库、retriever 与 rerank 形成可插拔管线，适合把“上下文工程”落为可运营组件；</li><li>可与第 8 章的 Working set 思路配合：长内容外置，链路只保留摘要与引用，并按需检索回填。</li></ul></li><li><strong>可观测与评测接口</strong><ul><li>Callback/Tracing 把 LLM 生成、tool call、retrieval 等事件变成统一轨迹，可对接 LangSmith 或自建 tracing；</li><li>便于把第 9 章的 eval 机制落到可回放链路：同一 Runnable 既能服务线上，也能被离线批量回归。</li></ul></li></ul><p>适用场景：</p><ul><li>强集成（多模型/多工具/多数据源）、链路相对可控、希望快速搭建可测试的 RAG/工具调用应用；</li><li>团队协作需要“组件标准化”的边界：工具、检索、解析、模型路由可以并行迭代并统一治理。</li></ul><h3 id="10-2-LangGraph：状态机-图执行运行时（可控、可恢复）"><a href="#10-2-LangGraph：状态机-图执行运行时（可控、可恢复）" class="headerlink" title="10.2 LangGraph：状态机/图执行运行时（可控、可恢复）"></a>10.2 LangGraph：状态机/图执行运行时（可控、可恢复）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_39.4n87e14s6a.webp" alt></p><p>LangGraph 解决的问题是：当链路从“线性流程”演进为“带循环、分支、并行与人类介入的长任务”时，如何让执行具备可控的状态演进与可恢复性。其基本模型是把 Agent 表达为显式状态机（state machine）：节点对共享状态做增量更新，运行时负责调度、合并、持久化与回放。</p><p>示意图（显式状态图 + 可恢复执行）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">+-------+        +---------+        +----------+        +------+</span><br><span class="line">| START | -----&gt; | Planner | -----&gt; | Executor | -----&gt; | END  |</span><br><span class="line">+-------+        +---------+        +----------+        +------+</span><br><span class="line">                        |                |</span><br><span class="line">                        |                +----&gt; [Tools / I-O] ----+</span><br><span class="line">                        |                                         |</span><br><span class="line">                        +----&gt; (Interrupt: approval/input) &lt;------+</span><br><span class="line">                                 ||</span><br><span class="line">                                 || resume(thread_id, state)</span><br><span class="line">                                 \/</span><br><span class="line">                              (continue)</span><br><span class="line"></span><br><span class="line">每个 super-step：节点读 state -&gt; 产出 patch -&gt; reducer 合并 -&gt; 写入 checkpoint(thread)</span><br></pre></td></tr></table></figure><p>核心抽象与执行语义：</p><ul><li><strong>StateGraph（状态图）</strong>：定义状态 schema、节点函数与边（条件路由/分支规则）；节点产出的是“对状态的 patch”，而不是仅依赖自由文本对话推进；</li><li><strong>Channels / Reducers（状态合并）</strong>：同一 super-step 内的并行节点可各自写入更新，通过 reducer 合并为下一轮状态，减少隐式冲突并显式定义“可合并的状态更新路径”；</li><li><strong>Super-step 调度（图运行时）</strong>：满足条件的节点被激活运行，直到到达 <code>END</code> 或无新的状态更新，从而自然支持循环、分支与并行；</li><li><strong>Checkpointer / Threads（持久化与恢复）</strong>：在每个 super-step 持久化状态快照与事件轨迹，形成 thread；可据此实现 resume、replay 与 time-travel debugging，把第 8 章的 durable execution 变为运行时能力；</li><li><strong>Interrupts / Human-in-the-loop</strong>：在关键节点触发 interrupt 暂停执行，将“等待外部输入/审批”纳入图语义；恢复依赖 thread_id 与持久化状态；</li><li><strong>Subgraphs / Parallelism（规模化编排）</strong>：子图用于 context quarantine，把子任务上下文与状态隔离；并行 worker 节点适配 deep research/批量检索/多候选生成，再由聚合节点做对齐与投票。</li></ul><p>示意图（并行节点 + reducer 合并）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">           (same super-step)</span><br><span class="line">        +---------------------+</span><br><span class="line">        |       STATE S       |</span><br><span class="line">        +----------+----------+</span><br><span class="line">                   |</span><br><span class="line">       +-----------+-----------+</span><br><span class="line">       |                       |</span><br><span class="line">+------+------+\        /+------+------+</span><br><span class="line">| worker A    | \      / | worker B    |</span><br><span class="line">| (patch A)   |  \    /  | (patch B)   |</span><br><span class="line">+-------------+   \  /   +-------------+</span><br><span class="line">                   \/</span><br><span class="line">            +--------------+</span><br><span class="line">            |  reducer()   |   merge patches</span><br><span class="line">            +------+-------+</span><br><span class="line">                   |</span><br><span class="line">            +------+------+</span><br><span class="line">            |  STATE S&apos;   |   checkpoint -&gt; thread</span><br><span class="line">            +-------------+</span><br></pre></td></tr></table></figure><p>与 LangChain 的关系（常见组合方式）：</p><ul><li>LangChain 更适合表达“节点内部怎么做”（提示词、RAG、tool calling、解析与路由组件）；</li><li>LangGraph 更适合表达“节点之间怎么跑”（循环、分支、并行、恢复、审批与运行时治理）。</li></ul><p>在组合架构中，LangChain 的 Runnable 常作为 LangGraph 节点实现；LangGraph 提供 checkpointer/thread 体系承载状态、轨迹与恢复路径，并可将 tracing/evals 串入第 9 章的评测与可观测闭环。</p><p>适用场景：</p><ul><li>长任务、多工具编排、需要审批/暂停/恢复、需要回放与审计的生产 Agent；</li><li>需要把“状态更新路径”做成显式合约（schema + reducer），并在此基础上做回归评测与线上监控。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_40.6wr7xipin4.webp" alt></p><h2 id="11-High-level-Agent-Frameworks-DeepAgents-Mastra-CrewAI-Agno"><a href="#11-High-level-Agent-Frameworks-DeepAgents-Mastra-CrewAI-Agno" class="headerlink" title="11. High-level Agent Frameworks: DeepAgents / Mastra / CrewAI / Agno"></a>11. High-level Agent Frameworks: DeepAgents / Mastra / CrewAI / Agno</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_41.466b1zb8d.webp" alt></p><p>上一节的 LangChain/LangGraph 更像“搭积木”：提供组件抽象与可控运行时，但要把它们拼成一个可交付系统，仍需要补齐大量工程胶水——计划与任务分解、工作区与上下文外置、记忆与存储、可观测与评测、权限与发布流程。这些恰好对应第 8/9 章的主线：<strong>把状态/持久化/上下文工程做成运行时能力，并把评测/可观测性做成回归与运维能力</strong>。</p><p>这一节的高层框架（high-level frameworks）更接近“带电池的 Agent 系统骨架”：把常见的 agent harness/编排/运维能力做成默认配置与最佳实践，让项目更快从 demo 走到可交付形态；代价是更强的约定与更明确的框架边界（通常需要按其抽象组织工具、状态与执行流）。</p><p>一个实用的选型问题是：框架主要解决哪类“系统问题”——</p><ul><li><strong>长任务与工作区（coding/研究型任务）</strong>：更像 CLI harness，把文件系统当上下文后端（DeepAgents）。</li><li><strong>工作流 + 记忆 + 可观测一体化</strong>：更像应用框架/平台原语，偏工程落地（Mastra）。</li><li><strong>角色化协作与任务编排</strong>：用“角色/流程”表达多代理分工与控制（CrewAI）。</li><li><strong>私有化运行时与控制平面</strong>：从编排走向服务化交付与部署治理（Agno）。</li></ul><h3 id="11-1-DeepAgents：长任务-harness（规划-文件系统-子代理）"><a href="#11-1-DeepAgents：长任务-harness（规划-文件系统-子代理）" class="headerlink" title="11.1 DeepAgents：长任务 harness（规划 + 文件系统 + 子代理）"></a>11.1 DeepAgents：长任务 harness（规划 + 文件系统 + 子代理）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_42.8vnenuv0yd.webp" alt></p><p>DeepAgents 的定位可以理解为“长任务 Agent 的工作台（harness）”：与其把全部能力压在单次 prompt 上，不如把任务拆成可执行步骤，并把长上下文与中间产物落到工作区，从而让多步任务更接近可交付的工程流程。([GitHub][24]; [LangChain Docs][26])</p><p>示意图（harness：todo + workspace + subagents）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">                      +----------------------+</span><br><span class="line">goal/spec ----------&gt; |   Main Agent Loop    |</span><br><span class="line">                      | (planner + executor) |</span><br><span class="line">                      +----+-----------+-----+</span><br><span class="line">                           |           |</span><br><span class="line">                 write_todos()         | task()</span><br><span class="line">                           |           |</span><br><span class="line">                    +------+----+      v</span><br><span class="line">                    |  TODOs   |  +-----------+</span><br><span class="line">                    | (plan)   |  | Sub-agent |  isolated context</span><br><span class="line">                    +------+----+  +-----+-----+</span><br><span class="line">                           |             |</span><br><span class="line">                           v             v</span><br><span class="line">                      +----+-------------+------------------+</span><br><span class="line">                      |           Workspace (files)          |</span><br><span class="line">                      |  inputs / notes / code / artifacts   |</span><br><span class="line">                      +--------------------------------------+</span><br><span class="line">                           ^</span><br><span class="line">                           |</span><br><span class="line">          ls/read_file/edit_file/write_file (context back-end)</span><br></pre></td></tr></table></figure><p>工程关键点（把第 6/8/9 章的建议“产品化”成默认能力）：</p><ul><li><strong>规划与任务分解（显式进度）</strong><ul><li><code>write_todos</code> 将 plan-and-execute 落为可读的任务清单，使“下一步做什么”不再依赖隐式对话记忆；</li><li>TODOs 既是对齐工具（便于人工 review/调整优先级），也是最小可用的运行时状态（完成/阻塞/风险）。</li></ul></li><li><strong>文件系统作为上下文后端（working set + 外置记忆）</strong><ul><li><code>ls/read_file/write_file/edit_file</code> 将长文本与中间产物外置到 workspace，把上下文工程从“塞 prompt”变为“读写可引用的工件”；</li><li>以文件为载体沉淀证据、草稿与变更路径，天然有利于审计与回放（尤其适合跨文件产出与多轮迭代）。</li></ul></li><li><strong>子代理委派（context quarantine + 分而治之）</strong><ul><li><code>task</code> 用于派生子代理处理子问题（检索、分析、实现某个模块等），主代理只接收“可交付摘要/结论”，降低上下文污染；</li><li>并行子任务在结构上更接近 DeepResearch 的编排方式：独立上下文 → 汇总对齐 → 合并到主计划。</li></ul></li><li><strong>与底座的组合</strong><ul><li>DeepAgents 常与 LangChain/LangGraph 生态的 tracing、存储与工具体系协同；例如结合 LangGraph Store 承载跨线程长期记忆与检索回填（适合偏长期运行的项目知识沉淀）。</li></ul></li></ul><p>官方提供了 CLI 文档与 quickstarts 示例工程，便于上手实践。([LangChain Docs][25]; [GitHub][27])</p><p>适用场景：研究/编码类长任务、需要把“计划—工件—子任务—汇总”串成稳定工作流的团队；尤其适合强调 workspace 产出与可审计过程的交付型 Agent。</p><h3 id="11-2-Mastra：TypeScript-生态的一体化框架（Workflows-Memory-Observability）"><a href="#11-2-Mastra：TypeScript-生态的一体化框架（Workflows-Memory-Observability）" class="headerlink" title="11.2 Mastra：TypeScript 生态的一体化框架（Workflows + Memory + Observability）"></a>11.2 Mastra：TypeScript 生态的一体化框架（Workflows + Memory + Observability）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_43.3k8i358yac.webp" alt></p><p>Mastra 是偏“工程化产品”的 TS 框架，提供 workflows、agents、RAG、integrations、evals 等原语，并强调模型路由与可观测。</p><p>工程关键点：</p><ul><li><strong>Workflows</strong>：用 workflow/step 把多步执行显式化，便于治理重试、分支与编排复杂度。</li><li><strong>Observability</strong>：启用后自动对 agent run、LLM 生成、tool call、workflow step 生成 trace，并携带 AI 语义上下文。</li><li><strong>Memory</strong>：提供 thread/消息存储与语义召回（向量）组合的记忆系统，支持不同存储后端。</li></ul><p>适用场景：Node/TS 团队、需要“工作流 + 记忆 + 可观测 + 多模型/多 provider”一体化工程栈。</p><h3 id="11-3-CrewAI：角色-任务编排（Crew-Process-Flows）"><a href="#11-3-CrewAI：角色-任务编排（Crew-Process-Flows）" class="headerlink" title="11.3 CrewAI：角色/任务编排（Crew / Process / Flows）"></a>11.3 CrewAI：角色/任务编排（Crew / Process / Flows）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_44.lw7zn0osy.webp" alt></p><p>CrewAI 以“角色化 agent + 任务编排”为中心，提供 Crews 与 Flows 两条主线。</p><p>工程关键点：</p><ul><li><strong>Process（Sequential / Hierarchical）</strong>：默认顺序执行；层级模式由 manager 协调分派与校验，并要求配置 <code>manager_llm</code> 或自定义 <code>manager_agent</code>。</li><li><strong>Memory</strong>：提供短期/长期/实体记忆等组件化能力。</li><li><strong>Flows</strong>：面向“事件驱动的工作流”，用于把 crews 与编码任务组合成更可控的自动化流程。</li><li><strong>可运维能力</strong>：如 tool hooks（拦截与控制工具执行）、replay（从最近一次 kickoff 回放特定任务）。</li></ul><p>适用场景：强调组织结构化协作、任务分工明确、希望用“流程/角色”表达编排逻辑的团队。</p><h3 id="11-4-Agno：框架-运行时-控制平面（面向私有化生产部署）"><a href="#11-4-Agno：框架-运行时-控制平面（面向私有化生产部署）" class="headerlink" title="11.4 Agno：框架 + 运行时 + 控制平面（面向私有化生产部署）"></a>11.4 Agno：框架 + 运行时 + 控制平面（面向私有化生产部署）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_45.41yjrqabv5.webp" alt></p><p>Agno 在定位上同时覆盖：multi-agent framework、FastAPI 运行时与控制平面，强调“自有云、自有数据”，并提供 AgentOS 作为生产服务形态。([GitHub][28]; [DeepWiki][29])</p><p>工程关键点（从“编排库”往“交付形态”走）：</p><ul><li><strong>Agents / Teams / Workflows</strong>：面向 multi-agent 与流程编排的核心抽象。</li><li><strong>记忆与知识</strong>：内置记忆与知识管理能力，面向长期运行的生产系统。</li><li><strong>Guardrails / Hooks / MCP</strong>：运行时治理与扩展点（策略、拦截、工具生态）。</li><li><strong>FastAPI runtime + UI 控制平面</strong>：把 agent 作为服务运行与管理，而不仅是本地脚本。</li></ul><p>文档中提供了案例与应用示例，包括 Deep Research Agent。([DeepWiki][30]; [docs.agno.com][31])</p><p>适用场景：更偏“生产系统交付”，关注私有化部署与服务化运行（而不仅是本地编排）。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_46.icm1x7m34.webp" alt></p><hr><h2 id="12-Legacy-Flow-based：Dify-Flowise-n8n"><a href="#12-Legacy-Flow-based：Dify-Flowise-n8n" class="headerlink" title="12. Legacy / Flow-based：Dify / Flowise / n8n"></a>12. Legacy / Flow-based：Dify / Flowise / n8n</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_47.8adr1k0kni.webp" alt></p><p>上一节讨论的高层 Agent 框架更偏“以运行时为中心”：围绕 agent loop 把状态、持久化、上下文工程做成可控系统骨架。而在另一条更早的工程路径上，业界长期存在一类“流程编排/自动化”产品：用节点图（flow）表达业务逻辑，把调用模型、检索知识库、触发外部系统当作图中的节点。随着 LLM 普及，这类平台快速吸收了 RAG、tool calling 与提示词管理能力，形成了 Dify / Flowise / n8n 等常见选择。</p><p>从第 8 章的工程骨架看，Flow-based 平台往往将 Agent 的关键问题“降维”为更易治理的对象：</p><ul><li><strong>状态</strong>：以 workflow variables / inputs / outputs 表达跨节点数据流，状态更新路径由执行图显式约束；</li><li><strong>持久化</strong>：以数据集/知识库、运行记录与版本发布等形态沉淀过程与资产，便于复现与审计；</li><li><strong>上下文工程</strong>：以知识库检索配置、节点级 prompt、以及变量拼装规则控制 working set，而不是把一切塞进对话历史。</li></ul><p>从第 9 章的运维视角看，它们的优势通常不在“推理范式”，而在“可运营性”：提供日志、调用统计、成本与延迟等指标入口，配合可视化编排与连接器生态，让 LLM 能更快嵌入既有业务流程。</p><p>取舍也很明确：Flow-based 平台更偏确定性的流程执行，对“开放式循环 + 细粒度 checkpoint/resume + 人类介入语义 + 可回放轨迹”的支持通常不如专用 agent runtime；但当目标是<strong>快速落地业务流程、降低编排门槛、强化连接器与平台化运维</strong>时，Flow-based 往往是更省工程成本的选项。</p><p>下面分别概览三类代表：Dify 偏平台化应用与 LLMOps；Flowise 偏可视化组装 LangChain 组件；n8n 偏企业自动化引擎，把 LLM 节点嵌入成熟工作流体系。</p><h3 id="12-1-Dify：workflow-RAG-LLMOps-平台化"><a href="#12-1-Dify：workflow-RAG-LLMOps-平台化" class="headerlink" title="12.1 Dify：workflow + RAG + LLMOps 平台化"></a>12.1 Dify：workflow + RAG + LLMOps 平台化</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_48.6iks6nh7rh.webp" alt></p><p>Dify 的定位更接近“LLM 应用的平台化交付层”：以可视化 workflow 为核心，把模型调用、知识库检索、工具/插件接入、运行日志与发布运维整合到一个统一的控制台中，从而降低端到端落地门槛。</p><p>示意图（平台化：构建台 + 运行时 + LLMOps）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+-------------------+        publish/version        +-------------------+</span><br><span class="line">|  Studio / Console | ----------------------------&gt; |   App Runtime     |</span><br><span class="line">|  (workflow UI)    |                               | (API / Web / chat)|</span><br><span class="line">+---------+---------+                               +----+---------+----+</span><br><span class="line">          |                                              |         |</span><br><span class="line">          | configure                                    | run     | observe</span><br><span class="line">          v                                              v         v</span><br><span class="line">+-------------------+                         +----------------+  +-------------------+</span><br><span class="line">| Workflow Graph    | ---- steps/nodes -----&gt; | Node Executor  |  | Logs / Analytics  |</span><br><span class="line">| (variables/branch)|                         | (LLM/RAG/tools)|  | tracing/cost/debug|</span><br><span class="line">+---------+---------+                         +--------+-------+  +-------------------+</span><br><span class="line">          |                                              |</span><br><span class="line">          | RAG                                           | providers/tools</span><br><span class="line">          v                                              v</span><br><span class="line">+-------------------+                         +------------------------+</span><br><span class="line">| Datasets / KB     | &lt;--&gt; embeddings/index   | Models / Plugins / APIs|</span><br><span class="line">+-------------------+                         +------------------------+</span><br></pre></td></tr></table></figure><p>工程要点（workflow + RAG + LLMOps 的平台化拆解）：</p><ul><li><strong>应用形态与发布</strong><ul><li>将“对话应用 / workflow 应用”等形态产品化为可配置实体，并提供发布与运维入口，适合业务侧快速迭代上线；</li><li>通过环境配置、密钥管理、权限与多租户等能力，将模型能力纳入平台治理。</li></ul></li><li><strong>Workflow 编排（可视化执行图）</strong><ul><li>节点化表达：LLM、检索、条件分支、变量读写、外部 API/插件调用等，降低复杂链路的表达成本；</li><li>变量系统支撑“轻量状态”：对话/工作流中可将中间结果写入会话变量（conversation variables），用于跨节点传递与复用。</li></ul></li><li><strong>知识库与 RAG 管线</strong><ul><li>数据集/知识库模块承载文档导入、切分、向量化与检索配置，使 RAG 更接近可运营资产；</li><li>适合与第 8 章的上下文工程结合：长资料在知识库中沉淀，workflow 只按需检索回填到 working set。</li></ul></li><li><strong>LLMOps（观测、排障与运营）</strong><ul><li>平台日志与运行记录使一次调用/一次 workflow run 可被定位与复盘，降低排障门槛；</li><li>成本、延迟与失败等数据更容易沉淀为运营指标与告警策略（呼应第 9 章的可观测与评测闭环）。</li></ul></li><li><strong>插件与连接器生态</strong><ul><li>Plugins 机制用于扩展模型提供方与外部能力，把“连接器生态 + 平台运维”作为默认优势；</li><li>适合将 LLM 节点嵌入既有业务系统（CRM、工单、知识库、审批流等）。</li></ul></li></ul><p>边界与取舍（Flow-based 平台的典型限制）：</p><ul><li>对长任务的“强状态机 + durable execution”支持通常弱于专用 agent runtime（checkpoint 粒度、可回放语义、人类介入编排能力更依赖平台能力边界）；</li><li>一旦需要高度定制的运行时控制、复杂的外部副作用治理或深度代码编排，可能需要下沉到代码框架或自建运行时。</li></ul><p>适用场景：快速落地业务流程、RAG 应用与可运营后台；强调可视化编排、连接器生态与平台运维，对“强状态机 + 可恢复执行”的要求相对较低或可接受平台约束。</p><h3 id="12-2-Flowise：可视化-LangChain-编排（从-Chatflow-到-Agentflow）"><a href="#12-2-Flowise：可视化-LangChain-编排（从-Chatflow-到-Agentflow）" class="headerlink" title="12.2 Flowise：可视化 LangChain 编排（从 Chatflow 到 Agentflow）"></a>12.2 Flowise：可视化 LangChain 编排（从 Chatflow 到 Agentflow）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_49.5q7wox0m18.webp" alt></p><p>Flowise 定位为开源可视化构建器，面向“把链路拖出来”。其文档将能力扩展到 Agentflow 等更复杂编排形态。</p><p>适用场景：原型期/交付期希望用可视化表达链路；或团队需要把 LangChain 组件化能力封装给非核心开发者使用。</p><h3 id="12-3-n8n：企业自动化工作流引擎（把-LLM-节点放入管道）"><a href="#12-3-n8n：企业自动化工作流引擎（把-LLM-节点放入管道）" class="headerlink" title="12.3 n8n：企业自动化工作流引擎（把 LLM 节点放入管道）"></a>12.3 n8n：企业自动化工作流引擎（把 LLM 节点放入管道）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_50.3d5a7pmsuj.webp" alt></p><p>n8n 的优势在于 triggers、节点连接器与成熟的工作流引擎；并可通过 LangChain 节点（如 <code>@n8n/n8n-nodes-langchain</code>）把 LLM/向量库等能力接入自动化管道。</p><p>适用场景：企业流程自动化（工单、消息、CRM、数据同步）中嵌入 LLM；强依赖连接器生态与可视化运维。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_51.4cldkvpk08.webp" alt></p><hr><h2 id="13-CLI-Agent-的实现思路：以-Claude-Code-为主线"><a href="#13-CLI-Agent-的实现思路：以-Claude-Code-为主线" class="headerlink" title="13. CLI Agent 的实现思路：以 Claude Code 为主线"></a>13. CLI Agent 的实现思路：以 Claude Code 为主线</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_52.8z70lko3nr.webp" alt></p><p>前面几章从不同抽象层给出了“Agent 工程化”的路径：LangChain/LangGraph 提供组件与运行时原语，高层框架提供 harness，而 Flow-based 平台把 LLM 能力嵌入业务流程。CLI Agent 则是另一种非常务实的交付形态：把执行环境直接放到终端与工作目录里，使“读文件 / 改代码 / 跑命令 / 写产物”成为默认动作。</p><p>对照第 8 章的骨架，CLI Agent 的关键不在于对话形式，而在于把<strong>工作区、工具与权限</strong>做成显式运行时：状态可追踪、变更可审计、上下文按需读取；对照第 9 章的要求，则需要把一次 run 做成可回放的轨迹，并把测试、成本与失败模式纳入日常开发与运维闭环。</p><p>本节以 Claude Code 为主线，拆解现代“编码型 Agent”的系统结构与落地要点：核心架构、扩展能力、Sub-agent、Skills。</p><h3 id="13-1-Core-Orchestrator-Plan-Tool-Agent-Loop"><a href="#13-1-Core-Orchestrator-Plan-Tool-Agent-Loop" class="headerlink" title="13.1 Core: Orchestrator, Plan, Tool, Agent Loop"></a>13.1 Core: Orchestrator, Plan, Tool, Agent Loop</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_53.1sfj88pldy.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_54.92qmjah6dh.webp" alt></p><p>这一节不讨论“某个模型有多会写代码”，而是回答一个更工程化的问题：<strong>像 Claude Code 这类 CLI 编码型 Agent，为什么能把长任务稳定跑完？</strong></p><p>核心在四个原语：<strong>Orchestrator（编排器）</strong>、<strong>Plan（计划）</strong>、<strong>Tool（工具）</strong>、<strong>Agent Loop（闭环循环）</strong>。它们把第 6/7/8/9 章的抽象（显式计划、状态/持久化、上下文工程、可观测与回放）落到一个可运行的终端执行系统里。</p><p>先给一张“最小闭环”结构图（概念级，具体实现细节留到第 14 章）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">User goal/spec</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">+------------------------+        +----------------------+</span><br><span class="line">|      Orchestrator      |&lt;------&gt;|   Plan (TODOs)       |</span><br><span class="line">| loop driver + LLM      |        | steps / acceptance   |</span><br><span class="line">| budget + safety gates  |        | status / risks       |</span><br><span class="line">+-----------+------------+        +----------------------+</span><br><span class="line">            |</span><br><span class="line">            | chooses tool + inputs</span><br><span class="line">            v</span><br><span class="line">+------------------------+        +----------------------+</span><br><span class="line">|      Tool runner       |-------&gt;|  Workspace / Env     |</span><br><span class="line">| registry + schema      |        |  files/shell/git/MCP |</span><br><span class="line">| permissions + sandbox  |        +----------------------+</span><br><span class="line">+-----------+------------+</span><br><span class="line">            |</span><br><span class="line">            | tool results (stdout/stderr/diff/errors)</span><br><span class="line">            v</span><br><span class="line">     Observations -&gt; state update -&gt; next decision</span><br></pre></td></tr></table></figure><p>下面分别解释这四个原语各自的“职责边界”和“为什么缺一不可”。</p><h4 id="13-1-1-Orchestrator：把“模型调用”变成“可控执行”"><a href="#13-1-1-Orchestrator：把“模型调用”变成“可控执行”" class="headerlink" title="13.1.1 Orchestrator：把“模型调用”变成“可控执行”"></a>13.1.1 Orchestrator：把“模型调用”变成“可控执行”</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_55.54y92m65qc.webp" alt></p><p>在 Chatbot 里，LLM 是主角；在 Agent 里，LLM 更像“策略模块（Policy）”，而 <strong>Orchestrator 才是运行时的控制面</strong>：它驱动循环、维护状态、约束工具副作用，并把一次 run 变成可审计、可恢复的执行过程。</p><p>一个面向编码任务的 Orchestrator，通常至少负责：</p><ul><li><strong>把用户目标转成可执行任务</strong>：明确范围、约束、交付物与验收方式（例如“改哪些文件、跑哪些测试、输出什么补丁/说明”）。</li><li><strong>上下文装配（working set）</strong>：决定本轮该读哪些文件、要不要搜索、要不要看 git diff；避免把全仓库塞进 prompt。</li><li><strong>计划生命周期管理</strong>：生成/更新计划、推进步骤状态、处理阻塞与重规划（与 Plan 强绑定，下一小节展开）。</li><li><strong>工具路由与调度</strong>：选择工具、填充参数、执行、捕获输出，并将结果转成可消费的 Observation。</li><li><strong>安全门禁（safety gates）</strong>：在关键副作用点强制“先计划/先 review 再执行”，并对高风险动作触发审批或策略拦截（与 permissions/sandbox 强绑定）。</li><li><strong>预算与停止条件</strong>：限制最大步数、工具调用次数、wall time、token 成本；当无法推进时及时停下来请求人工信息或确认（呼应第 7 章的 stop condition）。</li><li><strong>失败恢复与回放数据采集</strong>：为每一步记录 <code>step_id/tool_call_id</code>、输入/输出摘要、关键中间产物；出现错误时能定位、重试、回滚或从 checkpoint 继续。</li></ul><p>把 Orchestrator 视为“执行系统的内核”有一个直接好处：<strong>可以替换策略模型（Policy/LLM），但不破坏执行语义</strong>。例如把同一个工具层/权限层/日志层复用在不同模型或不同 prompt 策略上，使迭代更像软件工程而不是“调参玄学”。</p><h4 id="13-1-2-Plan：从“漂亮回答”到“可推进的任务状态”"><a href="#13-1-2-Plan：从“漂亮回答”到“可推进的任务状态”" class="headerlink" title="13.1.2 Plan：从“漂亮回答”到“可推进的任务状态”"></a>13.1.2 Plan：从“漂亮回答”到“可推进的任务状态”</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_56.7i0vjtjywy.webp" alt></p><p>Plan 不只是“开头写一段计划”，而是<strong>贯穿整个 run 的显式任务状态</strong>。它解决两个核心问题：</p><ol><li><strong>长任务漂移（plan drift）</strong>：模型容易在对话中忘记早期约束；计划把约束、里程碑与验收条件固定下来（呼应第 6 章）。</li><li><strong>可控推进</strong>：每一步都能回答“下一步要做什么、为什么要做、做完如何验证”，让 Orchestrator 能做 gating、预算控制和回放。</li></ol><p>在 CLI 编码型 Agent 中，计划的最佳形态往往是“可读、可更新、可对齐”的 TODO 列表（人类也能快速 review）。一个实用的最小字段集可以是：</p><ul><li><code>step</code>: 这一步要达成什么（动词开头）</li><li><code>acceptance</code>: 怎么算做完（可验证条件，尽量落到命令/测试/对比）</li><li><code>tools</code>: 预期会用哪些工具（读文件/搜索/跑命令/改文件）</li><li><code>artifacts</code>: 这一步会产出什么（diff、日志文件、说明文档）</li><li><code>risk</code>: 是否涉及副作用/权限（例如写配置、删文件、发请求）</li><li><code>status</code>: todo/doing/done/blocked（以及阻塞原因）</li></ul><p>示例（概念表达，具体格式可用 Markdown TODO / YAML / JSON 皆可）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">plan:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">id:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">step:</span> <span class="string">"定位问题与影响范围"</span></span><br><span class="line">    <span class="attr">acceptance:</span> <span class="string">"能用最小复现或日志解释失败原因"</span></span><br><span class="line">    <span class="attr">tools:</span> <span class="string">["rg",</span> <span class="string">"read_file"</span><span class="string">,</span> <span class="string">"shell"</span><span class="string">]</span></span><br><span class="line">    <span class="attr">artifacts:</span> <span class="string">["notes/analysis.md"]</span></span><br><span class="line">    <span class="attr">risk:</span> <span class="string">"read-only"</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">"todo"</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">id:</span> <span class="number">2</span></span><br><span class="line">    <span class="attr">step:</span> <span class="string">"实现修复并生成补丁"</span></span><br><span class="line">    <span class="attr">acceptance:</span> <span class="string">"关键路径逻辑正确，代码可构建"</span></span><br><span class="line">    <span class="attr">tools:</span> <span class="string">["edit_file"]</span></span><br><span class="line">    <span class="attr">artifacts:</span> <span class="string">["git</span> <span class="string">diff</span> <span class="string">/</span> <span class="string">patch"]</span></span><br><span class="line">    <span class="attr">risk:</span> <span class="string">"write"</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">"todo"</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">id:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">step:</span> <span class="string">"验证与回归"</span></span><br><span class="line">    <span class="attr">acceptance:</span> <span class="string">"相关测试通过或提供替代验证证据"</span></span><br><span class="line">    <span class="attr">tools:</span> <span class="string">["shell"]</span></span><br><span class="line">    <span class="attr">artifacts:</span> <span class="string">["notes/verification.md"]</span></span><br><span class="line">    <span class="attr">risk:</span> <span class="string">"exec"</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">"todo"</span></span><br></pre></td></tr></table></figure><p>计划在运行时需要“可演进”：</p><ul><li><strong>Plan validation（计划校验）</strong>：在执行前检查权限/依赖/可执行性/成本。例如计划里出现“重置数据库”这类不可逆动作，应被标红并要求明确审批；出现“跑全量测试”也要做时间预算评估。</li><li><strong>Plan refinement（逐步细化）</strong>：先用粗粒度里程碑锁定方向，再在接近执行时把步骤细化成可操作的 tool plan（输入输出、预期失败模式）。</li><li><strong>Replanning（基于 observation 重规划）</strong>：工具返回的信息与预期不一致时，应允许替换策略（换方案/回退/拆分子任务），但要把变更写回计划，避免“悄悄跑偏”。</li></ul><p>可以把计划理解为 CLI Agent 的“最小状态机”：<strong>计划状态 + 工具结果</strong>，共同决定下一步动作；而不是靠对话历史隐式推进。</p><h4 id="13-1-3-Tool：把外部世界纳入闭环，但要可治理"><a href="#13-1-3-Tool：把外部世界纳入闭环，但要可治理" class="headerlink" title="13.1.3 Tool：把外部世界纳入闭环，但要可治理"></a>13.1.3 Tool：把外部世界纳入闭环，但要可治理</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_57.8ok6sf8vi9.webp" alt></p><p>Tool 是 Agent 获得 grounding 和行动能力的入口（读文件、写补丁、跑命令、查 git、访问网络/MCP）。但工具也带来真实世界的不确定性：权限、速率限制、输出波动、以及不可逆副作用。编码型 Agent 的关键不是“工具越多越好”，而是<strong>工具必须有合约（contract）与治理（governance）</strong>。</p><p>一个成熟的工具层通常包含三部分：</p><ol><li><strong>Tool registry（工具注册表）</strong>：列出工具名、参数 schema、输出类型、失败类型、以及风险分级。</li><li><strong>Permission/Sandbox（权限与隔离）</strong>：把工具调用分级成 allow/ask/deny 或只读/可写/高危；并用沙箱减少“无意义审批”，同时把副作用控制在边界内（第 13.2 再展开具体扩展机制）。</li><li><strong>Tool runner（执行器）</strong>：统一超时、重试、输出截断、日志落盘与幂等策略，把“工具不稳定性”收敛成可预测的返回结构。</li></ol><p>实践中很有效的一种做法是按“副作用”给工具分层：</p><ul><li><strong>Read-only</strong>：<code>ls/read_file/rg/git show</code> —— 可随时自动调用，主要风险是泄露敏感信息与上下文污染（需要脱敏/截断策略）。</li><li><strong>Repeatable side effects（可重复副作用）</strong>：<code>format/lint/test</code> —— 失败可重试，适合作为验收手段；风险是耗时与资源占用（需要预算）。</li><li><strong>Non-reversible side effects（不可逆副作用）</strong>：<code>rm/mv/写生产配置/发外部请求</code> —— 默认需要明确审批、双写审计或 dry-run，必要时要求“先产出 diff/计划再执行”。</li></ul><p>另外两条“编码任务常用但容易被忽视”的工具治理原则：</p><ul><li><strong>输出要可引用</strong>：长 stdout/stderr 不要全塞回 prompt，优先落盘到 workspace（日志文件、patch 文件），再在上下文里引用摘要与关键片段（呼应第 8 章上下文工程）。</li><li><strong>验证要工具化</strong>：尽量把 acceptance 写成可执行命令或可检查的 diff，而不是“我觉得修好了”。这会显著降低长链路的误差累积（呼应第 9 章评测与回归思路）。</li></ul><h4 id="13-1-4-Agent-Loop：观察-决策-行动-更新-停止的工程化版本"><a href="#13-1-4-Agent-Loop：观察-决策-行动-更新-停止的工程化版本" class="headerlink" title="13.1.4 Agent Loop：观察-决策-行动-更新-停止的工程化版本"></a>13.1.4 Agent Loop：观察-决策-行动-更新-停止的工程化版本</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_58.73ufsybo1s.webp" alt></p><p>Agent Loop 是第 7 章提到的“observe-act loop”的落地：它把一次“问答”扩展为多轮执行，并把每一轮变成可审计的 step。</p><p>一个典型 CLI 编码型 Agent 的 loop，可以拆成“外层计划推进 + 内层工具闭环”两层（概念伪代码）：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">state = init(goal, constraints, env)</span><br><span class="line">plan  = make_or_load_plan(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not stop(state, plan):</span><br><span class="line">  obs = collect_observations(state, plan)     # files, diffs, tool results, user feedback</span><br><span class="line"></span><br><span class="line">  decision = policy(obs, state, plan)         # update_plan | call_tool | ask_user | finalize</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> decision.type == update_plan:</span><br><span class="line">    plan = apply_plan_update(plan, decision)  # validation + versioning</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> decision.type == call_tool:</span><br><span class="line">    assert permission_ok(decision.tool)</span><br><span class="line">    result = run_tool(decision.tool, decision.args)</span><br><span class="line">    state, plan = reduce(state, plan, result) # record trace, advance step, summarize outputs</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> decision.type == ask_user:</span><br><span class="line">    pause_for_human()</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> deliverable(plan, artifacts, verification)</span><br></pre></td></tr></table></figure><p>这里的关键不是“循环本身”，而是 Orchestrator 在每一轮强制执行的几类约束：</p><ul><li><strong>预算约束</strong>：步数/时间/token/tool calls；避免陷入无效循环或成本失控。</li><li><strong>循环检测</strong>：重复读同一文件、重复跑同一命令、计划不推进等信号出现时，触发重规划或请求人工信息。</li><li><strong>失败策略</strong>：工具失败时优先“修正输入→重试”，再到“换策略→回退 checkpoint→拆分子问题”；必要时进入 reflection（对失败原因结构化总结）并写入工作区，防止反复踩坑。</li><li><strong>交付导向</strong>：每个步骤都要产出可交付的工件（diff、日志、说明、测试结果），最终组合成用户可消费的结果，而不是只输出一段“我做了什么”的叙述。</li></ul><h3 id="13-2-Extends-Plugins-Slash-commands-MCP-Hooks"><a href="#13-2-Extends-Plugins-Slash-commands-MCP-Hooks" class="headerlink" title="13.2 Extends: Plugins / Slash commands / MCP / Hooks"></a>13.2 Extends: Plugins / Slash commands / MCP / Hooks</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_59.3gow5ffvjz.webp" alt></p><p>13.1 描述的是 CLI Agent 的“内核”（Orchestrator/Plan/Tool/Loop）；13.2 关注的是“扩展面”：<strong>在不改内核语义的前提下扩工具、加治理、做复用</strong>。</p><ul><li><strong>Slash commands（控制面）</strong>：用显式命令管理运行时配置与边界（例如 <code>/permissions</code>、<code>/sandbox</code>、<code>/mcp</code>、<code>/hooks</code>），把“策略/权限/调试”从自然语言里剥离出来。</li><li><strong>MCP（工具接入标准）</strong>：以 MCP server 形式提供 tools/resources，CLI 作为 client 纳入 tool registry；调用仍受权限、审计与沙箱约束。</li><li><strong>Plugins（可分发能力包）</strong>：将工具、命令、默认配置与 hooks 打包，便于团队复用同一套工作流与治理规则。</li><li><strong>Hooks（可插拔治理）</strong>：在 plan 更新、tool 调用、文件写入等节点拦截/改写/自动化（例如强制先产出 diff、自动跑最小验证）。</li><li><strong>Memory（可选）</strong>：把跨会话经验做成“可检索、可引用”的输入，按需回填到 working set，而不是当作无限对话历史。</li><li><strong>Permissions &amp; sandboxing（底线）</strong>：把“能做”与“允许做”分离，通过 allow/ask/deny 与隔离边界控制副作用。</li></ul><h3 id="13-3-Sub-agent-amp-Skills"><a href="#13-3-Sub-agent-amp-Skills" class="headerlink" title="13.3 Sub-agent &amp; Skills"></a>13.3 Sub-agent &amp; Skills</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_60.8z70lko3nh.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_61.26lyz3xw8s.webp" alt></p><p>在 13.1 的四原语里（Orchestrator/Plan/Tool/Loop），<strong>Sub-agent</strong> 与 <strong>Skills</strong> 都不属于“新的内核组件”，而是 Orchestrator 的两种常见“增强手段”：</p><ul><li><strong>Sub-agent</strong>：把一个大任务拆成子任务，交给一个上下文隔离的执行体去跑，再把结果（摘要/证据/产物）汇总回主 loop（对应“分层编排 + context quarantine”）。</li><li><strong>Skills</strong>：把一套稳定可复用的做事方式（步骤、约束、工具偏好、输出格式）模块化封装，供主 agent 在合适时机调用（对应“可复用策略/工作流”）。</li></ul><h4 id="13-3-1-Sub-agent：并行与隔离的执行单元"><a href="#13-3-1-Sub-agent：并行与隔离的执行单元" class="headerlink" title="13.3.1 Sub-agent：并行与隔离的执行单元"></a>13.3.1 Sub-agent：并行与隔离的执行单元</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_62.5xb4kcmrga.webp" alt></p><p>Sub-agent 适合解决两类问题：<strong>并行吞吐</strong>与<strong>上下文污染</strong>。</p><ul><li><strong>并行吞吐</strong>：多个互不依赖的子问题（调研、比对、生成候选方案）可以并发跑，提高端到端速度。</li><li><strong>上下文隔离（context quarantine）</strong>：子任务的噪声、错误推断、长日志不直接进入主上下文；主 agent 只接收“可交付摘要 + 证据/产物”。</li></ul><p>一个实用的 sub-task 输入输出合约（让 Orchestrator 更好调度与回放）：</p><ul><li><strong>输入（给子代理）</strong>：子目标、约束/风险点、允许工具与权限边界、working set（可读文件/链接/线索）、期望交付格式与验收点。</li><li><strong>输出（回主代理）</strong>：结论摘要、关键证据（引用到文件/命令输出/链接）、产生的工件（patch/log/notes）、未决问题与下一步建议。</li></ul><p>典型使用场景：</p><ul><li><strong>代码库快速摸底</strong>：让子代理分别理解不同模块/目录，主代理只做汇总与决策。</li><li><strong>多方案探索</strong>：并行提出 2–3 个实现方案与风险评估，主代理选择其一落地。</li><li><strong>失败定位</strong>：子代理专注复现/定位错误与最小修复路径，避免主 loop 被长日志淹没。</li></ul><p>需要注意的治理点（呼应 13.1）：</p><ul><li><strong>预算与停止条件</strong>：对子代理单独设定 step/time/tool budget，避免“子任务失控”拖垮主任务。</li><li><strong>权限与副作用</strong>：对子代理的写入/执行权限应更保守；高风险动作仍由主代理走审批或二次确认。</li><li><strong>汇总前验证</strong>：子代理给的结论优先要求“可验证证据”（diff、可运行命令、可复现步骤），否则容易把幻觉在汇总阶段放大。</li></ul><h4 id="13-3-2-Skills：可复用的策略模块（workflow-playbook）"><a href="#13-3-2-Skills：可复用的策略模块（workflow-playbook）" class="headerlink" title="13.3.2 Skills：可复用的策略模块（workflow/playbook）"></a>13.3.2 Skills：可复用的策略模块（workflow/playbook）</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_63.9ddgcfweih.webp" alt></p><p>如果 sub-agent 是“把人力拆成多个并行执行者”，skills 更像“把资深同事的做法写成 SOP”。它通常用于提升一致性与可控性：</p><ul><li><strong>稳定的步骤模板</strong>：例如“先读哪些文件、再做哪些检查、最后如何验收”的固定顺序。</li><li><strong>固定的输出结构</strong>：例如必须输出 TODO、风险点、验收命令、回滚方案等。</li><li><strong>工具偏好与约束</strong>：例如优先用只读工具收集证据；修改代码前必须产出 diff；跑命令前先说明目的与预期结果。</li></ul><p>从 13.1 的视角看，skills 主要影响的是：</p><ul><li><strong>Plan 的质量</strong>：把“计划怎么写、验收怎么写”固化成可复用规范。</li><li><strong>Tool 的使用方式</strong>：把“什么情况下调用什么工具、如何处理输出/失败”固化为策略。</li><li><strong>Loop 的稳定性</strong>：减少随机游走，降低“重复读/重复跑/不推进”的概率。</li></ul><h4 id="13-3-3-Sub-agent-vs-Skills：如何选择"><a href="#13-3-3-Sub-agent-vs-Skills：如何选择" class="headerlink" title="13.3.3 Sub-agent vs Skills：如何选择"></a>13.3.3 Sub-agent vs Skills：如何选择</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_64.2a5kwtqyyg.webp" alt></p><p>可以用一句话区分：</p><ul><li><strong>Skills</strong>：解决“怎么做更一致、更可控”（复用策略）。</li><li><strong>Sub-agent</strong>：解决“谁来做、怎么隔离”（分工与并行）。</li></ul><p>一个快速判断表：</p><table><thead><tr><th>维度</th><th>Sub-agent</th><th>Skills</th></tr></thead><tbody><tr><td>形态</td><td>运行时执行体（可并行）</td><td>静态策略/流程模块（可复用）</td></tr><tr><td>上下文</td><td>默认隔离，主代理汇总</td><td>通常直接作用于主代理</td></tr><tr><td>适合</td><td>多子问题、并行探索、隔离噪声</td><td>重复工作流、规范化输出、组织约束</td></tr><tr><td>风险</td><td>成本上升、结论不一致、汇总困难</td><td>过度约束、技能陈旧、与项目冲突</td></tr></tbody></table><p>实践中最常见的组合是：完全独立、高耗时、低频的事情，适合做 Sub-agent，反之可用 Skills。值得注意的是，Sub-agent 也可以加载 Skills，所以其实二者不冲突。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_65.4jolgbbpfb.webp" alt></p><hr><h2 id="14-CC-Build-From-Scratch-learn-claude-code"><a href="#14-CC-Build-From-Scratch-learn-claude-code" class="headerlink" title="14. CC Build From Scratch: learn-claude-code"></a>14. CC Build From Scratch: <a href="https://github.com/shareAI-lab/learn-claude-code" target="_blank" rel="noopener">learn-claude-code</a></h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_66.9rjw3b4pdh.webp" alt></p><p>这一章不做“Claude Code 逆向分析”，也不试图复刻产品 UI，而是用一个可运行的教学项目 [learn-claude-code][32]，把第 13 章的四原语（Orchestrator/Plan/Tool/Loop）落成 <strong>一套最小但完整的 CLI 编码型 Agent 骨架</strong>：能读仓库、能改文件、能跑命令、能拆任务、能按需加载技能。</p><p>learn-claude-code 的设计非常适合当“从 0 到 1 的工程脚手架”：<strong>v0→v4 五个版本，每个版本只新增一个概念</strong>（bash 工具 → 4 工具 → Todo → Sub-agent → Skills），总共约千行代码，但把核心机制讲得非常清楚。</p><ul><li>学习材料入口：<ul><li>repo: <a href="https://github.com/shareAI-lab/learn-claude-code" target="_blank" rel="noopener">https://github.com/shareAI-lab/learn-claude-code</a></li><li>articles（偏公众号风格）: <a href="https://github.com/shareAI-lab/learn-claude-code/tree/main/articles" target="_blank" rel="noopener">https://github.com/shareAI-lab/learn-claude-code/tree/main/articles</a></li><li>docs（偏技术讲解，中英）: <a href="https://github.com/shareAI-lab/learn-claude-code/tree/main/docs" target="_blank" rel="noopener">https://github.com/shareAI-lab/learn-claude-code/tree/main/docs</a></li></ul></li></ul><h3 id="14-1-目标与最小闭环：先把“能跑起来”变成第一原则"><a href="#14-1-目标与最小闭环：先把“能跑起来”变成第一原则" class="headerlink" title="14.1 目标与最小闭环：先把“能跑起来”变成第一原则"></a>14.1 目标与最小闭环：先把“能跑起来”变成第一原则</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_67.wj1ssfwxg.webp" alt></p><p>如果只记住一句话：<strong>编码型 Agent = 一个允许模型反复调用工具直到完成任务的循环</strong>。</p><p>learn-claude-code 给出的“最小闭环”几乎可以作为所有 CLI Agent 的伪代码模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">messages = [&#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: user_goal&#125;]</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  r = model(messages, tools)</span><br><span class="line">  <span class="keyword">if</span> r.stop_reason != <span class="string">"tool_use"</span>:</span><br><span class="line">    <span class="keyword">return</span> r.text</span><br><span class="line"></span><br><span class="line">  results = execute(r.tool_calls)  <span class="comment"># bash/read/write/edit/...</span></span><br><span class="line">  messages.append(&#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: results&#125;)</span><br></pre></td></tr></table></figure><p>把它对照回第 13 章四原语：</p><ul><li><strong>Orchestrator</strong>：驱动这个 loop；决定何时停、何时读文件、何时调用什么工具、何时重规划。</li><li><strong>Tool</strong>：把“动作”从自然语言里剥离为可执行 schema（bash/read/write/edit/…）。</li><li><strong>Plan</strong>：把“下一步做什么”变成显式状态（v2 的 TodoWrite 就是最小可用形态）。</li><li><strong>Agent Loop</strong>：observe→act→observe 的闭环，靠工具结果把 hallucination 拉回现实。</li></ul><p>这一章的写法会沿着 learn-claude-code 的版本递进来展开：每一小节都回答三件事：<strong>新增了什么？解决了什么问题？代价/边界是什么？</strong></p><h3 id="14-2-v0：Bash-is-All-You-Need（1-个工具也能涌现出“完整-Agent”）"><a href="#14-2-v0：Bash-is-All-You-Need（1-个工具也能涌现出“完整-Agent”）" class="headerlink" title="14.2 v0：Bash is All You Need（1 个工具也能涌现出“完整 Agent”）"></a>14.2 v0：Bash is All You Need（1 个工具也能涌现出“完整 Agent”）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_68.32igek7koj.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_69.6po0233d6a.webp" alt></p><p>v0 的核心洞察是：在类 Unix 环境里，<strong>bash 是通往一切能力的 meta-interface</strong>。读文件、写文件、搜索、运行测试、调用 git、本地脚本化——都可以通过 bash 完成。</p><p>因此 v0 只提供 1 个工具：</p><ul><li><code>bash(command: str) -&gt; stdout/stderr</code></li></ul><p>然后把系统提示词写得足够“工程化”（引导多用工具、少空谈、不要编造路径等），就已经能在不少任务上跑出可用效果。</p><h4 id="14-2-1-递归即子代理：用“进程隔离”换“上下文隔离”"><a href="#14-2-1-递归即子代理：用“进程隔离”换“上下文隔离”" class="headerlink" title="14.2.1 递归即子代理：用“进程隔离”换“上下文隔离”"></a>14.2.1 递归即子代理：用“进程隔离”换“上下文隔离”</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_70.5fl2vrldv5.webp" alt></p><p>v0 最精彩的地方是“子代理”实现几乎不要额外机制：<strong>通过 bash 调用自身</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python v0_bash_agent.py <span class="string">"explore src/ and summarize"</span></span><br></pre></td></tr></table></figure><p>为什么这等价于 Sub-agent（至少在教学版里足够）：</p><ul><li><strong>进程隔离 = 历史隔离</strong>：子进程有全新的 <code>history=[]</code>，不会把探索阶段的长输出污染主对话。</li><li><strong>stdout = 汇报通道</strong>：子代理只把最终总结打印出来，父代理把它当作工具结果读回。</li><li><strong>递归 = 层级拆解</strong>：复杂任务可以继续拆子任务（当然生产系统会对递归深度/预算做约束）。</li></ul><p>v0 的代价也很明确：</p><ul><li><strong>安全边界非常弱</strong>：子代理也能执行任意 bash（包括写文件/删文件）。</li><li><strong>治理全靠提示词</strong>：没有权限系统、没有 allowlist/denylist、没有审计与审批。</li></ul><p>所以 v0 更像“证明题”：证明 Agent 的本质确实极小；但真正可用的工程系统必须继续加护栏。</p><h3 id="14-3-v1：Model-as-Agent（4-工具-workspace-护栏，把-v0-变成“可维护系统”）"><a href="#14-3-v1：Model-as-Agent（4-工具-workspace-护栏，把-v0-变成“可维护系统”）" class="headerlink" title="14.3 v1：Model as Agent（4 工具 + workspace 护栏，把 v0 变成“可维护系统”）"></a>14.3 v1：Model as Agent（4 工具 + workspace 护栏，把 v0 变成“可维护系统”）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_71.26lyz3xw8g.webp" alt></p><p>v1 的目标是把 v0 的“bash 万能”拆成更稳定的 4 个工具接口，原因是：</p><ul><li><strong>可控性</strong>：读文件/写文件/编辑文件分别约束，行为更可预测。</li><li><strong>可观测性</strong>：工具层可以统一做路径检查、输出截断、超时、错误格式化。</li><li><strong>更少幻觉</strong>：模型不再需要“发明一条复杂 bash”来做精确编辑。</li></ul><p>v1 的“四件套”基本覆盖 90% 编码任务：</p><table><thead><tr><th>工具</th><th>用途</th><th>工程护栏（推荐）</th></tr></thead><tbody><tr><td><code>bash</code></td><td>跑命令：<code>rg</code>/<code>git</code>/<code>pytest</code>/<code>npm</code></td><td>超时、危险命令拦截、输出截断</td></tr><tr><td><code>read_file</code></td><td>读文件内容</td><td><code>safe_path</code>（不允许逃逸工作区）、可选行数上限</td></tr><tr><td><code>write_file</code></td><td>新建/整文件覆盖</td><td><code>safe_path</code>、自动建目录、显式返回写入摘要</td></tr><tr><td><code>edit_file</code></td><td>小范围“外科手术式”替换</td><td><strong>exact match</strong>（找不到 old_text 就报错）</td></tr></tbody></table><p>这里面最值得“抄作业”的是两类护栏：</p><ol><li><strong>workspace 逃逸防护（safe_path）</strong>：任何文件路径都 resolve 到 WORKDIR 下；<code>../</code> 逃逸直接拒绝。</li><li><strong>输出/成本控制</strong>：工具结果截断（例如 50KB），避免一次 <code>cat</code> 把上下文打爆。</li></ol><p>从第 13 章角度看，v1 让 Tool 层具备了“治理入口”：安全、超时、截断、错误处理可以在工具层统一做，而不是寄希望于模型每次都自觉。</p><h3 id="14-4-v2：TodoWrite（把“计划”从模型脑内拖到台面上）"><a href="#14-4-v2：TodoWrite（把“计划”从模型脑内拖到台面上）" class="headerlink" title="14.4 v2：TodoWrite（把“计划”从模型脑内拖到台面上）"></a>14.4 v2：TodoWrite（把“计划”从模型脑内拖到台面上）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_72.96a8h0a92n.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_73.4qrtbqxuum.webp" alt></p><p>v1 最大的问题不是“不会写代码”，而是做长任务时容易出现 <strong>context fade</strong>：</p><ul><li>前几轮说了要做 A→B→C，但十几个 tool call 后忘了做到哪一步；</li><li>执行顺序跳来跳去，用户也看不清进度；</li><li>完成定义不清晰，很难收敛到“可验收”。</li></ul><p>v2 的解法极其朴素，但效果非常强：增加一个 Todo 工具（计划板），让模型 <strong>必须在结构化状态机里推进任务</strong>。</p><h4 id="14-4-1-Todo-的-schema：约束不是限制，而是脚手架"><a href="#14-4-1-Todo-的-schema：约束不是限制，而是脚手架" class="headerlink" title="14.4.1 Todo 的 schema：约束不是限制，而是脚手架"></a>14.4.1 Todo 的 schema：约束不是限制，而是脚手架</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_74.lw7zn0orx.webp" alt></p><p>learn-claude-code 的 TodoWrite 设计有几个关键约束（不是随意的）：</p><ul><li><strong>最多 20 条</strong>：防止无限扩张的“愿望清单”。</li><li><strong>最多 1 条 in_progress</strong>：强制专注，避免并行导致的上下文漂移。</li><li><strong>字段必填</strong>：<code>content/status/activeForm</code> 三件套，保证可显示、可推进、可回放。</li></ul><p>呈现上一般会渲染成这种“看得见的执行状态”：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ ] Refactor auth module</span><br><span class="line">[&gt;] Add unit tests &lt;- Adding unit tests...</span><br><span class="line">[ ] Update documentation</span><br><span class="line">(0/3 completed)</span><br></pre></td></tr></table></figure><p>对照第 13 章，“Plan 不是一段漂亮 prose”，而是一个要被 Orchestrator 强制维护的对象；TodoWrite 是最小的可运行版本。</p><h4 id="14-4-2-提醒（nag）机制：把“流程偏好”变成运行时反馈"><a href="#14-4-2-提醒（nag）机制：把“流程偏好”变成运行时反馈" class="headerlink" title="14.4.2 提醒（nag）机制：把“流程偏好”变成运行时反馈"></a>14.4.2 提醒（nag）机制：把“流程偏好”变成运行时反馈</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_75.8hgywzmq24.webp" alt></p><p>v2 还加了一个非常实用的工程小招：如果模型连续多轮没更新 Todo，就往上下文里注入 reminder（软约束），提示它回到“计划→执行→更新”节奏。</p><p>这类 reminder 属于低成本但高收益的 harness：不改变核心 loop，却显著减少“跑偏”。</p><h3 id="14-5-v3：Task-Sub-agent（用上下文隔离解决“探索污染”）"><a href="#14-5-v3：Task-Sub-agent（用上下文隔离解决“探索污染”）" class="headerlink" title="14.5 v3：Task / Sub-agent（用上下文隔离解决“探索污染”）"></a>14.5 v3：Task / Sub-agent（用上下文隔离解决“探索污染”）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_76.1vz55yio2z.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_77.8l0kupfsru.webp" alt></p><p>当任务变成“先探索大仓库 → 再做精细改动”，单一上下文会被探索阶段的海量细节占满。v3 的核心目标就是解决这类 <strong>context pollution</strong>。</p><p>v3 引入 Task 工具，让主代理把子任务派给“上下文隔离”的子代理：</p><ul><li>主代理保留干净上下文，只接收子代理的<strong>最终总结</strong>；</li><li>子代理可以被赋予更窄的权限（工具白名单），例如 explore 只能读不能写。</li></ul><h4 id="14-5-1-Agent-类型注册表：用“职责分离”引导模型行为"><a href="#14-5-1-Agent-类型注册表：用“职责分离”引导模型行为" class="headerlink" title="14.5.1 Agent 类型注册表：用“职责分离”引导模型行为"></a>14.5.1 Agent 类型注册表：用“职责分离”引导模型行为</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_78.3rbpykv3oo.webp" alt></p><p>learn-claude-code 的教学版通常会定义三类子代理（也可以扩展）：</p><ul><li><code>explore</code>：只读探索（<code>bash + read_file</code>），负责“找文件/理解结构/定位入口”。</li><li><code>plan</code>：只读规划（<code>bash + read_file</code>），负责“基于现状产出可执行方案/风险点”。</li><li><code>code</code>：全权限实现（全部工具），负责“落地修改与验证”。</li></ul><p>这背后的设计意图不是“权限安全”这么简单，而是 <strong>行为塑形</strong>：当模型知道自己只有只读工具时，它更倾向于先把事实摸清楚，而不是边看边改。</p><h4 id="14-5-2-Task-的输入输出合约：子代理只交“可用结论”"><a href="#14-5-2-Task-的输入输出合约：子代理只交“可用结论”" class="headerlink" title="14.5.2 Task 的输入输出合约：子代理只交“可用结论”"></a>14.5.2 Task 的输入输出合约：子代理只交“可用结论”</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_79.13m9o822ci.webp" alt></p><p>一个实用的 Task 工具 schema 通常会包含：</p><ul><li><code>description</code>：简短任务名（用于进度显示）</li><li><code>prompt</code>：对子代理的详细指令（含范围/输出格式/验收点）</li><li><code>agent_type</code>：选择 explore/plan/code（决定工具集与系统提示词）</li></ul><p>执行时关键点是：<strong>子代理使用独立 <code>sub_messages=[]</code> 跑同一个 agent loop</strong>，跑完后只把最终文本返回主代理。</p><p>这就是第 13 章提到的 context quarantine：把“噪声/长日志/探索细节”关在子上下文里，只把“可复用的摘要与证据”带回主循环。</p><h3 id="14-6-v4：Skills（把“知识”外化成可编辑文件，并按需加载）"><a href="#14-6-v4：Skills（把“知识”外化成可编辑文件，并按需加载）" class="headerlink" title="14.6 v4：Skills（把“知识”外化成可编辑文件，并按需加载）"></a>14.6 v4：Skills（把“知识”外化成可编辑文件，并按需加载）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_80.86u53u7hwh.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_81.175vlxv528.webp" alt></p><p>到 v3 为止，我们解决的是“能干活、能拆活”。但编码 Agent 很快会遇到另一个瓶颈：很多任务不是缺工具，而是缺 <strong>做法</strong>：</p><ul><li>如何写一个 MCP server？</li><li>如何做系统化 code review？</li><li>如何处理 PDF/音频/图像等非文本输入？</li></ul><p>这类“专家方法论”如果全塞进 system prompt，会把上下文撑爆；如果完全靠模型即兴发挥，会不稳定。v4 的 Skills 机制就是为此设计的：<strong>工具管能力，技能管知识</strong>。</p><h4 id="14-6-1-SKILL-md：一个可版本化、可分发的“知识包”"><a href="#14-6-1-SKILL-md：一个可版本化、可分发的“知识包”" class="headerlink" title="14.6.1 SKILL.md：一个可版本化、可分发的“知识包”"></a>14.6.1 SKILL.md：一个可版本化、可分发的“知识包”</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_82.5xb4kcmrfj.webp" alt></p><p>最常见的 skills 目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">skills/</span><br><span class="line">  pdf/</span><br><span class="line">    SKILL.md</span><br><span class="line">    references/</span><br><span class="line">    scripts/</span><br><span class="line">    assets/</span><br><span class="line">  mcp-builder/</span><br><span class="line">    SKILL.md</span><br></pre></td></tr></table></figure><p>其中 <code>SKILL.md</code> 通常采用 YAML frontmatter 提供元数据（name/description），正文是详细步骤、命令模板、注意事项等。</p><h4 id="14-6-2-渐进式披露（progressive-disclosure）：让-context-只为“当前任务”付费"><a href="#14-6-2-渐进式披露（progressive-disclosure）：让-context-只为“当前任务”付费" class="headerlink" title="14.6.2 渐进式披露（progressive disclosure）：让 context 只为“当前任务”付费"></a>14.6.2 渐进式披露（progressive disclosure）：让 context 只为“当前任务”付费</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_83.1aphjno7rx.webp" alt></p><p>v4 把 skill 加载拆成三层：</p><ul><li><strong>Layer 1（始终加载）</strong>：只加载 name + description（很短，用于路由/触发）。</li><li><strong>Layer 2（触发时加载）</strong>：加载 SKILL.md 正文（可能上千 token，但只在需要时注入）。</li><li><strong>Layer 3（按需）</strong>：再去读 references/scripts/assets（无限深，但按需取用）。</li></ul><p>这基本是生产级 CLI Agent（包括 Claude Code/Kode 一类产品）在“技能/文档注入”上最常见的工程策略：让上下文保持 lean，同时保留无限扩展深度。</p><h4 id="14-6-3-缓存友好的注入方式：skill-内容走-tool-result，而不是-system-prompt"><a href="#14-6-3-缓存友好的注入方式：skill-内容走-tool-result，而不是-system-prompt" class="headerlink" title="14.6.3 缓存友好的注入方式：skill 内容走 tool_result，而不是 system prompt"></a>14.6.3 缓存友好的注入方式：skill 内容走 tool_result，而不是 system prompt</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_84.6po0233d5o.webp" alt></p><p>learn-claude-code 特别强调一个容易被忽略的成本点：很多 LLM API 有 <strong>prompt prefix cache</strong>，但命中条件通常是“前缀完全相同”。因此：</p><ul><li><strong>不要</strong>把动态状态/skill 内容写进 system prompt（会让缓存失效，成本可能飙升）。</li><li><strong>要</strong>把 skill 作为工具结果追加到 messages 尾部（前缀不变，缓存命中）。</li></ul><p>这也是为什么 v4 的 Skill tool 会把 SKILL.md 内容作为 <code>tool_result</code> 返回，追加到对话历史里（而不是修改 system prompt）。</p><p>如果对这类成本差异敏感，可以把 learn-claude-code 的《上下文缓存经济学》当作必读补丁：一旦在历史中间插入/编辑/替换消息，成本与延迟都可能出现数量级变化。</p><h3 id="14-7-从“教学版”到“可用版”：还差哪些工程补丁？"><a href="#14-7-从“教学版”到“可用版”：还差哪些工程补丁？" class="headerlink" title="14.7 从“教学版”到“可用版”：还差哪些工程补丁？"></a>14.7 从“教学版”到“可用版”：还差哪些工程补丁？</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_85.39lo9ztq3f.webp" alt></p><p>learn-claude-code 的 v0-v4 已经覆盖了 Claude Code 类 CLI Agent 的核心机制，但离“生产可用”通常还差一组工程化补丁。可以把它们理解为第 13 章 13.2（扩展面）里那些能力的“落地清单”：</p><ol><li><strong>权限与沙箱（底线治理）</strong><ul><li><code>bash</code> 的 allow/ask/deny；对网络、写入、危险命令做分级审批；</li><li>workspace 逃逸、敏感文件规则、secret redaction。</li></ul></li><li><strong>变更交付方式：从 write/edit 到 patch/diff</strong><ul><li>让模型产出 diff/patch，再由运行时应用（更可审计、更易 review/回滚）；</li><li>默认先 <code>git diff</code> 再解释变更，形成“证据优先”的交付习惯。</li></ul></li><li><strong>可观测与可回放（debuggable agent）</strong><ul><li>记录每次 tool_call 的输入/输出摘要、耗时、失败原因；</li><li>保存 transcript，使一次 run 可回放、可复盘、可评测。</li></ul></li><li><strong>最小验证闭环（不要只‘写完’，要‘验收’）</strong><ul><li>把“怎么验收”写进计划（Todo/Plan），并在结束前自动执行（例如 <code>pytest</code>/<code>npm test</code>/<code>go test</code>）。</li></ul></li><li><strong>上下文工程（working set）</strong><ul><li>读文件前先 search；大文件按需截取；探索任务交给子代理；</li><li>避免把“所有历史”当作上下文：用计划、日志、产物来承载状态。</li></ul></li><li><strong>插件化与生态接入</strong><ul><li>MCP servers / connectors；hooks；skills 分发与版本管理。</li></ul></li></ol><p>做到这里，基本就拥有了一套“Claude Code 风格”的可演进骨架：核心 loop 很小，但通过 Plan/Tools/Sub-agents/Skills/Permissions/Hooks 不断叠加工程约束，让它能稳定跑长任务、并且可控可审计。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_86.1zir3obqsa.webp" alt></p><hr><h2 id="15-Claude-Code-Alternatives-Gemini-CLI-Codex-OpenCode"><a href="#15-Claude-Code-Alternatives-Gemini-CLI-Codex-OpenCode" class="headerlink" title="15. Claude Code Alternatives: Gemini CLI, Codex, OpenCode"></a>15. Claude Code Alternatives: Gemini CLI, Codex, OpenCode</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_87.mkdc68gq.webp" alt></p><p>下面选取三个常被拿来对标 Claude Code 的 CLI 编码型 Agent：Gemini CLI、Codex CLI、OpenCode，并用同一套维度做横向对比（信息以各项目公开仓库/文档为准；推出时间按 GitHub 仓库首次公开粗略标注）。</p><ul><li>Claude Code: <a href="https://github.com/anthropics/claude-code" target="_blank" rel="noopener">https://github.com/anthropics/claude-code</a></li><li>Gemini CLI: <a href="https://github.com/google-gemini/gemini-cli" target="_blank" rel="noopener">https://github.com/google-gemini/gemini-cli</a></li><li>Codex CLI: <a href="https://github.com/openai/codex" target="_blank" rel="noopener">https://github.com/openai/codex</a></li><li>OpenCode: <a href="https://github.com/anomalyco/opencode" target="_blank" rel="noopener">https://github.com/anomalyco/opencode</a></li></ul><table><thead><tr><th>维度</th><th>Claude Code</th><th>Gemini CLI</th><th>Codex CLI</th><th>OpenCode</th></tr></thead><tbody><tr><td>源公司/组织</td><td>Anthropic</td><td>Google（google-gemini）</td><td>OpenAI</td><td>Anomaly（anomalyco）</td></tr><tr><td>主要实现语言</td><td>Shell/Python/TypeScript（Node 生态）</td><td>TypeScript（Node.js）</td><td>Rust</td><td>TypeScript</td></tr><tr><td>开源程度</td><td><strong>部分开源 / 源码可见但非开源</strong>（LICENSE 为商业条款）</td><td><strong>完全开源</strong>（Apache-2.0）</td><td><strong>完全开源</strong>（Apache-2.0）</td><td><strong>完全开源</strong>（MIT）</td></tr><tr><td>Model 绑定与限制</td><td>主要绑定 <strong>Claude</strong>（Anthropic 账户/API）；更换 provider 需借助第三方路由/代理</td><td>主要绑定 <strong>Gemini</strong>（Google 账号/API/Vertex）；强调 Gemini 2.5 Pro / 1M context</td><td>主要绑定 <strong>OpenAI</strong>（ChatGPT 登录或 API key）</td><td><strong>Provider-agnostic</strong>：Claude/OpenAI/Gemini/本地模型等（同一套交互切模型）</td></tr><tr><td>工具与扩展面</td><td>终端 agent + 代码理解/写入 + git workflows；支持插件（commands/agents）</td><td>内置 Search grounding、文件/命令/抓取工具；支持 MCP 扩展</td><td>本地运行；sandbox/审批；配置/skills；支持 MCP 与通知 hooks</td><td>强 TUI + LSP；内置 build/plan agent 与 <code>@general</code> subagent；client/server 架构便于多客户端</td></tr><tr><td>功能丰富度（相对）</td><td>高（产品化工作流与生态）</td><td>中-高（“直达模型”+ 常用工具 + MCP）</td><td>中-高（本地 agent + 安全治理 + 可配置）</td><td>高（终端体验 + 多 provider + 多客户端）</td></tr><tr><td>推出时间（约）</td><td>2025-02</td><td>2025-04</td><td>2025-04</td><td>2025-04</td></tr></tbody></table><p>补充：几个“设计取向”的差异（也决定了该怎么选）：</p><ul><li><strong>厂商第一方 vs 第三方</strong>：Claude Code / Gemini CLI / Codex 属于模型厂商第一方工具，默认强绑定各自账号与模型；OpenCode 更强调 provider-agnostic，在模型快速迭代/价格波动时更容易“换发动机”。</li><li><strong>扩展机制</strong>：Claude Code 更偏“插件化工作流”（自定义命令/agents）；Gemini CLI 与 Codex 都把 MCP 作为对接外部能力的主入口（协议化、可组合）；OpenCode 既做 agent，也把终端体验（TUI/LSP）作为核心卖点。</li><li><strong>运行时形态</strong>：Codex 强调 sandbox/审批这类“副作用治理”；Gemini CLI 强调 Search grounding 与大上下文；OpenCode 的 client/server 更像把 agent runtime 做成可被不同前端驱动的服务。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_88.1aphjno7rs.webp" alt></p><h2 id="16-2B-的-Agent-Manus-解析"><a href="#16-2B-的-Agent-Manus-解析" class="headerlink" title="16. $2B 的 Agent: Manus 解析"></a>16. $2B 的 Agent: Manus 解析</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_89.5fl2vrlduf.webp" alt></p><p>上一章我们把 Claude Code、Gemini CLI、Codex CLI、OpenCode 放在同一张表里对比，关注点是：当 Agent 被交付为 CLI 工具时，运行时、权限与扩展机制会决定它能跑多长、多稳、多可控。</p><p>但在真实工作流里，“终端 + 工作区”并不是唯一的默认执行环境。更多任务发生在浏览器、邮箱、IM 与各类 SaaS：跨站点、跨账号、跨格式，既要求过程可见（可接管/可审计），也要求产物可交付（文档/链接/自动化结果）。Manus 正是在这个位置上，把 Agent 从“给建议”推进到“把事做完”，并把它产品化成一个可持续运营的系统——也因此被市场用 <strong>\$2B</strong> 这样的叙事来标记其商业化潜力（这里的 <strong>\$2B</strong> 更像是对增长与护城河的预期，而不只是模型能力本身）。</p><p>本章不把 Manus 当作某个单点功能，而把它当作一套可运行、可扩展、可治理的 Agent Runtime 来拆解：任务如何建模、计划如何落地、工具如何编排、状态如何持久、失败如何治理、权限如何收束。我们将从产品定位与典型端到端任务入手，拆解 Cloud Browser / Browser Operator / Mail Manus / Collab 等能力面板，再用 Task / Plan / Tool / State / Observability 的视角归纳其架构要点，并补全 API 与生态接入方式，最后回到一个更现实的问题：为什么这种“闭环交付”的 Agent 能被卖到 $2B。</p><h3 id="16-1-Manus-是什么：产品定位与典型端到端任务"><a href="#16-1-Manus-是什么：产品定位与典型端到端任务" class="headerlink" title="16.1 Manus 是什么：产品定位与典型端到端任务"></a>16.1 Manus 是什么：产品定位与典型端到端任务</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_90.102nqi8zme.webp" alt></p><p>Manus 是一种面向通用任务的自主 AI Agent，引入了 <a href="https://www.baytechconsulting.com/blog/manus-ai-an-analytical-guide-to-the-autonomous-ai-agent-2025" target="_blank" rel="noopener">从意图到行动闭环交付</a> 的全新范式。不同于传统对话式 AI（如 ChatGPT）只能提供答案，Manus 能自主计划并执行一系列操作，最终交付 <a href="https://manus.im/tools" target="_blank" rel="noopener">完整的工作成果</a>。其名称源自拉丁语“手”（Manus），寓意这个 AI 就像用户的 <a href="https://www.baytechconsulting.com/blog/manus-ai-an-analytical-guide-to-the-autonomous-ai-agent-2025" target="_blank" rel="noopener">数字双手</a>——不仅会给建议，更会亲自动手完成任务。从研究信息、编写代码到上线部署，Manus 致力于贯通任务执行的各个环节。</p><p>Manus 可以胜任跨领域的复杂任务，覆盖知识工作和数字化操作的各个方面。例如：</p><ul><li>内容创作与报告：给定一个主题，Manus 能自主检索多源资料并撰写完整的<a href="https://www.baytechconsulting.com/blog/manus-ai-an-analytical-guide-to-the-autonomous-ai-agent-2025" target="_blank" rel="noopener">调研报告或文章</a>。又如处理长邮件或 PDF 文档，将其摘要后输出成要点或决策建议。</li><li>幻灯片与文档生成：用户只需提供要点，Manus 就能自动生成 PowerPoint 演示文稿，包括草拟内容、设计幻灯片版式，甚至添加<a href="https://manus.im/zh-cn/tools" target="_blank" rel="noopener">引人注目的图表</a>。整个过程耗时可能仅数分钟，且成果可编辑完善。</li><li>代码编写与应用部署：Manus 配备了代码生成和执行环境，能够根据自然语言要求编写程序、构建网页或小游戏，并将结果<a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">部署到在线空间</a>。例如，用户一句话描述游戏需求，Manus 可产出完整的浏览器游戏或克隆一个网站。</li><li>数据分析与可视化：Manus 可以读取用户提供的表格或数据库，通过内置的数据分析库处理数据，输出分析报告或<a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">交互式图表</a>。例如，自动生成销售数据的可视化图表并分析趋势。</li><li>在线事务处理：借助浏览器控制能力，Manus 能尝试代替用户执行网上事务，如<a href="https://medium.com/@jalajagr/inside-manus-the-anatomy-of-an-autonomous-ai-agent-b3042e5e5084" target="_blank" rel="noopener">填写表单、预订服务甚至下单购买</a>（尽管在实际使用中有时会受限于验证步骤）。这一能力展示了其从对话助手向数字代理的跃升。</li></ul><p>简而言之，Manus 的典型工作流是：用户以自然语言描述目标，Manus 将此视作一个任务，自主拆解为子步骤计划，利用各种工具执行每一步，监控中间状态并迭代调整，最终将成果交付给用户（可参考 <a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">Helicone 的机制拆解</a>）。这种端到端的自动化，大幅减少了用户在不同应用之间来回手动操作的时间，使复杂任务的完成如同使用“一站式服务”般流畅（也可参考 <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Integrations 文档</a>）。</p><h3 id="16-2-Manus-能做什么"><a href="#16-2-Manus-能做什么" class="headerlink" title="16.2 Manus 能做什么"></a>16.2 Manus 能做什么</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_91.5j4othegk5.webp" alt></p><p>与 Claude Code 这种 CLI Agent 不同，Manus 是一个『通用』Agent：它不把“工作区/终端”当作唯一入口，而是把浏览器、邮箱与协作平台当作默认执行面，把任务拆成“能交付的结果”而不是“可运行的命令”。给出目标后，它会在云端/本地拉起合适的环境，读取附件与数据源，选择工具执行，并把中间产物沉淀为可追踪的文件、链接与日志。</p><p>更具体地说，这个『通用』体现在三个层面：</p><ul><li><strong>跨场景</strong>：从调研与写作、幻灯片与文档生成，到数据分析、代码生成与部署，再到需要登录态的网页操作与跨系统事务处理；</li><li><strong>跨介质</strong>：网页、PDF/Office、邮件线程、表格、代码仓库、IM 消息等都可能是输入/输出的一部分；</li><li><strong>跨触点</strong>：不仅在网页 UI 里对话，还能通过浏览器插件、邮件、Slack 等渠道触发与交付（后面 16.4 会展开）。</li></ul><p>同时，Manus 的“通用”也依赖于连接器/集成层：通过 <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Integrations</a> / <a href="https://open.manus.im/docs/integrations/data-sources" target="_blank" rel="noopener">Data Sources</a> 以及 <a href="https://zapier.com/apps/manus/integrations/slack" target="_blank" rel="noopener">Zapier</a> 这类工作流平台，它可以把任务接到已有的业务系统里，比如 Notion 的知识库、Google Calendar 的日程，乃至 Stripe 这类支付与账务系统。这样交付就不止是“生成一段内容”，而是对真实系统进行创建/更新/通知等可执行动作。</p><p>为了把“通用能力”落到可用的产品形态，Manus 在能力面板上做了几类典型的执行通道（不同通道对应不同的权限边界与用户参与度）：</p><ul><li><a href="https://open.manus.im/docs/features/cloud-browser" target="_blank" rel="noopener">Cloud Browser</a>：在隔离的云端浏览器里执行跨站点操作，界面实时可见、可接管；适合需要长链网页流程、且不想污染本地环境的任务。</li><li><a href="https://open.manus.im/docs/features/browser-operator" target="_blank" rel="noopener">Browser Operator</a>：通过浏览器扩展在本地浏览器里“借用现成登录态”执行操作，省去重复登录与验证；但也意味着需要更严格的授权与确认。</li><li><a href="https://open.manus.im/docs/features/mail-manus" target="_blank" rel="noopener">Mail Manus</a>：把任务放进邮件工作流——转发/抄送即可触发，附件天然携带；适合异步处理长文档、报表与审批类任务。</li><li><a href="https://open.manus.im/docs/features/collab" target="_blank" rel="noopener">Collab</a>：把任务与产物放到团队协作语境里，让“谁在做/做到了哪/交付了什么”可共享、可审计，便于 review 与交接。</li></ul><p>如果把这些“产品入口与执行通道”抽象掉，Manus 的本质仍然是一套 Agent Runtime：Task 如何建模、Plan 如何演进、Tool 如何选择与编排、State 如何持久、以及 Observability 如何支撑可控与可靠——这也是下一节要用运行时视角拆解的部分。</p><h3 id="16-3-Agent-Runtime-视角：Task-Plan-Tool-State-Observability"><a href="#16-3-Agent-Runtime-视角：Task-Plan-Tool-State-Observability" class="headerlink" title="16.3 Agent Runtime 视角：Task / Plan / Tool / State / Observability"></a>16.3 Agent Runtime 视角：Task / Plan / Tool / State / Observability</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_92.4g4zilimoj.webp" alt></p><p>第 13/14 章已经把 Orchestrator/Plan/Tool/Loop 的通用机制讲清楚了，这里不再重复定义，而是把 Manus 的产品形态映射到运行时的五个维度（参考 <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus" target="_blank" rel="noopener">Context Engineering for AI Agents: Lessons from Building Manus</a>）。</p><ul><li>Task（任务）：API 层的 Task 是顶层执行单元（prompt + 附件 + 元数据），有唯一 ID/状态/时间戳等；运行时由顶层的 <a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">Executor Agent</a> 负责接住请求并推进执行（另见 <a href="https://open.manus.im/docs/api-reference" target="_blank" rel="noopener">Manus API</a>）。</li><li>Plan（计划）：复杂任务会被 <a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">Planner Agent</a> 拆成 UI 可见的 to-do 列表；执行中按新信息与偏差持续插入/修改步骤，保证可对齐、可推进。</li><li>Tool（工具）：工具层包含沙盒内置能力（浏览器/终端/Python/文件等）+ 通过 <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Integrations</a> 接入的第三方服务；每步“选工具→执行→回填结果”把推理闭环拉回现实。</li><li>State（状态）：每个任务运行在隔离沙盒与工作空间里，沉淀中间工件（文件/数据/代码）供多步复用与最终交付；长任务用分层记忆/多 Agent 分工缓解遗忘与噪声。</li><li>Observability（可观测性）：用户侧暴露计划、当前步骤、工具操作与中间结果；系统侧记录结构化日志，并可通过 <a href="https://open.manus.im/docs/integrations/webhooks" target="_blank" rel="noopener">Webhooks</a> / <a href="https://open.manus.im/docs/integrations/slack-integration" target="_blank" rel="noopener">Slack 集成</a> 推送状态变化，同时配合重试与误差预算等治理策略（见 <a href="https://skywork.ai/blog/ai-agent/observability-manus-1-5-agents-best-practices/" target="_blank" rel="noopener">Observability for Manus 1.5 Agents: Logs, Retries, Error Budgets</a>）。</li></ul><h3 id="16-4-API-与生态接入：Projects-Tasks-Webhooks-Slack-Zapier-Data-Sources"><a href="#16-4-API-与生态接入：Projects-Tasks-Webhooks-Slack-Zapier-Data-Sources" class="headerlink" title="16.4 API 与生态接入：Projects / Tasks / Webhooks + Slack / Zapier / Data Sources"></a>16.4 API 与生态接入：Projects / Tasks / Webhooks + Slack / Zapier / Data Sources</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_93.2dp6ujk1n5.webp" alt></p><p>除了网页 UI，Manus 也提供可程序化调用的 <a href="https://open.manus.im/docs/api-reference" target="_blank" rel="noopener">Manus API</a>：用 Project 管理默认配置与权限边界，用 Task 承载一次次“从输入到交付”的执行；附件与中间/最终工件以 File 形态沉淀，并可通过 <a href="https://open.manus.im/docs/integrations/webhooks" target="_blank" rel="noopener">Webhooks</a> 把状态变化与结果就绪推送回系统。这样 Manus 更像一个可嵌入业务流程的“交付引擎”，而不只是一个独立 App。</p><p>除了直接调用 API，Manus 还提供开箱即用的工作流集成，方便把触发与交付放进团队日常工具链（可参考 <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Integrations</a>）：</p><ul><li><a href="https://open.manus.im/docs/integrations/slack-integration" target="_blank" rel="noopener">Slack 集成</a>：团队安装后可在 Slack 聊天中直接 @ 调用 Manus 完成任务。Manus 能读取线程上下文和共享文件，根据指令执行任务，并把结果发回同一线程，供成员共同查看和调整。</li><li><a href="https://zapier.com/apps/manus/integrations/slack" target="_blank" rel="noopener">Zapier 集成</a>：通过 Zapier 连接大量应用，把 Manus 作为工作流中的智能节点。例如“表格新增记录 → 触发 Manus 汇总分析 → 把摘要发到 Slack”。</li><li><a href="https://open.manus.im/docs/integrations/data-sources" target="_blank" rel="noopener">数据源集成（Data Sources）</a>：在任务中直接获取实时外部数据（如财经市场、社交媒体趋势、内容平台热门信息等），开箱即用，无需用户单独管理 API 密钥。</li><li><a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">MCP Connectors</a>：通过 OAuth 授权连接 Gmail、Notion、Stripe、HubSpot、Google 日历、GitHub、Google Drive 等应用，也支持开发者编写自定义 MCP 接入，把内部工具纳入 Manus 的行动空间。</li></ul><h3 id="16-5-为什么值钱：交付闭环、可靠性治理、分发与护城河"><a href="#16-5-为什么值钱：交付闭环、可靠性治理、分发与护城河" class="headerlink" title="16.5 为什么值钱：交付闭环、可靠性治理、分发与护城河"></a>16.5 为什么值钱：交付闭环、可靠性治理、分发与护城河</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_94.lw7zn0or4.webp" alt></p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_95.491rn5wh8t.webp" alt></p><p>把 “\$2B” 的叙事拆成更可验证的变量，其实更像三件事：<strong>效果（能不能把事做成）</strong>、<strong>收入（有没有真实付费）</strong>、<strong>团队（能不能在应用层持续迭代并把领先变成系统）</strong>。</p><h4 id="16-5-1-效果：通用-Agent-断档领先"><a href="#16-5-1-效果：通用-Agent-断档领先" class="headerlink" title="16.5.1 效果：通用 Agent 断档领先"></a>16.5.1 效果：通用 Agent 断档领先</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_96.2rvmlesci3.webp" alt></p><p>在公开对比里，Manus 被描述为在 <strong>GAIA benchmark</strong>（面向真实任务的通用助手评测）上达到 SOTA，并在三档难度上超过 OpenAI Deep Research（见 <a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">Helicone 的 benchmark 对比</a>）。这类“真任务基准 + 端到端交付”的领先，是“断档领先”叙事最直接的依据：同样一句目标，系统更可能交付可用产物，而不是停在建议层。</p><h4 id="16-5-2-收入：可观-ARR（年化口径）"><a href="#16-5-2-收入：可观-ARR（年化口径）" class="headerlink" title="16.5.2 收入：可观 ARR（年化口径）"></a>16.5.2 收入：可观 ARR（年化口径）</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_97.39lo9ztq2y.webp" alt></p><p>相比“只有 demo、没有现金流”的产品，Manus 已经跑出可观的经常性收入量级。TechNode 报道其披露的 revenue run rate（RRR，年化口径）达到 <strong>\$90M</strong>，并指出其在 3 月上线付费服务（见 <a href="https://technode.com/2025/08/21/manus-ai-reports-90-million-annualized-revenue/" target="_blank" rel="noopener">Manus AI reports \$90 million annualized revenue</a>）。无论用 RRR 还是 ARR 来表述，这类指标的意义在于：价值主张足够强，用户愿意持续付费。</p><h4 id="16-5-3-团队：应用层能力（全球第一梯队）"><a href="#16-5-3-团队：应用层能力（全球第一梯队）" class="headerlink" title="16.5.3 团队：应用层能力（全球第一梯队）"></a>16.5.3 团队：应用层能力（全球第一梯队）</h4><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_98.54y92m65oo.webp" alt></p><p>模型可以被追平，但把 Agent 做成“可交付、可控、可嵌入”的应用层系统更难：上下文工程、工具/连接器、沙盒与权限、可观测与可靠性治理、以及把多触点交付融进协作工作流（见 16.3/16.4）。Manus 团队在这些环节有持续的工程化输出与方法论沉淀（例如 <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus" target="_blank" rel="noopener">Context Engineering for AI Agents: Lessons from Building Manus</a> 与 <a href="https://skywork.ai/blog/ai-agent/observability-manus-1-5-agents-best-practices/" target="_blank" rel="noopener">Observability for Manus 1.5 Agents: Logs, Retries, Error Budgets</a>），也因此更接近“全球第一梯队”的应用层能力：能把一次领先快速产品化、规模化，并在可靠性与生态上持续复利。</p><p>综上，Manus 的估值更像是“效果领先 × 经常性收入 × 应用层团队能力”的乘积，而不是单点模型的溢价。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_99.54y92m65on.webp" alt></p><hr><h2 id="17-Manus-Alternatives-agenticSeek-OpenHands-AgentGPT-UI-TARS-desktop"><a href="#17-Manus-Alternatives-agenticSeek-OpenHands-AgentGPT-UI-TARS-desktop" class="headerlink" title="17. Manus Alternatives: agenticSeek, OpenHands, AgentGPT, UI-TARS-desktop"></a>17. Manus Alternatives: agenticSeek, OpenHands, AgentGPT, UI-TARS-desktop</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_100.491rn5wh9k.webp" alt></p><p>如果把 Manus 理解为“通用任务的闭环交付 Agent”，那么开源/替代方案大多是在某个维度上做取舍：要么更强调本地隐私，要么更聚焦软件工程（SWE），要么专注 GUI/浏览器操作。下面给一个足够简洁的对比。</p><table><thead><tr><th>项目</th><th>形态/定位</th><th>强项</th><th>主要短板</th><th>适合谁</th><th>参考</th></tr></thead><tbody><tr><td>agenticSeek</td><td>本地优先的通用 Agent（定位为 “local Manus alternative”）</td><td>100% 本地运行与隐私、带 web browsing/coding、多 agent 规划</td><td>产品化与稳定性、生态/集成深度通常弱于商业产品</td><td>想要“Manus 方向 + 本地隐私”的个人/团队</td><td><a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener">https://github.com/Fosowl/agenticSeek</a></td></tr><tr><td>OpenHands</td><td>面向软件开发的 Agent（SDK/CLI/GUI/Cloud）</td><td>工程化的 SWE 流程、CLI/GUI 都齐、公开 benchmark（如 SWEBench）</td><td>主要聚焦开发任务，不是“通用办公+跨 SaaS 交付”</td><td>把 Agent 当“开源 Devin/Jules”用来写代码/改仓库</td><td><a href="https://github.com/OpenHands/OpenHands" target="_blank" rel="noopener">https://github.com/OpenHands/OpenHands</a></td></tr><tr><td>AgentGPT</td><td>浏览器里配置/运行自治 Agent（偏玩法与演示）</td><td>上手快、可视化地跑“任务→子任务”循环</td><td>能力边界偏早期 AutoGPT 路线，闭环交付/治理能力有限</td><td>想快速体验/教学演示“自治 Agent 是怎么跑的”</td><td><a href="https://github.com/reworkd/AgentGPT" target="_blank" rel="noopener">https://github.com/reworkd/AgentGPT</a></td></tr><tr><td>UI-TARS-desktop</td><td>桌面端 GUI Agent（本地/远程 computer &amp; browser operator）</td><td>多模态 UI 操作能力强，适合“像人一样点点点”</td><td>重点在 GUI/浏览器操作，不等同于通用交付型 Agent</td><td>需要 UI 自动化/浏览器 Operator 的场景</td><td><a href="https://github.com/bytedance/UI-TARS-desktop" target="_blank" rel="noopener">https://github.com/bytedance/UI-TARS-desktop</a></td></tr></tbody></table><p>补充一条务实信息：开源协议与自托管限制也会影响选型——<code>agenticSeek</code>/<code>AgentGPT</code> 为 GPL-3.0，<code>UI-TARS-desktop</code> 为 Apache-2.0，<code>OpenHands</code> 核心为 MIT（<code>enterprise/</code> 目录单独授权）。</p><p>一个朴素的选型法：</p><ul><li>要“本地隐私的通用 Agent”→ <code>agenticSeek</code></li><li>要“开源软件工程 Agent（写代码/修 bug/改仓库）”→ <code>OpenHands</code></li><li>要“UI/浏览器 Operator（GUI 自动化）”→ <code>UI-TARS-desktop</code></li><li>要“快速演示自治循环”→ <code>AgentGPT</code></li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_101.et047ejc6.webp" alt></p><hr><h2 id="18-总结"><a href="#18-总结" class="headerlink" title="18. 总结"></a>18. 总结</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_102.9kgo7vijxh.webp" alt></p><p>从模型视角看，LLM 仍然是 next-token predictor：它擅长在给定上下文里“生成最可能的下一句”，但不天然具备长期记忆、稳定执行与对外部事实的强约束。Chatbot 把它包装成对话产品，而 Agent 则把它放进一个可运行的执行系统里——用工具获得 grounding，用循环与状态跑长任务，用评测与可观测把不确定性纳入治理。</p><p>本文的主线可以压缩成三句话：</p><ol><li><strong>把推理落到现实</strong>：从 Tool Calling/MCP，到 ReAct 的 Thought/Action/Observation，再到 DeepResearch 的分层编排与证据体系，核心都是让“结论有来源、行动可回填”，用可验证的 Observation 对抗 hallucination。</li><li><strong>把长任务做成系统</strong>：Plan + Agent Loop 把目标拆成可推进步骤；State/Checkpoint/Resume 让执行可恢复、可回放、可审计；上下文工程用 working set、外置记忆与子任务隔离对抗 context overflow 与噪声传播。</li><li><strong>把不确定性变成可运营</strong>：权限与 sandbox 收束副作用；最小验证闭环（tests/校验器）让“完成”有可执行定义；离线 golden tasks + tracing 把迭代变成可回归的工程流程，把上线变成可监控的系统。</li></ol><p>落到工程实践，Agent 的“公式”大致是：</p><ul><li><strong>Policy（LLM）</strong>：负责决策与生成候选方案；</li><li><strong>Runtime（Orchestrator/Loop）</strong>：负责循环、预算、停止条件与重规划；</li><li><strong>Tools &amp; Environment</strong>：负责真实世界 I/O 与可验证证据；</li><li><strong>State &amp; Memory</strong>：负责持久化、工件与跨步骤连续性；</li><li><strong>Governance（permissions/observability/evals）</strong>：负责可控、可审计与可运营。</li></ul><p>最后回到“交付形态”：LangChain/LangGraph 提供构建原语与可恢复运行时，高层框架与 flow 平台提供更快的工程入口；CLI Agent 把“工作区 + 工具 + 权限”变成默认执行环境；而 Manus 这类产品进一步证明，护城河往往在应用层系统能力（上下文工程、工具生态、可靠性与协作交付），而不是某个单点模型。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_103.3yexu0h943.webp" alt></p><h2 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h2><ul><li>[1] <a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener">Function calling | OpenAI API</a></li><li>[2] <a href="https://platform.openai.com/docs/guides/structured-outputs/key-ordering?api-mode=chat" target="_blank" rel="noopener">Structured model outputs | OpenAI API</a></li><li>[3] <a href="https://developers.openai.com/apps-sdk/concepts/mcp-server" target="_blank" rel="noopener">Model Context Protocol (MCP) | OpenAI Developers</a></li><li>[4] <a href="https://platform.openai.com/docs/guides/tools-connectors-mcp" target="_blank" rel="noopener">Connectors and MCP servers | OpenAI API</a></li><li>[5] <a href="https://platform.openai.com/docs/api-reference/completions" target="_blank" rel="noopener">Completions | OpenAI API Reference</a></li><li>[6] <a href="https://platform.openai.com/docs/api-reference/chat?locale=en" target="_blank" rel="noopener">Chat Completions | OpenAI API Reference</a></li><li>[7] <a href="https://openai.com/index/function-calling-and-other-api-updates/" target="_blank" rel="noopener">Function calling and other API updates - OpenAI</a></li><li>[8] <a href="https://platform.openai.com/docs/assistants/migration" target="_blank" rel="noopener">Assistants migration guide | OpenAI API</a></li><li>[9] <a href="https://platform.openai.com/docs/deprecations" target="_blank" rel="noopener">Deprecations | OpenAI API</a></li><li>[10] <a href="https://developers.openai.com/blog/responses-api" target="_blank" rel="noopener">Why we built the Responses API</a></li><li>[11] <a href="https://platform.openai.com/docs/guides/migrate-to-responses" target="_blank" rel="noopener">Migrate to the Responses API | OpenAI API</a></li><li>[12] <a href="https://platform.openai.com/docs/guides/reasoning" target="_blank" rel="noopener">Reasoning models | OpenAI API</a></li><li>[13] <a href="https://platform.openai.com/docs/guides/conversation-state" target="_blank" rel="noopener">Conversation state | OpenAI API</a></li><li>[14] <a href="https://github.com/openai/completions-responses-migration-pack" target="_blank" rel="noopener">completions-responses-migration-pack | GitHub</a></li><li>[15] <a href="https://www.datacamp.com/tutorial/openai-responses-api" target="_blank" rel="noopener">OpenAI Responses API: The Ultimate Developer Guide | DataCamp</a></li><li>[16] <a href="https://open.manus.im/docs/integrations/manus-api" target="_blank" rel="noopener">Manus API - Manus Documentation</a></li><li>[17] <a href="https://open.manus.im/docs/api-reference" target="_blank" rel="noopener">Overview - Manus API</a></li><li>[18] <a href="https://open.manus.im/docs/features/browser-operator" target="_blank" rel="noopener">Browser Operator - Manus Documentation</a></li><li>[19] <a href="https://www.gadgets360.com/ai/news/meta-acquisition-manus-ai-agent-developer-fifth-deal-2025-10129781" target="_blank" rel="noopener">Meta Acquires Autonomous Agent Developer Manus AI, Marks Its Fifth Deal in 2025 | Gadgets 360</a></li><li>[20] <a href="https://aimagazine.com/news/how-manus-puts-meta-ahead-in-the-agentic-ai-economy" target="_blank" rel="noopener">Inside Meta’s Groundbreaking Acquisition of Manus | AI Magazine</a></li><li>[21] <a href="https://www.techradar.com/pro/meta-buys-manus-for-usd2-billion-to-power-high-stakes-ai-agent-race" target="_blank" rel="noopener">Meta buys Manus for $2 billion to power high-stakes AI agent race | TechRadar</a></li><li>[22] <a href="https://www.allaboutai.com/ai-news/what-meta-acquisition-of-manus-means-for-the-future-of-ai-agents" target="_blank" rel="noopener">What Meta’s Acquisition of Manus Means for the Future of AI Agents | All About AI</a></li><li>[23] <a href="https://en.wikipedia.org/wiki/Manus_%28AI_agent%29" target="_blank" rel="noopener">Manus (AI agent) | Wikipedia</a></li><li>[24] <a href="https://github.com/langchain-ai/deepagents" target="_blank" rel="noopener">GitHub - langchain-ai/deepagents</a></li><li>[25] <a href="https://docs.langchain.com/oss/python/deepagents/cli" target="_blank" rel="noopener">Deep Agents CLI | LangChain Docs</a></li><li>[26] <a href="https://docs.langchain.com/oss/python/deepagents/overview" target="_blank" rel="noopener">Deep Agents overview | LangChain Docs</a></li><li>[27] <a href="https://github.com/langchain-ai/deepagents-quickstarts" target="_blank" rel="noopener">GitHub - langchain-ai/deepagents-quickstarts</a></li><li>[28] <a href="https://github.com/agno-agi/agno" target="_blank" rel="noopener">GitHub - agno-agi/agno</a></li><li>[29] <a href="https://deepwiki.com/agno-agi/agno-docs" target="_blank" rel="noopener">agno-agi/agno-docs | DeepWiki</a></li><li>[30] <a href="https://deepwiki.com/agno-agi/agno-docs/5-examples-and-applications" target="_blank" rel="noopener">Examples and Applications | agno-agi/agno-docs | DeepWiki</a></li><li>[31] <a href="https://docs.agno.com/examples/use-cases/agents/deep_research_agent_exa" target="_blank" rel="noopener">Deep Research Agent - Agno</a></li><li>[32] <a href="https://github.com/shareAI-lab/learn-claude-code" target="_blank" rel="noopener">GitHub - shareAI-lab/learn-claude-code</a></li><li>[33] <a href="https://www.baytechconsulting.com/blog/manus-ai-an-analytical-guide-to-the-autonomous-ai-agent-2025" target="_blank" rel="noopener">Manus AI: An Analytical Guide to the Autonomous AI Agent 2025</a></li><li>[34] <a href="https://manus.im/tools" target="_blank" rel="noopener">Manus AI Agent Toolkit for Delivering Work</a></li><li>[35] <a href="https://manus.im/zh-cn/tools" target="_blank" rel="noopener">用于交付工作的 Manus AI Agent 工具包</a></li><li>[36] <a href="https://www.helicone.ai/blog/manus-benchmark-operator-comparison" target="_blank" rel="noopener">What is Manus AI? Benchmarks &amp; How it Compares to Operator and Computer Use</a></li><li>[37] <a href="https://medium.com/@jalajagr/inside-manus-the-anatomy-of-an-autonomous-ai-agent-b3042e5e5084" target="_blank" rel="noopener">Inside Manus: The Anatomy of an Autonomous AI Agent | by Jalaj Agrawal | Medium</a></li><li>[38] <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Integrate Manus with Your Existing Tools - Manus Documentation</a></li><li>[39] <a href="https://open.manus.im/docs/features/cloud-browser" target="_blank" rel="noopener">Cloud browser - Manus Documentation</a></li><li>[40] <a href="https://open.manus.im/docs/features/mail-manus" target="_blank" rel="noopener">Mail Manus - Manus Documentation</a></li><li>[41] <a href="https://open.manus.im/docs/features/collab" target="_blank" rel="noopener">Manus Collab - Manus Documentation</a></li><li>[42] <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus" target="_blank" rel="noopener">Context Engineering for AI Agents: Lessons from Building Manus</a></li><li>[43] <a href="https://open.manus.im/docs/api-reference" target="_blank" rel="noopener">Overview - Manus API (API Reference)</a></li><li>[44] <a href="https://natesnewsletter.substack.com/p/i-read-everything-google-anthropic" target="_blank" rel="noopener">Get the Cheat Code on Long-Running AI Agents—Here’s What Manus, Google, and Anthropic Learned After Trial and Error + 12 Prompts to Help Build Long-Running Agents Yourself</a></li><li>[45] <a href="https://open.manus.im/docs/integrations/slack-integration" target="_blank" rel="noopener">Slack Integration - Manus Documentation</a></li><li>[46] <a href="https://open.manus.im/docs/integrations/integrations" target="_blank" rel="noopener">Overview - Manus API (Integrations)</a></li><li>[47] <a href="https://skywork.ai/blog/ai-agent/observability-manus-1-5-agents-best-practices/" target="_blank" rel="noopener">Observability for Manus 1.5 Agents: Logs, Retries, Error Budgets</a></li><li>[48] <a href="https://open.manus.im/docs/integrations/webhooks" target="_blank" rel="noopener">Webhooks - Manus API</a></li><li>[49] <a href="https://zapier.com/apps/manus/integrations/slack" target="_blank" rel="noopener">Manus Slack Integration - Quick Connect - Zapier</a></li><li>[50] <a href="https://open.manus.im/docs/integrations/data-sources" target="_blank" rel="noopener">Data Sources - Manus Documentation</a></li><li>[51] <a href="https://arxiv.org/abs/2305.04091" target="_blank" rel="noopener">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models | arXiv</a></li><li>[52] <a href="https://arxiv.org/abs/2305.18323" target="_blank" rel="noopener">ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models | arXiv</a></li><li>[53] <a href="https://arxiv.org/abs/2303.11366" target="_blank" rel="noopener">Reflexion: Language Agents with Verbal Reinforcement Learning | arXiv</a></li><li>[54] <a href="https://www.anthropic.com/news/model-context-protocol" target="_blank" rel="noopener">Model Context Protocol (MCP) - Anthropic</a></li><li>[55] <a href="https://github.com/modelcontextprotocol/specification" target="_blank" rel="noopener">modelcontextprotocol/specification | GitHub</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;上篇我们把“Agent 从哪来”讲清楚了：LLM/Chatbot 的结构性局限决定了它“会答”不等于“会做事”，而 Tool Calling
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Agent" scheme="https://xhinliang.github.io/tags/Agent/"/>
    
      <category term="AI" scheme="https://xhinliang.github.io/tags/AI/"/>
    
      <category term="LangGraph" scheme="https://xhinliang.github.io/tags/LangGraph/"/>
    
      <category term="LangChain" scheme="https://xhinliang.github.io/tags/LangChain/"/>
    
      <category term="ClaudeCode" scheme="https://xhinliang.github.io/tags/ClaudeCode/"/>
    
      <category term="Manus" scheme="https://xhinliang.github.io/tags/Manus/"/>
    
  </entry>
  
  <entry>
    <title>从 LLM 到 Agent（上）：原理、局限与演进脉络</title>
    <link href="https://xhinliang.github.io/2026/02/backend/from-llm-to-agent-part1/"/>
    <id>https://xhinliang.github.io/2026/02/backend/from-llm-to-agent-part1/</id>
    <published>2026-02-25T11:39:16.000Z</published>
    <updated>2026-02-25T07:09:26.607Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2025 年结束，Agent 这个概念在所有科技行业从业者中每天都会听到 10 次以上。那到底什么是 Agent，为什么有了 LLM 不够，还要做 Agent，我们是怎么从 LLM 演进到 Agent 的？</p><p>本文尝试从下面两个方面解释清楚上面的问题：</p><ul><li>理解 LLM/Chatbot 的结构性局限：为什么“会答”不等于“会做事”</li><li>搞清 Tool Calling / MCP / ReAct / DeepResearch 的演化脉络：推理如何与证据、行动与产物形成闭环</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_1.mkdc68kf.webp" alt></p><p>本文 @coauthor: ChatGPT DeepResearch, Codex, Manus, AnyGen, Nano Banana </p><h2 id="0-背景简介"><a href="#0-背景简介" class="headerlink" title="0. 背景简介"></a>0. 背景简介</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_2.9ddgcfweku.webp" alt></p><p>近两年，LLM 的“能对话”已被验证，但把它变成“能完成复杂任务的执行系统”仍是工程难题。本文试图从模型原理出发，解释 Chatbot 的结构性局限，并梳理从 Tool Calling、MCP 到 Agent 框架与运行时的演化脉络，最终落到工程实践的关键能力：状态、持久化、上下文工程、可观测与可控执行。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_3.39lo9ztq6d.webp" alt></p><h2 id="1-LLM-的工作原理：token-预测"><a href="#1-LLM-的工作原理：token-预测" class="headerlink" title="1. LLM 的工作原理：token 预测"></a>1. LLM 的工作原理：token 预测</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_4.5trimmtosk.webp" alt></p><p>大语言模型的核心是 <strong>next-token prediction</strong>：给定上下文 token 序列，输出下一个 token 的概率分布，并以此自回归生成后续文本。该机制决定了两个关键事实：</p><ol><li>模型输出本质上是<strong>条件概率采样</strong>的结果；</li><li>模型“理解/推理”能力在接口层面表现为对概率分布的调制，而非显式执行符号规则。</li></ol><p>推理阶段的关键组成：</p><ul><li><strong>Context window</strong>：模型可“看到”的输入长度上限；上下文越长并不必然越好，过长会导致注意力稀释、相关信息被淹没。</li><li><strong>KV cache</strong>：自回归生成中复用注意力的 key/value，以降低重复计算成本并提升吞吐。</li><li><strong>Prompt / System prompt</strong>：通过指令与约束塑造角色、风格与边界；系统提示词在对齐层面通常优先级更高。</li><li><strong>Instruction tuning</strong>：在预训练之后进行监督微调（SFT）以提升指令遵循；再通过 <strong>RLHF / DPO</strong> 等方法进行偏好对齐（alignment），降低有害输出并改善可用性。</li><li><strong>幻觉（hallucination）与不确定性</strong>：模型会生成“高似然但不保证真实”的内容；其不确定性来自数据覆盖、上下文缺失以及采样策略，概率分布并不等价于可校准的事实置信度。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_5.4g4zilimrn.webp" alt></p><h2 id="2-Chatbot-的局限性"><a href="#2-Chatbot-的局限性" class="headerlink" title="2. Chatbot 的局限性"></a>2. Chatbot 的局限性</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_6.45i5pg3ema.webp" alt></p><p>仅靠一次或少数几次模型调用，Chatbot 在长流程任务上存在结构性瓶颈，典型体现在：</p><ul><li><strong>Hallucination / grounding</strong>：模型缺少对外部事实的强约束；没有检索与校验时，容易用“合理文本”填补空白。</li><li><strong>Non-determinism</strong>：温度采样、并行计算与外部工具结果都会带来波动；对生产流程而言，这意味着输出不稳定与回归困难。</li><li><strong>Context overflow / context saturation</strong>：上下文有限且“越长越贵”；当历史对话、文档与中间产物不断累积，模型会出现注意力分散、遗漏关键约束等问题。</li><li><strong>Long-term memory 缺失</strong>：模型调用天然是“无持久记忆”的；跨天/跨会话任务需要外部记忆体（数据库、向量库、文件系统等）。</li><li><strong>Long-horizon error accumulation</strong>：多步骤任务中，早期小错误会被后续步骤放大；纯文本链式推理缺少强校验机制。</li><li><strong>Tool reliability 与 I/O 边界</strong>：真实世界 I/O 存在权限、网络、速率限制、格式变更与失败重试；模型本身无法保证这些边界条件被正确处理。</li><li><strong>Statefulness vs stateless</strong>：模型单次调用是无状态的；而任务执行需要状态机（进度、依赖、产物、失败恢复）。</li><li><strong>规划能力/执行能力割裂</strong>：模型可以写出漂亮计划，但在执行时容易忽略约束、误用工具或无法闭环验证。</li><li><strong>观察-行动闭环（observe-act loop）缺失</strong>：没有“执行→观察结果→修正策略”的闭环时，模型难以稳定完成需要反馈迭代的任务。</li></ul><p>举个例子：在服务器上配置 Nginx 时，若依赖 Chatbot 协助，通常需要手动把现有配置复制粘贴到对话里，再让模型给出下一步命令；执行后把输出贴回，模型再继续判断。整个过程是“人手工搬运上下文 + 执行 + 反馈”的反复循环。</p><p>Agent 的价值在于把上述瓶颈外置为工程可控的系统能力：</p><ul><li>用工具获得<strong>grounding</strong>（检索/数据库/计算/运行代码/读写文件）；</li><li>用状态与循环控制长任务（step budget、检查点、恢复）；</li><li>用人类审核兜底高风险动作（human-in-the-loop）；</li><li>用日志与评测把不确定性纳入治理（可观测性与回归测试）。</li></ul><p>继续用 Nginx 的例子：若使用 Agent，它可以主动读取配置文件与服务状态，先给出修改计划并请求人工审批；随后按计划执行命令、落盘、测试与 reload。中间若报错（语法校验失败、端口冲突），它能基于真实输出定位问题并迭代修复。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_7.3k8i358ybl.webp" alt></p><h2 id="3-Tool-Calling"><a href="#3-Tool-Calling" class="headerlink" title="3. Tool Calling"></a>3. Tool Calling</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_8.3yexu0h96o.webp" alt></p><p>我们回到 Chatbot。在 2022 年的 Chatbot 中，Chatbot 不知道当前时间，不知道现在的天气，也很难计算 123456 * 987654，也很难数清楚 “strawberry” 里有几个 r。<br>为了解决这个问题，业界就想出了 Tool Calling 的方案，这个方案初期是由 OpenAI 实现的 Function Calling，后续集成到各种 LLM 基础模型中，最后演变成现在的 MCP 方案。</p><h3 id="3-1-Function-Call-模型感知能力"><a href="#3-1-Function-Call-模型感知能力" class="headerlink" title="3.1 Function Call: 模型感知能力"></a>3.1 Function Call: 模型感知能力</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_9.5c1gy1sb7j.webp" alt></p><p>OpenAI 于 2023 年发布 Function Calling 能力：开发者以 JSON schema 描述函数签名，模型可输出可执行的参数结构，由系统调用真实工具（数据查询、业务 API、计算等），再将工具结果回填继续生成。([OpenAI][7])</p><p><strong>Function Calling</strong> 把“动作”从文本里抽离出来，成为可校验、可执行的结构化输出：模型决定“是否调用、调用哪个工具、参数是什么”，系统负责执行与回填结果，再由模型整合为最终响应。([OpenAI Platform][1])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">tools = [&#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"function"</span>,</span><br><span class="line">  <span class="string">"function"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"get_weather"</span>,</span><br><span class="line">    <span class="string">"description"</span>: <span class="string">"查询某城市天气（示例工具）"</span>,</span><br><span class="line">    <span class="string">"parameters"</span>: &#123;&#125;, <span class="comment"># ...</span></span><br><span class="line">  &#125;,</span><br><span class="line">&#125;]</span><br><span class="line"></span><br><span class="line">r = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">"gpt-4.1-mini"</span>,</span><br><span class="line">  messages=[&#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"查一下新加坡天气。"</span>&#125;],</span><br><span class="line">  tools=tools,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tc = r.choices[<span class="number">0</span>].message.tool_calls[<span class="number">0</span>]</span><br><span class="line">args = parse_json(tc.function.arguments)  <span class="comment"># =&gt; &#123;"city":"新加坡"&#125;</span></span><br><span class="line"></span><br><span class="line">tool_result = &#123;<span class="string">"city"</span>: args[<span class="string">"city"</span>], <span class="string">"temp_c"</span>: <span class="number">31</span>&#125;</span><br><span class="line"></span><br><span class="line">r2 = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">"gpt-4.1-mini"</span>,</span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"查一下新加坡天气。"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"role"</span>: <span class="string">"tool"</span>, <span class="string">"tool_call_id"</span>: tc.id, <span class="string">"content"</span>: to_json(tool_result)&#125;,</span><br><span class="line">  ],</span><br><span class="line">)</span><br><span class="line">print(r2.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><p>通过 Function Call，Chatbot 初步具备了跟外界交互的能力，此时只要 Prompt 写得还不错，上述的时间、天气等问题可以得到很好地解决。</p><h3 id="3-2-MCP：把工具生态标准化"><a href="#3-2-MCP：把工具生态标准化" class="headerlink" title="3.2 MCP：把工具生态标准化"></a>3.2 MCP：把工具生态标准化</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_10.5c1gy1sb8e.webp" alt></p><p>在 OpenAI 的 Function Call 取得初步成功后，各家 LLM 和 LLM 框架迅速跟进，支持了类似 Function Call 的能力。<br>在各家 LLM 基础模型中，Function Call 的实现思路一致，但接口各有差别。这就导致工具生态被“各家 SDK 语义”割裂：schema 表达、调用返回格式、流式事件、错误处理都不统一，开发者需要为不同厂商重复适配。</p><p>2024 年 11 月，Anthropic 结合自己对 Function Call 的理解，设计并开源了 MCP 的方案。这个方案把“模型 ↔ 工具/资源”的交互抽象为统一协议，通过标准化的 server、工具定义与资源访问，降低多模型/多工具集成成本，并让工具生态具备可移植性。</p><p>需要区分的是：MCP 是 Anthropic 提出的协议；OpenAI 等平台对 MCP 的支持，属于“在自身工具体系中接入/托管 MCP server”的实现方式（如 remote MCP servers / connectors）。([Anthropic][54]; [GitHub][55]; [OpenAI Developers][3])</p><p><strong>MCP（Model Context Protocol）</strong>旨在标准化“模型如何连接外部工具与资源”。MCP server 对外暴露可调用工具（含 schema），并返回结构化结果；在 OpenAI 平台中，开发者可通过 <strong>remote MCP servers</strong> 或 OpenAI 维护的 <strong>connectors</strong> 赋予模型新的能力。([OpenAI Developers][3])<br>在工具治理上，MCP/connector 体系强调：工具调用可设置为自动允许或需要显式审批，以满足安全与合规要求。([OpenAI Platform][4])</p><p>下面用“查询当地天气”的极简示例说明 MCP 的用法：MCP server 暴露 <code>get_weather</code> 工具，模型决定是否调用，运行时负责执行并把结果回填。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mcp <span class="keyword">import</span> Client</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">mcp = Client(<span class="string">"http://localhost:8080"</span>)   <span class="comment"># 本地天气 MCP server</span></span><br><span class="line">tools = mcp.list_tools()                <span class="comment"># =&gt; [&#123;"name": "get_weather", ...&#125;]</span></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line">r = client.responses.create(</span><br><span class="line">  model=<span class="string">"gpt-4.1-mini"</span>,</span><br><span class="line">  input=<span class="string">"查一下我当地天气"</span>,</span><br><span class="line">  tools=tools,                           <span class="comment"># 让模型可用 MCP 工具</span></span><br><span class="line">)</span><br><span class="line">tc = r.output[<span class="number">0</span>]                         <span class="comment"># 运行时解析出 tool call</span></span><br><span class="line">weather = mcp.call_tool(tc.name, tc.arguments)  <span class="comment"># 实际调用 MCP server</span></span><br><span class="line">final = client.responses.create(</span><br><span class="line">  model=<span class="string">"gpt-4.1-mini"</span>,</span><br><span class="line">  input=[&#123;<span class="string">"role"</span>: <span class="string">"tool"</span>, <span class="string">"content"</span>: weather&#125;],</span><br><span class="line">)</span><br><span class="line">print(final.output_text)</span><br></pre></td></tr></table></figure><h3 id="3-3-CoT-amp-Thinking-Model"><a href="#3-3-CoT-amp-Thinking-Model" class="headerlink" title="3.3 CoT &amp; Thinking Model"></a>3.3 CoT &amp; Thinking Model</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_11.4g4zilimsd.webp" alt></p><p>在 LLM 的早期阶段，提升推理能力的常见做法是用 CoT（Chain-of-Thought）提示词“扶一把”：在问题前加一句 “Think step by step / 逐步思考”，或给出带推理过程的示例，让模型把中间步骤展开，从而显著提升数学、逻辑与多步问题的正确率。这个阶段的改进主要依赖提示工程与输出格式控制，效果可观但稳定性有限。</p><p>后来 OpenAI 干脆把这个 CoT 的能力内置到 o1 模型中，然后把这个模型称为 Thinking Model 或者 Reasoning Model。</p><p>Thinking Model 使得模型不只是“会答”，而是把更多计算预算花在推理过程上，通过更长的内部思考链与过程监督（process supervision）提升复杂任务的正确率与稳健性。由此衍生出“Thinking Model”的基本概念：在推理时显式分配思考预算，模型倾向于先规划再验证，必要时调用工具完成外部校验，但对外仅输出精炼结论与可审计的中间结果。</p><p>这套思路很快被行业吸收并扩散。DeepSeek-R1 以“推理优先”的训练路径与蒸馏版本打开了开源推理模型的供给，随后各家厂商陆续发布带 “Reasoning/Think” 标签的模型族系，强调在数学、编程、规划与多步工具调用上的可靠性。整体趋势是：<strong>从参数规模扩展转向“推理时算力扩展”</strong>，以更高的推理预算换取更高的任务成功率；同时也带来延迟与成本的权衡，因此生产系统常会引入推理路由与预算控制。</p><p>下面是当前 OpenAI API 的标准调用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">r = client.responses.create(</span><br><span class="line">  model=<span class="string">"gpt-5.1"</span>,</span><br><span class="line">  input=<span class="string">"用 3 条 bullet 总结一下 OpenAI Responses API 的核心变化，并给出一个使用场景。"</span>,</span><br><span class="line">  reasoning=&#123;<span class="string">"effort"</span>: <span class="string">"medium"</span>&#125;,</span><br><span class="line">  tools=[&#123;<span class="string">"type"</span>: <span class="string">"web_search"</span>&#125;],  <span class="comment"># 需要时模型会自己决定是否调用</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>此时不难发现，Thinking Model 虽然最终输出结果还是跟 Chatbot 类似的文本，但其已经基本具备了 Agent 的基本形态和能力。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_12.6f168xo53w.webp" alt></p><h2 id="4-从-ReAct-到-DeepResearch：Agent-思维范式的演化"><a href="#4-从-ReAct-到-DeepResearch：Agent-思维范式的演化" class="headerlink" title="4. 从 ReAct 到 DeepResearch：Agent 思维范式的演化"></a>4. 从 ReAct 到 DeepResearch：Agent 思维范式的演化</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_13.3gow5ffvmj.webp" alt></p><p>如果说 Thinking Model 让模型“更会思考”，那么 ReAct 让模型“会做事”，而 DeepResearch 则把这套能力扩展成“可交付的研究系统”。这条演化路径的核心，是让推理与证据、行动与产物形成闭环。</p><h3 id="4-1-ReAct：把推理与行动打通的最小闭环"><a href="#4-1-ReAct：把推理与行动打通的最小闭环" class="headerlink" title="4.1 ReAct：把推理与行动打通的最小闭环"></a>4.1 ReAct：把推理与行动打通的最小闭环</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_14.5fl2vrldy2.webp" alt></p><p>ReAct（Yao et al., 2022）将推理（Reasoning）与行动（Acting）交织：在每轮中显式组织 <strong>Thought / Action / Observation</strong>，用工具获得外部证据，再基于观察继续推进。它强调“先行动获取事实，再基于事实推理”，工程价值主要体现在：</p><ul><li>将“推理链”拆成可执行片段，天然适配工具调用与错误恢复；</li><li>以 Observation 作为对抗 hallucination 的基本单元；</li><li>以轨迹（trajectory）形式沉淀过程数据，便于评测与回放。</li></ul><p>ReAct 的短板也很明确：它仍是单体循环，容易被长上下文拖垮；当任务跨度变大、证据来源增多时，需要更强的编排与证据治理能力，这正是 DeepResearch 的切入点。</p><h3 id="4-2-DeepResearch"><a href="#4-2-DeepResearch" class="headerlink" title="4.2 DeepResearch"></a>4.2 DeepResearch</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_15.3k8i358yc8.webp" alt></p><p>与 Thinking Model / ReAct 相比，DeepResearch 更接近“分层研究系统”，而非单一对话。它不是模型能力的单点提升，而是一套面向研究交付的运行时方法论，典型流程包括：</p><ul><li><strong>问题分解与研究计划</strong>：明确范围、问题边界与验收标准，再拆成可并行的子问题。</li><li><strong>分层编排与并行执行</strong>：用 orchestrator 下发任务，由 sub-agent 进行检索、实验或数据整理。</li><li><strong>Context quarantine</strong>：对子代理上下文做隔离，避免噪声与错误传播；汇总阶段做证据对齐与一致性检查。</li><li><strong>Source attribution</strong>：将“结论—证据—来源”绑定，形成可审计的引用链。</li><li><strong>File-system as memory</strong>：用工作区承载中间产物与数据集，降低对单次上下文的依赖。</li><li><strong>Report synthesis</strong>：以“报告 + 附件数据”为目标输出，而非聊天式回复。</li></ul><p>对比 Non-Thinking Model、Thinking Model、ReAct、DeepResearch：</p><table><thead><tr><th>维度</th><th>Non-Thinking Model</th><th>Thinking Model</th><th>ReAct</th><th>DeepResearch</th></tr></thead><tbody><tr><td>目标</td><td>基础对话与文本生成</td><td>更强的单体推理</td><td>可靠执行与工具闭环</td><td>研究级交付与证据体系</td></tr><tr><td>结构</td><td>单轮生成</td><td>单模型思考</td><td>思考-行动-观察循环</td><td>分层编排 + 多子代理</td></tr><tr><td>证据</td><td>无显式证据</td><td>可选调用工具</td><td>Observation 驱动</td><td>结论-证据-来源绑定</td></tr><tr><td>上下文</td><td>单上下文、短对话</td><td>单上下文增长</td><td>逐步追加</td><td>隔离 + 汇总</td></tr><tr><td>产物</td><td>文本回答</td><td>精炼答案</td><td>过程结论 + 工具结果</td><td>报告 + 附件数据</td></tr><tr><td>适用场景</td><td>写作/FAQ</td><td>复杂推理题</td><td>多步执行任务</td><td>长周期研究/情报汇总</td></tr></tbody></table><p>相关实现可参考 Agno 的 Deep Research Agent 示例。([docs.agno.com][31])</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_16.8ok6sf8vkn.webp" alt></p><h2 id="5-混沌初期的试探：AutoGPT"><a href="#5-混沌初期的试探：AutoGPT" class="headerlink" title="5. 混沌初期的试探：AutoGPT"></a>5. 混沌初期的试探：AutoGPT</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_17.6bhkb7v2du.webp" alt></p><p>从实现层面看，上面介绍的 DeepResearch 已具备工业级 Agent 的关键结构。但从用户体验看，它的交付形态偏单一（主要是一份报告）。日常工作往往需要“能动手”的系统，因此我们关注更狭义的通用 Agent：除了产出文字，还能改文件、跑命令、调用工具完成任务。</p><p>为了简化表述，后文的“Agent”默认指通用 Agent。</p><p>在 2023 年，随着 ChatGPT 引爆公众关注，社区很快冒出了一个『奇葩』项目 AutoGPT。它把 LLM 包装成一个“自驱执行体”：设定目标、生成计划、调用工具、写入记忆，再继续循环。示例里常见的能力包括搜索、写文、发 Twitter 等，给人的直观感受是“模型开始自己动手了”。</p><p>但 AutoGPT 的本质不是算法突破，而是一种工程式拼装：Prompt 结构化 + 工具调用 + 简易记忆。它让人第一次看到“Agent Loop”的雏形，却也暴露了早期的混沌：计划质量不稳定、状态与记忆缺乏约束、任务容易发散并陷入错误循环，成本与结果不可预期。</p><p>即便如此，AutoGPT 仍是重要的里程碑：它点燃了开源 Agent 的浪潮，让“自主循环”成为可讨论的工程形态，也让人们开始认真思考状态管理、预算控制、工具可靠性等后续必须解决的问题。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_18.96a8h0a95d.webp" alt></p><h2 id="6-Plan-and-Solve-Reflection：把“隐式思考”变成“显式控制”"><a href="#6-Plan-and-Solve-Reflection：把“隐式思考”变成“显式控制”" class="headerlink" title="6. Plan-and-Solve / Reflection：把“隐式思考”变成“显式控制”"></a>6. Plan-and-Solve / Reflection：把“隐式思考”变成“显式控制”</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_19.96a8h0a95c.webp" alt></p><p>在 ReAct 架构之外，人们在 Agent 开发层面还有一些别的探索，我们以 Plan-and-Solve 和 Plan-and-Execute 举例。</p><p>值得说明的是，Plan-and-Solve / Plan-and-Execute / ReAct 并不是互斥的关系，在业界实践中经常是互补的关系。工程师们经常会在 Agent Loop （后文中会提到）中混合使用这几种架构，以优化不同的任务的 Agent 效果。</p><h3 id="6-1-Plan-and-Solve-Plan-and-Execute"><a href="#6-1-Plan-and-Solve-Plan-and-Execute" class="headerlink" title="6.1 Plan-and-Solve / Plan-and-Execute"></a>6.1 Plan-and-Solve / Plan-and-Execute</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_20.6bhkb7v2dp.webp" alt></p><p>Plan-and-Solve 这类“先规划、后求解”的套路，最早以 prompting 形式被系统化提出，用来改善 zero-shot CoT 的稳定性与正确率。([arxiv.org][51]) 在 Agent 工程里，同样思路常被扩展为 Plan-and-Execute：计划不只描述推理步骤，还要显式包含工具、输入输出与验收方式，便于执行器逐步落地并回填观察；更结构化的分层实现可参考将“推理计划”与“工具观察”解耦的 ReWOO。([arxiv.org][52])<br>Plan-and-Solve 通过显式计划降低长任务漂移：</p><ul><li><strong>Decomposition</strong>：先分解为可验收的子目标与里程碑（milestone）；</li><li><strong>Plan validation</strong>：对计划做约束检查（权限、依赖、成本预算、可执行性）；</li><li><strong>Executor loop</strong>：按步骤执行，必要时查询工具、写入工作区；</li><li><strong>Step budget / iteration limit</strong>：限制最大步数与成本，避免失控；</li><li><strong>Backtracking</strong>：在关键失败点回溯到上一个可用 checkpoint，重规划或切换策略；</li><li><strong>Tool plan</strong>：将“用哪些工具、输入输出、预期失败模式”写入计划，使执行器具备可操作性。</li></ul><h3 id="6-2-Reflection-Reflexion（自我纠错与记忆）"><a href="#6-2-Reflection-Reflexion（自我纠错与记忆）" class="headerlink" title="6.2 Reflection / Reflexion（自我纠错与记忆）"></a>6.2 Reflection / Reflexion（自我纠错与记忆）</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_21.2ksepz6764.webp" alt></p><p>Reflection（反思）是一类通用的自我纠错策略；而 Reflexion 则更强调把“失败—归因—改进”写入可检索记忆，在下一轮尝试中显式触发策略更新与重试。该思路在 Reflexion 论文中被系统化提出。([arxiv.org][53])<br>Reflection 强调把失败经验转为可复用策略：</p><ul><li><strong>Self-critique</strong>：对结果与过程做自检（是否满足需求、是否引用充分、是否越权/越界）；</li><li><strong>Retry with feedback</strong>：将错误信息与失败原因反馈给模型，触发有条件重试；</li><li><strong>Critic model</strong>：用独立评审模型或规则系统做质量门禁；</li><li><strong>Post-mortem &amp; episodic memory</strong>：将失败案例沉淀为可检索的“情景—原因—修复”记录，形成面向任务域的策略记忆。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_22.3d5a7pmswb.webp" alt></p><h2 id="7-Agent-的核心概念：Agent-Loop"><a href="#7-Agent-的核心概念：Agent-Loop" class="headerlink" title="7. Agent 的核心概念：Agent Loop"></a>7. Agent 的核心概念：Agent Loop</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_23.3ns40v211m.webp" alt></p><p>截至目前（2026 年初），Agent 的开发已经变得非常成熟，衍生出很多很多成熟的框架和落地实践。</p><p>在业界经验中，一个可运行的 Agent 至少包含：<strong>状态（State）</strong>、<strong>环境（Environment）</strong>、<strong>策略（Policy）</strong>与<strong>停止条件（Stop condition）</strong>，并在预算约束下循环迭代。</p><p>最小闭环可表述为：</p><ul><li><strong>Observation</strong>：读取环境与工作区（用户输入、文件、工具结果、事件日志）。</li><li><strong>Decision</strong>：选择下一步动作（直接回答 / 调工具 / 更新计划 / 请求人工确认）。</li><li><strong>Action</strong>：执行工具调用或写入产物。</li><li><strong>State update</strong>：更新进度、上下文工作集与记忆摘要。</li><li><strong>Stop</strong>：达成目标、超出预算、遇到不可恢复错误或需要人工介入时停止。</li></ul><p>以 Claude Code 举例，工程落地时常见的最小组件集通常是：</p><ul><li><strong>Tool registry</strong>：工具清单、schema、权限与路由规则；</li><li><strong>Scratchpad / workspace</strong>：中间推理结构与产物存放区（含可审计日志）；</li><li><strong>Iteration limit / budget</strong>：token、工具调用次数、时间、成本上限；</li><li><strong>Checkpoint / resume</strong>：关键状态可持久化，支持中断恢复与回放。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_24.4g4zilimrs.webp" alt></p><h2 id="8-Agent-工程核心：状态、持久化、上下文工程"><a href="#8-Agent-工程核心：状态、持久化、上下文工程" class="headerlink" title="8. Agent 工程核心：状态、持久化、上下文工程"></a>8. Agent 工程核心：状态、持久化、上下文工程</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_25.54y92m65s7.webp" alt></p><p>上一节把 Agent loop 拆成了“观察-决策-行动-更新-停止”。要把这个循环变成可交付的工程系统，核心不在模型有多聪明，而在运行时是否<strong>可恢复、可审计、可控成本</strong>。这背后有三块最关键的工程骨架：</p><ul><li><strong>状态</strong>：明确“当前任务到哪一步了”，避免隐式对话漂移；</li><li><strong>持久化</strong>：让执行过程可回放、可恢复，防止一次失败就全盘重来；</li><li><strong>上下文工程</strong>：保证有限 token 内放对的信息，其余外置。</li></ul><h3 id="8-1-状态模型：把“隐式对话”变成“显式运行时”"><a href="#8-1-状态模型：把“隐式对话”变成“显式运行时”" class="headerlink" title="8.1 状态模型：把“隐式对话”变成“显式运行时”"></a>8.1 状态模型：把“隐式对话”变成“显式运行时”</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_26.et047ejeu.webp" alt></p><p>Agent 工程的首要问题不是“能否回答”，而是“能否稳定执行”。可操作的抽象需要把系统状态显式化，并保证可验证的更新路径。常见的状态划分：</p><ul><li><strong>对话状态（Conversation state）</strong>：用户目标、约束、历史交互与已确认的决策。</li><li><strong>任务状态（Task state）</strong>：计划/里程碑、待办、已完成步骤、失败原因与重试记录。</li><li><strong>环境状态（Environment state）</strong>：工作目录、文件变更、外部系统（API/DB）副作用、权限与工具可用性。</li></ul><p>把状态落实到 schema / state machine 能带来三类收益：</p><ol><li><strong>可恢复</strong>：长任务中断后可从确定的节点继续；</li><li><strong>可审计</strong>：工具调用与变更路径可追溯；</li><li><strong>可控</strong>：每一步的输入、输出、预算与停止条件可被约束。</li></ol><p>实践中还需要两条额外约束：</p><ul><li><strong>状态变更要可验证</strong>：状态更新尽量通过结构化 action 完成，而不是自由文本；必要时加校验规则（如“未完成的步骤不能被标记完成”）。</li><li><strong>状态版本化</strong>：长周期任务会演进，schema 需要可迁移，避免“旧状态无法恢复”。</li></ul><p>状态 schema 示例（JSON 结构）：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"1.0"</span>,</span><br><span class="line">  <span class="attr">"conversation"</span>: &#123;</span><br><span class="line">    <span class="attr">"goal"</span>: <span class="string">"更新 Nginx 配置并 reload"</span>,</span><br><span class="line">    <span class="attr">"constraints"</span>: [<span class="string">"不改动 upstream"</span>, <span class="string">"需要审批 reload"</span>],</span><br><span class="line">    <span class="attr">"decisions"</span>: [<span class="string">"已确认使用 /etc/nginx/nginx.conf"</span>]</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"task"</span>: &#123;</span><br><span class="line">    <span class="attr">"plan"</span>: [<span class="string">"读取配置"</span>, <span class="string">"生成补丁"</span>, <span class="string">"语法校验"</span>, <span class="string">"reload"</span>],</span><br><span class="line">    <span class="attr">"current_step"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="attr">"status"</span>: <span class="string">"running"</span>,</span><br><span class="line">    <span class="attr">"errors"</span>: []</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"environment"</span>: &#123;</span><br><span class="line">    <span class="attr">"cwd"</span>: <span class="string">"/etc/nginx"</span>,</span><br><span class="line">    <span class="attr">"dirty_files"</span>: [<span class="string">"nginx.conf"</span>],</span><br><span class="line">    <span class="attr">"tool_permissions"</span>: [<span class="string">"read_file"</span>, <span class="string">"write_file"</span>, <span class="string">"shell"</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="8-2-Durable-execution：Checkpoint、Resume-与事件日志"><a href="#8-2-Durable-execution：Checkpoint、Resume-与事件日志" class="headerlink" title="8.2 Durable execution：Checkpoint、Resume 与事件日志"></a>8.2 Durable execution：Checkpoint、Resume 与事件日志</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_27.1e93hdhake.webp" alt></p><p>长任务的失败往往来自“累积误差 + 外部 I/O 不确定性”。工程上需要把执行过程当作<strong>可持久化的事务</strong>：</p><ul><li><strong>Checkpointing</strong>：在关键步骤保存状态快照（输入、工具参数、工具结果、下一步节点）。</li><li><strong>Resume / Replay</strong>：支持从某个 checkpoint 继续，或回放历史步骤复现问题（time-travel debugging）。</li><li><strong>Event log（事件溯源）</strong>：以“事件”记录状态变更，便于回放与审计；必要时可做幂等键（idempotency key）避免重复副作用。</li></ul><p>在图/状态机式运行时里，checkpoint 往往以“每个 super-step 生成快照”的方式实现，从而天然支持人类介入、回放与容错。进一步提升稳定性还需要：</p><ul><li><strong>副作用分类</strong>：把工具调用分成只读/可重复/不可逆三类，对不可逆动作强制审批或双写审计。</li><li><strong>结果重放优先</strong>：恢复时优先使用历史工具结果，避免因外部环境变化导致“同一步不同结果”。</li><li><strong>补偿动作</strong>：对可逆的副作用设计补偿路径（例如撤销改动、回滚配置），让失败不必从头来。</li></ul><p>checkpoint 记录示例（事件 + 快照）：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"checkpoint_id"</span>: <span class="string">"ckpt_2025-01-12T10:14:03Z"</span>,</span><br><span class="line">  <span class="attr">"step"</span>: <span class="string">"syntax_check"</span>,</span><br><span class="line">  <span class="attr">"state_ref"</span>: <span class="string">"state_v1.0"</span>,</span><br><span class="line">  <span class="attr">"input"</span>: &#123;<span class="attr">"file"</span>: <span class="string">"/etc/nginx/nginx.conf"</span>&#125;,</span><br><span class="line">  <span class="attr">"tool_call"</span>: &#123;<span class="attr">"name"</span>: <span class="string">"shell"</span>, <span class="attr">"args"</span>: [<span class="string">"nginx"</span>, <span class="string">"-t"</span>]&#125;,</span><br><span class="line">  <span class="attr">"output"</span>: &#123;<span class="attr">"ok"</span>: <span class="literal">true</span>, <span class="attr">"stdout"</span>: <span class="string">"syntax is ok"</span>&#125;,</span><br><span class="line">  <span class="attr">"next_step"</span>: <span class="string">"reload"</span>,</span><br><span class="line">  <span class="attr">"timestamp"</span>: <span class="string">"2025-01-12T10:14:03Z"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="8-3-上下文工程：Working-set-管理比“塞满上下文”更重要"><a href="#8-3-上下文工程：Working-set-管理比“塞满上下文”更重要" class="headerlink" title="8.3 上下文工程：Working set 管理比“塞满上下文”更重要"></a>8.3 上下文工程：Working set 管理比“塞满上下文”更重要</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_28.77e1qo4qt8.webp" alt></p><p>上下文工程的目标是：在固定 token 预算内，维持正确性与任务连续性。有效方法不是“塞满上下文”，而是<strong>分层与取舍</strong>。常见策略：</p><ul><li><strong>Working set</strong>：把当前决策必要信息维持在短上下文；其余内容外置。</li><li><strong>Summarization / compression</strong>：对历史对话、检索结果、日志做结构化压缩；明确“保留事实/结论/待办/风险”。</li><li><strong>文件系统作为记忆</strong>：将长文本、代码片段、表格与中间产物写入 workspace，再按需读回（避免 context overflow）。</li><li><strong>Context quarantine</strong>：对子任务使用隔离上下文（子代理/子图），主代理只接收“可交付摘要 + 引用路径”。</li></ul><p>更工程化的做法通常会显式设定“上下文预算”：</p><ul><li><strong>固定槽位</strong>：系统规则、任务目标、当前计划、关键证据四类信息优先保留。</li><li><strong>引用而非复制</strong>：长文档只保留摘要与路径/行号引用，模型需要时再读回。</li><li><strong>准入门槛</strong>：只有通过“相关性 + 可靠性”校验的信息才进入工作集，避免垃圾上下文污染决策。</li></ul><h3 id="8-4-Memory、缓存与成本控制"><a href="#8-4-Memory、缓存与成本控制" class="headerlink" title="8.4 Memory、缓存与成本控制"></a>8.4 Memory、缓存与成本控制</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_29.7ps8rsdyz.webp" alt></p><ul><li><strong>短期记忆</strong>：最近 N 轮消息、当前待办与里程碑。</li><li><strong>长期记忆</strong>：向量库/语义检索、跨会话偏好与项目知识；需要“写入门槛”（避免把噪声固化）。</li><li><strong>缓存</strong>：对稳定工具结果（如依赖解析、目录树）做缓存；对不可缓存工具（实时数据）标记 TTL。</li><li><strong>成本预算</strong>：同时约束 token 与 tool call 次数；在并发场景下要设置 max_concurrency，防止工具风暴与费用失控。</li></ul><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_30.et047ejei.webp" alt></p><hr><h2 id="9-Agent-扩展迭代：效果评估、可观测性"><a href="#9-Agent-扩展迭代：效果评估、可观测性" class="headerlink" title="9. Agent 扩展迭代：效果评估、可观测性"></a>9. Agent 扩展迭代：效果评估、可观测性</h2><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_31.mkdc68jd.webp" alt></p><p>前面几章讨论了 Agent 的运行时骨架：工具、循环、状态、持久化与上下文。把这些能力拼起来，确实能“跑起来”；但在真实生产里更难的是第二步：<strong>如何证明它变好了、出了问题如何复现、上线后如何监控与止血</strong>。这就落到两个问题：</p><ul><li><strong>效果评估（Evaluation）</strong>：把“好不好”变成可重复的指标与回归门禁；</li><li><strong>可观测性（Observability）</strong>：把一次 run 变成可追踪、可回放、可定位的轨迹数据。</li></ul><h3 id="9-1-“效果”到底是什么：结果、过程与边界"><a href="#9-1-“效果”到底是什么：结果、过程与边界" class="headerlink" title="9.1 “效果”到底是什么：结果、过程与边界"></a>9.1 “效果”到底是什么：结果、过程与边界</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_32.8l0kupfstz.webp" alt></p><p>Agent 的“效果”不是单轮回答质量，而是一个端到端执行系统的综合表现。更可操作的定义通常包含三层：</p><ul><li><strong>结果层（Outcome）</strong>：最终交付物是否满足验收标准（正确性、完整性、格式、业务约束）。</li><li><strong>过程层（Process）</strong>：是否走了合理路径（工具是否用对、是否出现无效循环、是否能自我纠错与恢复）。</li><li><strong>边界层（Safety/Policy）</strong>：是否遵守权限与合规边界（敏感操作是否审批、敏感信息是否泄露、不可逆副作用是否可控）。</li></ul><p>工程上建议在需求阶段就写清“验收条件”，否则评估只能停留在主观印象。对可自动验证的任务，优先把验收条件写成测试/校验器；对开放式任务，再考虑 rubric 或人工抽检。</p><h3 id="9-2-指标体系：从“成功率”到“可运营”"><a href="#9-2-指标体系：从“成功率”到“可运营”" class="headerlink" title="9.2 指标体系：从“成功率”到“可运营”"></a>9.2 指标体系：从“成功率”到“可运营”</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_33.232d1e4tkj.webp" alt></p><p>一套能落地的指标通常会同时覆盖质量、效率与风险，并能分解到 task/step/tool 三个层级：</p><ul><li><strong>任务级（Task-level）</strong><ul><li><strong>成功率</strong>：按验收条件通过的比例（success rate），最好区分 hard fail / soft fail。</li><li><strong>质量分</strong>：rubric 打分或自动评分（例如信息覆盖、格式一致性、引用完整性）。</li><li><strong>交付时间</strong>：端到端 wall time、以及用户等待时间（含人类审批等待）。</li><li><strong>成本</strong>：tokens、tool call 次数、外部 API 费用、以及峰值并发。</li><li><strong>人工介入率</strong>：需要人工修正/补充上下文/重复审批的比例。</li></ul></li><li><strong>步骤/工具级（Step/Tool-level）</strong><ul><li><strong>工具成功率</strong>：按工具类型统计错误率、超时率、重试次数与平均延迟。</li><li><strong>恢复能力</strong>：失败后能否通过重试/回溯/checkpoint 恢复（recovery rate）。</li><li><strong>无效循环</strong>：重复读同一文件、反复调用同一工具、计划不推进等（loop rate）。</li></ul></li><li><strong>安全与合规（Safety/Policy）</strong><ul><li><strong>高风险动作占比</strong>：不可逆工具调用、写生产配置、删库等（需结合权限模型）。</li><li><strong>越权/越界事件</strong>：被拦截的调用、敏感信息命中、policy violation 次数。</li></ul></li></ul><p>这些指标的价值在于：一旦能够稳定采集，就能做趋势、对比、回归与告警，把“模型不确定性”纳入工程治理。</p><h3 id="9-3-离线评测：Golden-tasks、回放与回归门禁"><a href="#9-3-离线评测：Golden-tasks、回放与回归门禁" class="headerlink" title="9.3 离线评测：Golden tasks、回放与回归门禁"></a>9.3 离线评测：Golden tasks、回放与回归门禁</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_34.1vz55yio4r.webp" alt></p><p>Agent 的离线评测最好像软件测试一样分层组织，而不是只跑几条 demo：</p><ul><li><strong>Golden task suite（固定任务集）</strong>：把真实任务抽象成可复现样本（输入、初始文件/数据、工具集合、验收标准）。</li><li><strong>Record/Replay（录制/回放）</strong>：将外部 I/O（搜索/API/DB）结果录制为 fixture，回放时优先使用历史结果，避免评测被外部变化污染。</li><li><strong>多次运行看分布</strong>：同一任务建议跑多次（不同 seed/温度/并发），关注成功率与方差，而不是单次结果。</li><li><strong>自动验收优先</strong>：能用测试、lint、schema 校验、diff 校验解决的，不要用主观评审；开放题再用 rubric 或 LLM-as-judge（并保留抽检）。</li><li><strong>回归门禁（gating）</strong>：把关键指标阈值写进 CI（成功率不降、成本不升、风险事件不增），让“改 prompt/改工具/改策略”都可回归。</li></ul><p>评测的终点不是“跑出一个分数”，而是形成稳定的反馈闭环：每一次线上事故与失败 case，都能沉淀进 task suite，变成下一次的回归用例。</p><h3 id="9-4-可观测性：把一次-run-变成可调试的系统"><a href="#9-4-可观测性：把一次-run-变成可调试的系统" class="headerlink" title="9.4 可观测性：把一次 run 变成可调试的系统"></a>9.4 可观测性：把一次 run 变成可调试的系统</h3><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_35.64ecfs8wx6.webp" alt></p><p>可观测性的目标是：<strong>任何失败都能被复现与定位</strong>。对 Agent 而言，日志不只记录错误栈，更要记录“决策轨迹”：</p><ul><li><strong>Trace（链路追踪）</strong>：用 <code>run_id/thread_id/step_id/tool_call_id</code> 串起一次执行中的模型调用、工具调用、状态更新与 checkpoint。</li><li><strong>事件日志 + 快照</strong>：记录关键事件（Observation/Decision/Action/ToolResult/StateUpdate）以及必要的状态快照，支持 time-travel debugging。</li><li><strong>输入输出治理</strong>：对 prompt、工具参数、工具结果做脱敏/截断/哈希（既能排障，又不泄露敏感信息）。</li><li><strong>指标面板与告警</strong>：把 9.2 的指标落到 dashboard（成功率、成本、延迟、工具错误、循环率），并为突发退化设置告警与自动降级策略。</li></ul><p>实践中一个常见的经验是：<strong>把“可回放”当作第一原则</strong>。只要 run 能 replay，很多“偶发问题”就会变成可定位的确定性问题；而这依赖运行时记录足够多的结构化轨迹数据。</p><p><img src="https://cdn.jsdmirror.com/gh/XhinLiang/picx-images-hosting@master/20260114/image_w1376_h768_image_36.232d1e4tk8.webp" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>读到这里，你大概已经意识到：从 LLM 到 Agent，并不是“把 Chatbot 接上几个工具”这么简单。真正的分水岭在于：我们是否把推理放进一个<strong>可执行、可验证、可回放</strong>的闭环里——让模型从“会答”，走向“能交付”。</p><p>如果你准备把 Agent 落到真实业务里，不妨先把问题问得更工程一点：</p><ul><li>这个任务的<strong>验收标准</strong>是什么？哪些可以自动校验，哪些必须人工把关？</li><li>失败是常态：需要怎样的<strong>重试/回滚/checkpoint</strong>，才能让长流程不崩盘？</li><li>上线以后怎么运营：<strong>指标、日志、告警、权限边界</strong>是否足够清晰？</li></ul><p>当这三件事有了答案，Tool Calling / MCP / ReAct / DeepResearch 才会从“概念名词”变成“系统能力”；否则，Agent 只是在更大的上下文里，讲出更像样的故事。</p><p>回到文章开头的疑问：为什么有了 LLM 还要做 Agent？因为“会答”解决的是信息表达，“会做事”解决的是任务交付。而 Agent 的本质，就是把交付做成系统：每一步都能执行、能验证、能复盘。</p><h2 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h2><ul><li>[1] <a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener">Function calling | OpenAI API</a></li><li>[2] <a href="https://platform.openai.com/docs/guides/structured-outputs/key-ordering?api-mode=chat" target="_blank" rel="noopener">Structured model outputs | OpenAI API</a></li><li>[3] <a href="https://developers.openai.com/apps-sdk/concepts/mcp-server" target="_blank" rel="noopener">Model Context Protocol (MCP) | OpenAI Developers</a></li><li>[4] <a href="https://platform.openai.com/docs/guides/tools-connectors-mcp" target="_blank" rel="noopener">Connectors and MCP servers | OpenAI API</a></li><li>[5] <a href="https://platform.openai.com/docs/api-reference/completions" target="_blank" rel="noopener">Completions | OpenAI API Reference</a></li><li>[6] <a href="https://platform.openai.com/docs/api-reference/chat?locale=en" target="_blank" rel="noopener">Chat Completions | OpenAI API Reference</a></li><li>[7] <a href="https://openai.com/index/function-calling-and-other-api-updates/" target="_blank" rel="noopener">Function calling and other API updates - OpenAI</a></li><li>[8] <a href="https://platform.openai.com/docs/assistants/migration" target="_blank" rel="noopener">Assistants migration guide | OpenAI API</a></li><li>[9] <a href="https://platform.openai.com/docs/deprecations" target="_blank" rel="noopener">Deprecations | OpenAI API</a></li><li>[10] <a href="https://developers.openai.com/blog/responses-api" target="_blank" rel="noopener">Why we built the Responses API</a></li><li>[11] <a href="https://platform.openai.com/docs/guides/migrate-to-responses" target="_blank" rel="noopener">Migrate to the Responses API | OpenAI API</a></li><li>[12] <a href="https://platform.openai.com/docs/guides/reasoning" target="_blank" rel="noopener">Reasoning models | OpenAI API</a></li><li>[13] <a href="https://platform.openai.com/docs/guides/conversation-state" target="_blank" rel="noopener">Conversation state | OpenAI API</a></li><li>[14] <a href="https://github.com/openai/completions-responses-migration-pack" target="_blank" rel="noopener">completions-responses-migration-pack | GitHub</a></li><li>[15] <a href="https://www.datacamp.com/tutorial/openai-responses-api" target="_blank" rel="noopener">OpenAI Responses API: The Ultimate Developer Guide | DataCamp</a></li><li>[16] <a href="https://open.manus.im/docs/integrations/manus-api" target="_blank" rel="noopener">Manus API - Manus Documentation</a></li><li>[17] <a href="https://open.manus.im/docs/api-reference" target="_blank" rel="noopener">Overview - Manus API</a></li><li>[18] <a href="https://open.manus.im/docs/features/browser-operator" target="_blank" rel="noopener">Browser Operator - Manus Documentation</a></li><li>[19] <a href="https://www.gadgets360.com/ai/news/meta-acquisition-manus-ai-agent-developer-fifth-deal-2025-10129781" target="_blank" rel="noopener">Meta Acquires Autonomous Agent Developer Manus AI, Marks Its Fifth Deal in 2025 | Gadgets 360</a></li><li>[20] <a href="https://aimagazine.com/news/how-manus-puts-meta-ahead-in-the-agentic-ai-economy" target="_blank" rel="noopener">Inside Meta’s Groundbreaking Acquisition of Manus | AI Magazine</a></li><li>[21] <a href="https://www.techradar.com/pro/meta-buys-manus-for-usd2-billion-to-power-high-stakes-ai-agent-race" target="_blank" rel="noopener">Meta buys Manus for $2 billion to power high-stakes AI agent race | TechRadar</a></li><li>[22] <a href="https://www.allaboutai.com/ai-news/what-meta-acquisition-of-manus-means-for-the-future-of-ai-agents" target="_blank" rel="noopener">What Meta’s Acquisition of Manus Means for the Future of AI Agents | All About AI</a></li><li>[23] <a href="https://en.wikipedia.org/wiki/Manus_%28AI_agent%29" target="_blank" rel="noopener">Manus (AI agent) | Wikipedia</a></li><li>[24] <a href="https://github.com/langchain-ai/deepagents" target="_blank" rel="noopener">GitHub - langchain-ai/deepagents</a></li><li>[25] <a href="https://docs.langchain.com/oss/python/deepagents/cli" target="_blank" rel="noopener">Deep Agents CLI | LangChain Docs</a></li><li>[26] <a href="https://docs.langchain.com/oss/python/deepagents/overview" target="_blank" rel="noopener">Deep Agents overview | LangChain Docs</a></li><li>[27] <a href="https://github.com/langchain-ai/deepagents-quickstarts" target="_blank" rel="noopener">GitHub - langchain-ai/deepagents-quickstarts</a></li><li>[28] <a href="https://github.com/agno-agi/agno" target="_blank" rel="noopener">GitHub - agno-agi/agno</a></li><li>[29] <a href="https://deepwiki.com/agno-agi/agno-docs" target="_blank" rel="noopener">agno-agi/agno-docs | DeepWiki</a></li><li>[30] <a href="https://deepwiki.com/agno-agi/agno-docs/5-examples-and-applications" target="_blank" rel="noopener">Examples and Applications | agno-agi/agno-docs | DeepWiki</a></li><li>[31] <a href="https://docs.agno.com/examples/use-cases/agents/deep_research_agent_exa" target="_blank" rel="noopener">Deep Research Agent - Agno</a></li><li>[32] <a href="https://github.com/shareAI-lab/learn-claude-code" target="_blank" rel="noopener">GitHub - shareAI-lab/learn-claude-code</a></li><li>[33] <a href="https://www.baytechconsulting.com/blog/manus-ai-an-analytical-guide-to-the-autonomous-ai-agent-2025" target="_blank" rel="noopener">Manus AI: An Analytical Guide to the Autonomous AI Agent 2025</a></li><li>[34] <a href="https://manus.im/tools" target="_blank" rel="noopener">Manus AI Agent Toolkit for Delivering Work</a></li><li>[35] <a href="https://manus.im/zh-cn/tools" target="_blank" rel="noopener">用于交付工作的 Manus AI Agent 工具包</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;2025 年结束，Agent 这个概念在所有科技行业从业者中每天都会听到 10 次以上。那到底什么是 Agent，为什么有了 LLM 不够，
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Agent" scheme="https://xhinliang.github.io/tags/Agent/"/>
    
      <category term="LLM" scheme="https://xhinliang.github.io/tags/LLM/"/>
    
      <category term="MCP" scheme="https://xhinliang.github.io/tags/MCP/"/>
    
      <category term="ReAct" scheme="https://xhinliang.github.io/tags/ReAct/"/>
    
      <category term="DeepResearch" scheme="https://xhinliang.github.io/tags/DeepResearch/"/>
    
      <category term="AI" scheme="https://xhinliang.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>HTTP 请求的优雅取消（Graceful Cancellation）</title>
    <link href="https://xhinliang.github.io/2024/04/backend/http-graceful-cancelation-zh/"/>
    <id>https://xhinliang.github.io/2024/04/backend/http-graceful-cancelation-zh/</id>
    <published>2024-04-24T01:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.607Z</updated>
    
    <content type="html"><![CDATA[<p>我们经常会写一些“很慢”的 HTTP 接口：比如触发导出、跑一段复杂计算、调用外部服务、或者生成大文件。</p><p>问题在于：<strong>客户端可能在任务完成前就取消了请求</strong>（关闭页面、点了取消、网络切换、超时等）。</p><p>那服务端能不能“及时知道”客户端已经取消？如果能知道，就可以尽早停止后续的昂贵操作，省 CPU/IO/下游资源。</p><p>这篇文章用一个简单的 Go + Gin 示例，把常见场景拆开说清楚：</p><ul><li>不经过 Nginx：HTTP/1.1、HTTP/2</li><li>客户端“主动取消” vs “突然断网离线”</li><li>经过 Nginx 反向代理时：Nginx→Server 用 HTTP/1.1 或 HTTP/2</li></ul><p>核心结论先说：</p><ol><li><strong>Go 服务端可以通过 <code>Request.Context()</code> 感知“连接生命周期”的结束</strong>，从而在 handler 里停止工作。</li><li><strong>HTTP/2/HTTP/3 支持显式取消（RST_STREAM）</strong>，通常能更“及时地”通知服务端。</li><li><strong>突然离线（没有 FIN/RST/RST_STREAM）时，服务端往往感知不到</strong>，除非你自己做了超时、心跳、或应用层中断机制。</li></ol><h2 id="不经过-Nginx"><a href="#不经过-Nginx" class="headerlink" title="不经过 Nginx"></a>不经过 Nginx</h2><h3 id="HTTP-1-1：用-request-context-做中断"><a href="#HTTP-1-1：用-request-context-做中断" class="headerlink" title="HTTP/1.1：用 request context 做中断"></a>HTTP/1.1：用 request context 做中断</h3><p>下面用 Gin 写一个慢接口 <code>/http1</code>：每秒做一次“昂贵操作”，并在每一轮检查 <code>ctx.Done()</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"context"</span></span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"github.com/gin-gonic/gin"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    router := gin.Default()</span><br><span class="line"></span><br><span class="line">    router.GET(<span class="string">"/http1"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">        ctx := c.Request.Context()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">            <span class="keyword">if</span> err := costlyOperation(ctx, i); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="comment">// 499 并不是标准 HTTP code，但在 Nginx 世界里常用来表示 client closed request</span></span><br><span class="line">                c.AbortWithStatusJSON(<span class="number">499</span>, gin.H&#123;<span class="string">"error"</span>: <span class="string">"client disconnected"</span>&#125;)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        c.JSON(<span class="number">200</span>, gin.H&#123;<span class="string">"status"</span>: <span class="string">"ok"</span>&#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    router.Run(<span class="string">":8080"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">costlyOperation</span><span class="params">(ctx context.Context, i <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-time.After(<span class="number">1</span> * time.Second):</span><br><span class="line">        fmt.Println(<span class="string">"Working..."</span>, i)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">        fmt.Println(<span class="string">"Client disconnected. Stop."</span>, i)</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"client disconnected"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="用-cURL-测试（HTTP-1-1）"><a href="#用-cURL-测试（HTTP-1-1）" class="headerlink" title="用 cURL 测试（HTTP/1.1）"></a>用 cURL 测试（HTTP/1.1）</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8080/http1</span><br><span class="line">^C  <span class="comment"># 过几秒手动中断</span></span><br></pre></td></tr></table></figure><p>你会看到服务端在某个循环里打印到 <code>ctx.Done()</code> 的分支，说明 <strong>服务端感知到连接已经关闭</strong>。</p><h4 id="用浏览器-XHR-测试"><a href="#用浏览器-XHR-测试" class="headerlink" title="用浏览器 XHR 测试"></a>用浏览器 XHR 测试</h4><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> xhr = <span class="keyword">new</span> XMLHttpRequest();</span><br><span class="line">xhr.open(<span class="string">'GET'</span>, <span class="string">'http://localhost:8080/http1'</span>);</span><br><span class="line">xhr.send();</span><br><span class="line"></span><br><span class="line">setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> xhr.abort(), <span class="number">7000</span>);</span><br></pre></td></tr></table></figure><p>一般也能触发服务端的取消感知（取决于浏览器/网络栈，通常会导致连接关闭或复用连接上的中断）。</p><h4 id="这背后发生了什么？"><a href="#这背后发生了什么？" class="headerlink" title="这背后发生了什么？"></a>这背后发生了什么？</h4><p>在 Go 里，每个请求的 <code>Context</code> 会绑定到请求/连接的生命周期。</p><ul><li>当底层 TCP 连接被关闭（FIN/RST），Go 的 net/http 会取消这个 context</li><li>handler 里只要在耗时操作之间不断检查 <code>ctx.Done()</code>，就能“尽快”停下来</li></ul><p>注意：Go 不会神奇地“打断”你正在执行的 CPU 密集型函数，它只会把 <code>ctx.Done()</code> 变成可读。<strong>是否及时停下来，取决于你是否在关键路径里检查并传播 context。</strong></p><h3 id="HTTP-2：更明确的取消信号"><a href="#HTTP-2：更明确的取消信号" class="headerlink" title="HTTP/2：更明确的取消信号"></a>HTTP/2：更明确的取消信号</h3><p>Gin 在 TLS 下可以跑 HTTP/2（本地实验可以用自签证书）：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为了启用 HTTP/2，把 Run 换成 RunTLS</span></span><br><span class="line">router.RunTLS(<span class="string">":8081"</span>, <span class="string">"/tmp/server.crt"</span>, <span class="string">"/tmp/server.key"</span>)</span><br></pre></td></tr></table></figure><h4 id="用-cURL（HTTP-2）测试"><a href="#用-cURL（HTTP-2）测试" class="headerlink" title="用 cURL（HTTP/2）测试"></a>用 cURL（HTTP/2）测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https://localhost:8081/http2 --insecure --verbose</span><br><span class="line">^C  <span class="comment"># 中断</span></span><br></pre></td></tr></table></figure><p>HTTP/2 支持对单个 stream 发送 RST_STREAM 来取消请求，所以服务端通常能更早收到“取消”的明确语义。</p><h3 id="突然离线（Offline）：最容易误判的场景"><a href="#突然离线（Offline）：最容易误判的场景" class="headerlink" title="突然离线（Offline）：最容易误判的场景"></a>突然离线（Offline）：最容易误判的场景</h3><p>如果用户不是“点取消/关闭连接”，而是直接断网（例如手机关 Wi‑Fi、飞行模式），可能不会马上产生 TCP FIN/RST，也可能不会立刻发 HTTP/2 RST_STREAM。</p><p>结果就是：<strong>服务端还以为请求正常进行</strong>，你会看到循环完整跑完，最后写响应时才可能发现对端不可达（甚至写响应时也可能没立刻报错，取决于内核缓冲和重传）。</p><p>所以：</p><ul><li>“能否感知取消” ≠ “用户不想等了”</li><li>仅靠连接状态，无法覆盖所有“用户已离线/不再关心”的情况</li></ul><h3 id="小结（不经过-Nginx）"><a href="#小结（不经过-Nginx）" class="headerlink" title="小结（不经过 Nginx）"></a>小结（不经过 Nginx）</h3><ul><li>HTTP/1.1：更多是依赖 TCP 连接状态（FIN/RST），通常在你下一次读写或检查 context 时才知道</li><li>HTTP/2/HTTP/3：协议层有显式取消（RST_STREAM），更及时、更明确</li><li>Offline：可能没有任何显式信号，服务端往往无法立刻知道</li></ul><h2 id="经过-Nginx（反向代理）"><a href="#经过-Nginx（反向代理）" class="headerlink" title="经过 Nginx（反向代理）"></a>经过 Nginx（反向代理）</h2><p>现实里，大多数服务端前面还有 Nginx：</p><ul><li>Client → Nginx：常见是 HTTP/2</li><li>Nginx → Server：可能是 HTTP/1.1，也可能是 HTTP/2</li></ul><p>这会影响“取消信号”是否能传递到你的应用。</p><p>下面是一个示意配置（仅用于说明）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">events &#123; worker_connections 1024; &#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">  error_log /tmp/https_error.log debug;</span><br><span class="line"></span><br><span class="line">  server &#123;</span><br><span class="line">    listen 443 ssl http2;</span><br><span class="line">    server_name local_nginx_http_2;</span><br><span class="line"></span><br><span class="line">    ssl_certificate /tmp/server.crt;</span><br><span class="line">    ssl_certificate_key /tmp/server.key;</span><br><span class="line"></span><br><span class="line">    location /http1 &#123;</span><br><span class="line">      proxy_pass http://127.0.0.1:8080;</span><br><span class="line">      proxy_set_header Host $host;</span><br><span class="line">      proxy_http_version 1.1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /http2 &#123;</span><br><span class="line">      proxy_pass https://127.0.0.1:8081;</span><br><span class="line">      proxy_set_header Host $host;</span><br><span class="line">      proxy_ssl_verify off;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="场景-1：Client-HTTP-2-→-Nginx-→-Server-HTTP-1-1"><a href="#场景-1：Client-HTTP-2-→-Nginx-→-Server-HTTP-1-1" class="headerlink" title="场景 1：Client(HTTP/2) → Nginx → Server(HTTP/1.1)"></a>场景 1：Client(HTTP/2) → Nginx → Server(HTTP/1.1)</h3><ul><li>客户端取消：对 Nginx 来说是 HTTP/2 RST_STREAM</li><li>但 Nginx 到后端是 HTTP/1.1，<strong>没有“RST_STREAM”这种语义</strong></li><li>Nginx 只能选择：<ul><li>关闭到后端的 TCP 连接（让后端通过连接断开间接感知）</li><li>或者继续把后端请求跑完但丢弃响应（浪费后端资源）</li></ul></li></ul><p>是否会“帮你断后端连接”，取决于 Nginx 行为和配置，以及后端写回时机。</p><h3 id="场景-2：Client-HTTP-2-→-Nginx-→-Server-HTTP-2"><a href="#场景-2：Client-HTTP-2-→-Nginx-→-Server-HTTP-2" class="headerlink" title="场景 2：Client(HTTP/2) → Nginx → Server(HTTP/2)"></a>场景 2：Client(HTTP/2) → Nginx → Server(HTTP/2)</h3><ul><li>客户端取消：Nginx 收到 RST_STREAM</li><li>Nginx 到后端也是 HTTP/2：<strong>可以把取消语义转发</strong>（再发一个 RST_STREAM）</li><li>后端能更快、更明确地停止工作</li></ul><h3 id="小结（经过-Nginx）"><a href="#小结（经过-Nginx）" class="headerlink" title="小结（经过 Nginx）"></a>小结（经过 Nginx）</h3><p>如果你的业务非常在意“取消要尽快释放资源”，优先保证：</p><ul><li>后端代码层面：长任务支持 <code>context</code> 贯穿（DB/HTTP/RPC 调用都能被中断/超时）</li><li>链路层面：尽量让 Nginx→Server 也跑 HTTP/2（或至少让连接断开能被后端及时感知）</li></ul><h2 id="实战建议（避免“以为能取消，但其实没停”）"><a href="#实战建议（避免“以为能取消，但其实没停”）" class="headerlink" title="实战建议（避免“以为能取消，但其实没停”）"></a>实战建议（避免“以为能取消，但其实没停”）</h2><ol><li><strong>所有慢操作都要接收并传播 <code>context.Context</code></strong>：DB 查询、HTTP 调用、队列 publish、文件上传等。</li><li><strong>为长任务设置上限</strong>：服务端超时（<code>context.WithTimeout</code>）比“等客户端取消”可靠。</li><li><strong>把“请求取消”当成优化而不是正确性依赖</strong>：离线、网络抖动、代理行为都可能让你感知不到。</li><li><strong>对昂贵任务用异步化</strong>：把任务丢到队列，HTTP 只返回 task id；客户端取消不影响任务一致性，由你控制是否可取消。</li></ol><hr><p>如果你想我补一段更贴近生产的示例（例如：Gin handler 里同时调用 DB + 下游 HTTP，并在中断时做清理/指标上报/日志关联 trace id），我也可以基于这个文章继续扩展。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们经常会写一些“很慢”的 HTTP 接口：比如触发导出、跑一段复杂计算、调用外部服务、或者生成大文件。&lt;/p&gt;
&lt;p&gt;问题在于：&lt;strong&gt;客户端可能在任务完成前就取消了请求&lt;/strong&gt;（关闭页面、点了取消、网络切换、超时等）。&lt;/p&gt;
&lt;p&gt;那服务端能不能“及
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Chinese" scheme="https://xhinliang.github.io/tags/Chinese/"/>
    
      <category term="XHR" scheme="https://xhinliang.github.io/tags/XHR/"/>
    
      <category term="HTTP" scheme="https://xhinliang.github.io/tags/HTTP/"/>
    
      <category term="HttpClient" scheme="https://xhinliang.github.io/tags/HttpClient/"/>
    
      <category term="Nginx" scheme="https://xhinliang.github.io/tags/Nginx/"/>
    
      <category term="HTTP/2" scheme="https://xhinliang.github.io/tags/HTTP-2/"/>
    
      <category term="Cancelation" scheme="https://xhinliang.github.io/tags/Cancelation/"/>
    
      <category term="Cancel" scheme="https://xhinliang.github.io/tags/Cancel/"/>
    
  </entry>
  
  <entry>
    <title>HTTP Graceful Cancellation</title>
    <link href="https://xhinliang.github.io/2024/04/backend/http-graceful-cancelation/"/>
    <id>https://xhinliang.github.io/2024/04/backend/http-graceful-cancelation/</id>
    <published>2024-04-24T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.607Z</updated>
    
    <content type="html"><![CDATA[<p>Suppose we’re writing an HTTP server with an endpoint that takes a long time to finish.</p><p>A client may start a request and then cancel it before the handler completes (closing the tab, navigating away, hitting “stop”, etc.).</p><p>The question is: can the server detect that the client has canceled the request, and do so early enough to stop expensive downstream work?</p><h2 id="Without-Nginx"><a href="#Without-Nginx" class="headerlink" title="Without Nginx"></a>Without Nginx</h2><h3 id="HTTP-1-1"><a href="#HTTP-1-1" class="headerlink" title="HTTP/1.1"></a>HTTP/1.1</h3><p>Let’s start with a simple example using Go’s Gin framework to serve the endpoint <code>/http1</code>.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"context"</span></span><br><span class="line">    <span class="string">"github.com/gin-gonic/gin"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    router := gin.Default()</span><br><span class="line"></span><br><span class="line">    router.GET(<span class="string">"/http1"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">        <span class="comment">// Context from the Gin handler</span></span><br><span class="line">        ctx := c.Request.Context()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Simulate a long-running task with periodic checks</span></span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">            <span class="comment">// Costly operation</span></span><br><span class="line">            <span class="keyword">if</span> err := costlyOperation(ctx, i); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                c.AbortWithStatusJSON(<span class="number">499</span>, gin.H&#123;<span class="string">"error"</span>: <span class="string">"Client has disconnected"</span>&#125;)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Complete the task</span></span><br><span class="line">        c.JSON(<span class="number">200</span>, gin.H&#123;<span class="string">"status"</span>: <span class="string">"Task completed successfully"</span>&#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    router.Run(<span class="string">":8080"</span>) <span class="comment">// listen and serve on 0.0.0.0:8080</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">costlyOperation</span><span class="params">(ctx context.Context, i <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-time.After(<span class="number">1</span> * time.Second): <span class="comment">// simulate work by sleeping</span></span><br><span class="line">        fmt.Println(<span class="string">"Working..."</span>, i)</span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done(): <span class="comment">// check if the context is done</span></span><br><span class="line">        fmt.Println(<span class="string">"Client has disconnected. Stopping task."</span>, i)</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"client has disconnected"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cURL"><a href="#cURL" class="headerlink" title="cURL"></a>cURL</h4><p>Test with local curl over HTTP/1.1:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8080/http1</span><br><span class="line">^C # after 2 seconds</span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[GIN-debug] Listening and serving HTTP on :8080</span><br><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Client has disconnected. Stopping task.  3</span><br><span class="line">[GIN] 2024/04/23 - 23:14:17 | 499 |  3.332915625s |       127.0.0.1 | GET      "/http1"</span><br></pre></td></tr></table></figure><p>This shows the server notices the disconnect and cancels the request context.</p><h4 id="XHR"><a href="#XHR" class="headerlink" title="XHR"></a>XHR</h4><p>Now test the same endpoint from a browser using XHR.</p><p>In real code you would call <code>xhr.abort()</code> from a timer or a user action; the snippet below just highlights the cancellation call site.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> xhr = <span class="keyword">new</span> XMLHttpRequest();</span><br><span class="line">xhr.open(<span class="string">"GET"</span>, <span class="string">"http://localhost:8080/http1"</span>);</span><br><span class="line">xhr.onreadystatechange = <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (xhr.readyState === XMLHttpRequest.DONE) &#123;</span><br><span class="line">        <span class="keyword">if</span> (xhr.status === <span class="number">200</span>) &#123;</span><br><span class="line">            <span class="built_in">console</span>.log(<span class="string">"Response:"</span>, xhr.responseText);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">console</span>.error(<span class="string">"Request failed with status:"</span>, xhr.status);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">xhr.send();</span><br><span class="line">xhr.abort(); <span class="comment">// after 7 seconds</span></span><br></pre></td></tr></table></figure><p>The server log is:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Working<span class="built_in">..</span>. 0</span><br><span class="line">Working<span class="built_in">..</span>. 1</span><br><span class="line">Working<span class="built_in">..</span>. 2</span><br><span class="line">Working<span class="built_in">..</span>. 3</span><br><span class="line">Working<span class="built_in">..</span>. 4</span><br><span class="line">Working<span class="built_in">..</span>. 5</span><br><span class="line">Working<span class="built_in">..</span>. 6</span><br><span class="line">Working<span class="built_in">..</span>. 7</span><br><span class="line">Client has disconnected. Stopping task.  8</span><br></pre></td></tr></table></figure></p><h4 id="How-does-it-work"><a href="#How-does-it-work" class="headerlink" title="How does it work?"></a>How does it work?</h4><p>In Go, each request carries a <code>context.Context</code> that is tied to the request lifecycle. If the underlying connection is closed, Go’s HTTP server cancels that context, and frameworks like Gin surface it via <code>c.Request.Context()</code>.</p><p>This cancellation can be triggered by a client disconnect (TCP FIN/RST), a server-side timeout, or application logic choosing to abort the request processing.</p><h3 id="HTTP-2"><a href="#HTTP-2" class="headerlink" title="HTTP/2"></a>HTTP/2</h3><p>Gin can also serve HTTP/2 when TLS is enabled, so we can repeat the same experiment over HTTP/2.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"context"</span></span><br><span class="line">    <span class="string">"github.com/gin-gonic/gin"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// ... no changes in the router setup</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// To enable HTTP/2, replace the `Run` method with `RunTLS`</span></span><br><span class="line">    router.RunTLS(<span class="string">":8081"</span>, <span class="string">"/tmp/server.crt"</span>, <span class="string">"/tmp/server.key"</span>) <span class="comment">// listen and serve on 0.0.0.0:8081 using TLS, necessary for HTTP/2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ... no changes in the handler</span></span><br></pre></td></tr></table></figure><h4 id="cURL-1"><a href="#cURL-1" class="headerlink" title="cURL"></a>cURL</h4><p>Test with local curl over HTTP/2:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">curl https://localhost:8081/http2 --insecure --verbose</span><br><span class="line">*   Trying 127.0.0.1:8081...</span><br><span class="line">* Connected to</span><br><span class="line"></span><br><span class="line"> localhost (127.0.0.1) port 8081 (#0)</span><br><span class="line">* ALPN, offering h2</span><br><span class="line">* ALPN, offering http/1.1</span><br><span class="line">* successfully set certificate verify locations:</span><br><span class="line">* TLSv1.3 (OUT), TLS handshake, Client hello (1):</span><br><span class="line">* TLSv1.3 (IN), TLS handshake, Server hello (2):</span><br><span class="line">* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):</span><br><span class="line">* TLSv1.3 (IN), TLS handshake, Certificate (11):</span><br><span class="line">* TLSv1.3 (IN), TLS handshake, CERT verify (15):</span><br><span class="line">* TLSv1.3 (IN), TLS handshake, Finished (20):</span><br><span class="line">* TLSv1.3 (OUT), TLS handshake, Finished (20):</span><br><span class="line">* SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256</span><br><span class="line">* ALPN, server accepted h2 as the protocol</span><br><span class="line">* Server certificate:</span><br><span class="line">*  subject: C=AU; ST=Some-State; O=Internet Widgits Pty Ltd</span><br><span class="line">*  start date: Apr 23 15:35:12 2024 GMT</span><br><span class="line">*  expire date: Apr 23 15:35:12 2025 GMT</span><br><span class="line">*  issuer: C=AU; ST=Some-State; O=Internet Widgits Pty Ltd</span><br><span class="line">*  SSL certificate verify result: self signed certificate (18), continuing anyway.</span><br><span class="line">* Using HTTP2, server supports multiplexing</span><br><span class="line">* Using Stream ID: 1 (easy handle 0x14e80bc00)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> GET /http2 HTTP/2</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Host: localhost:8081</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> user-agent: curl/7.86.0</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> accept: */*</span></span><br><span class="line"><span class="meta">&gt;</span></span><br><span class="line">* Connection state changed (MAX_CONCURRENT_STREAMS == 250)!</span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Working... 3</span><br><span class="line">Working... 4</span><br><span class="line">Client has disconnected. Stopping task.  5</span><br></pre></td></tr></table></figure><p>Again, the server detects the cancellation while the handler is still running.</p><h4 id="XHR-1"><a href="#XHR-1" class="headerlink" title="XHR"></a>XHR</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> xhr = <span class="keyword">new</span> XMLHttpRequest();</span><br><span class="line">xhr.open(<span class="string">"GET"</span>, <span class="string">"https://localhost:8081/http2"</span>);</span><br><span class="line">xhr.onreadystatechange = <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (xhr.readyState === XMLHttpRequest.DONE) &#123;</span><br><span class="line">        <span class="keyword">if</span> (xhr.status === <span class="number">200</span>) &#123;</span><br><span class="line">            <span class="built_in">console</span>.log(<span class="string">"Response:"</span>, xhr.responseText);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">console</span>.error(<span class="string">"Request failed with status:"</span>, xhr.status);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">xhr.send();</span><br><span class="line">xhr.abort(); <span class="comment">// after 6 seconds</span></span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Working... 3</span><br><span class="line">Working... 4</span><br><span class="line">Working... 5</span><br><span class="line">Working... 6</span><br><span class="line">Client has disconnected. Stopping task.  7</span><br></pre></td></tr></table></figure><h3 id="Offline"><a href="#Offline" class="headerlink" title="Offline"></a>Offline</h3><p>What if the client disappears without a clean close—for example, the device loses Wi‑Fi or switches to airplane mode? In that case the server might not receive a TCP FIN/RST (or an HTTP/2 reset) immediately.</p><p>We can use another device to test it:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. setup the WiFi</span></span><br><span class="line"><span class="comment"># 1. open the browser and go to http://localhost:8080/http1</span></span><br><span class="line"><span class="comment"># 2. close the WiFi</span></span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Working... 3</span><br><span class="line">Working... 4</span><br><span class="line">Working... 5</span><br><span class="line">Working... 6</span><br><span class="line">Working... 7</span><br><span class="line">Working... 8</span><br><span class="line">Working... 9</span><br><span class="line">[GIN] 2024/04/23 - 23:49:49 | 200 | 10.011190667s |   192.168.0.105 | GET      "/http1"</span><br></pre></td></tr></table></figure><p>In this run, the server keeps working and completes the handler; from the server’s perspective, nothing was canceled.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>At a high level, HTTP/1.1 doesn’t have an explicit, per-request “cancel” signal. What you usually get is “the connection went away”, and the server only learns that once the TCP stack (or the application runtime) notices the close.</p><p>HTTP/2 and HTTP/3 are different: they support stream-level cancellation. A client can reset a single stream (e.g., via a <code>RST_STREAM</code> frame), which makes it much easier for the server to stop work promptly.</p><p>In practice:</p><ul><li><strong>HTTP/1.1</strong>: “Cancellation” is typically the client closing the TCP connection. The server may only notice on read/write, or when the runtime surfaces the disconnect (for Go, that means <code>Request.Context()</code> is canceled).</li><li><strong>HTTP/2 and HTTP/3</strong>: The client can explicitly cancel a request by resetting the stream, so the server can stop work sooner and more reliably.</li></ul><p>No matter the protocol, cancellation is only useful if your handler checks for it during long-running work.</p><h2 id="With-Nginx"><a href="#With-Nginx" class="headerlink" title="With Nginx"></a>With Nginx</h2><p>We usually use Nginx as a reverse proxy between the client and the server.</p><p>Since HTTP/2 is commonly used on the client-to-Nginx side, after integrating Nginx the traffic flow typically looks like this:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Client - HTTP/2 -&gt; Nginx - HTTP/1.1 -&gt; Server</span><br></pre></td></tr></table></figure><p>or<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Client - HTTP/2 -&gt; Nginx - HTTP/2 -&gt; Server</span><br></pre></td></tr></table></figure></p><p>So we prepare an Nginx configuration in <code>nginx.conf</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"># HTTP block</span><br><span class="line">http &#123;</span><br><span class="line">  error_log /tmp/https_error.log debug;  # Specify a custom path and log level</span><br><span class="line"></span><br><span class="line">    # HTTPS server</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 443 ssl http2;</span><br><span class="line">        server_name local_nginx_http_2;</span><br><span class="line"></span><br><span class="line">        ssl_certificate /tmp/server.crt;</span><br><span class="line">        ssl_certificate_key /tmp/server.key;</span><br><span class="line"></span><br><span class="line">        location /http1 &#123;</span><br><span class="line">            proxy_pass http://127.0.0.1:8080;</span><br><span class="line">            proxy_set_header Host $host;</span><br><span class="line">            proxy_http_version 1.1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location /http2 &#123;</span><br><span class="line">            proxy_pass https://127.0.0.1:8081;</span><br><span class="line">            proxy_set_header Host $host;</span><br><span class="line">            proxy_ssl_verify off;  # Add this line to disable SSL verification for the proxy</span><br><span class="line">        &#125;        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Above (without a proxy), we saw that both curl and XHR can trigger graceful cancellation.</p><p>In this section, to keep things simple, we’ll only use curl to test cancellation through Nginx.</p><h4 id="Nginx-HTTP-1-1-gt-Server"><a href="#Nginx-HTTP-1-1-gt-Server" class="headerlink" title="Nginx - HTTP/1.1 -&gt; Server"></a>Nginx - HTTP/1.1 -&gt; Server</h4><p>The curl command is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https://localhost/http1 --insecure</span><br><span class="line">^C # after 6 seconds</span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Working... 3</span><br><span class="line">Working... 4</span><br><span class="line">Working... 5</span><br><span class="line">Client has disconnected. Stopping task.  6</span><br></pre></td></tr></table></figure><p>This shows the upstream server still observes the cancellation.</p><h4 id="Nginx-HTTP-2-gt-Server"><a href="#Nginx-HTTP-2-gt-Server" class="headerlink" title="Nginx - HTTP/2 -&gt; Server"></a>Nginx - HTTP/2 -&gt; Server</h4><p>The curl command is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https://localhost/http2 --insecure</span><br><span class="line">^C # after 6 seconds</span><br></pre></td></tr></table></figure><p>The server log is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Working... 0</span><br><span class="line">Working... 1</span><br><span class="line">Working... 2</span><br><span class="line">Working... 3</span><br><span class="line">Working... 4</span><br><span class="line">Working... 5</span><br><span class="line">Client has disconnected. Stopping task.  6</span><br></pre></td></tr></table></figure><p>This shows the upstream server still observes the cancellation.</p><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>When there is an Nginx reverse proxy sitting between the client and the server, handling client requests and forwarding them to the server, the behavior upon cancellation can vary depending on the protocols used between the client and Nginx, and between Nginx and the server. Let’s consider the two scenarios:</p><h4 id="Scenario-1-Client-HTTP-2-gt-Nginx-gt-Server-HTTP-1-1"><a href="#Scenario-1-Client-HTTP-2-gt-Nginx-gt-Server-HTTP-1-1" class="headerlink" title="Scenario 1: Client (HTTP/2) -&gt; Nginx -&gt; Server (HTTP/1.1)"></a>Scenario 1: Client (HTTP/2) -&gt; Nginx -&gt; Server (HTTP/1.1)</h4><ol><li>Client Cancels Request: The client sends a request using HTTP/2 and cancels it by sending a RST_STREAM frame to Nginx.</li><li>Nginx Behavior: Nginx can observe the reset from the client. Toward the upstream (HTTP/1.1), it can’t forward a per-request reset frame, so it either keeps the upstream request running and drops the response later, or it closes the upstream connection to force the server to observe a disconnect. Which path it takes depends on configuration and internal behavior.</li><li>Server Side: If Nginx closes the upstream connection, the server sees a disconnect and can stop work (e.g., via request context cancellation). If Nginx keeps the upstream request running, the server may do unnecessary work even though the client is already gone.</li></ol><h4 id="Scenario-2-Client-HTTP-2-gt-Nginx-gt-Server-HTTP-2"><a href="#Scenario-2-Client-HTTP-2-gt-Nginx-gt-Server-HTTP-2" class="headerlink" title="Scenario 2: Client (HTTP/2) -&gt; Nginx -&gt; Server (HTTP/2)"></a>Scenario 2: Client (HTTP/2) -&gt; Nginx -&gt; Server (HTTP/2)</h4><ol><li>Client Cancels Request: As before, the client sends a request using HTTP/2 and cancels it by sending a RST_STREAM frame.</li><li>Nginx Behavior: Because both legs use HTTP/2, Nginx can forward the cancellation to the upstream by resetting the corresponding stream.</li><li>Server Side: The server receives an explicit stream reset and can stop work immediately, which is both clearer and more efficient.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Suppose we’re writing an HTTP server with an endpoint that takes a long time to finish.&lt;/p&gt;
&lt;p&gt;A client may start a request and then canc
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="XHR" scheme="https://xhinliang.github.io/tags/XHR/"/>
    
      <category term="HTTP" scheme="https://xhinliang.github.io/tags/HTTP/"/>
    
      <category term="HttpClient" scheme="https://xhinliang.github.io/tags/HttpClient/"/>
    
      <category term="Nginx" scheme="https://xhinliang.github.io/tags/Nginx/"/>
    
      <category term="HTTP/2" scheme="https://xhinliang.github.io/tags/HTTP-2/"/>
    
      <category term="Cancel" scheme="https://xhinliang.github.io/tags/Cancel/"/>
    
      <category term="Cancellation" scheme="https://xhinliang.github.io/tags/Cancellation/"/>
    
  </entry>
  
  <entry>
    <title>本地限流器实战：四种经典算法与实现思路</title>
    <link href="https://xhinliang.github.io/2023/12/backend/rate-limiter-in-action-zh/"/>
    <id>https://xhinliang.github.io/2023/12/backend/rate-limiter-in-action-zh/</id>
    <published>2023-12-29T01:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p>高 QPS 的后端服务，几乎一定需要“限流”（Rate Limiting）。</p><p>这里讨论的是<strong>本地限流器（local rate limiter）</strong>：限流状态只存在于<strong>单个进程/单个实例</strong>中，不和其他实例共享。</p><p>它的目标很朴素：</p><ul><li>在流量突增时保护服务自身（CPU/线程池/DB 连接/下游依赖）</li><li>让系统吞吐更可控，避免被瞬时尖峰打穿</li></ul><p>本地限流最常见的四种算法：</p><ol><li>固定窗口（Fixed Window）</li><li>滑动窗口（Sliding / Floating Window）</li><li>漏桶（Leaky Bucket）</li><li>令牌桶（Token Bucket）</li></ol><p>下面逐个讲它们的“直觉、优缺点、以及一个简化实现”。</p><blockquote><p>提醒：示例代码为了讲清楚思路，刻意简化了很多工程细节（时间单位、溢出、并发性能、时钟回拨、分布式场景等）。</p></blockquote><h2 id="1-固定窗口（Fixed-Window）"><a href="#1-固定窗口（Fixed-Window）" class="headerlink" title="1. 固定窗口（Fixed Window）"></a>1. 固定窗口（Fixed Window）</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>把时间切成一个个<strong>固定长度的窗口</strong>（比如 1 秒一个窗口），每个窗口允许最多 N 次请求。</p><h3 id="典型问题：窗口边界突刺"><a href="#典型问题：窗口边界突刺" class="headerlink" title="典型问题：窗口边界突刺"></a>典型问题：窗口边界突刺</h3><p>固定窗口的最大缺点是<strong>边界效应</strong>。</p><p>举个极端例子：限制是 <strong>1000/s</strong>。</p><ul><li>前 999ms 一次请求都没有</li><li>在最后 1ms 突然来了 1000 个请求（全放行）</li><li>下一秒的第 1ms 又来了 1000 个请求（也全放行）</li></ul><p>结果就是：在非常短的时间内，系统可能承受接近 2000 的瞬时突刺。</p><h3 id="缓解方式：把窗口切得更细"><a href="#缓解方式：把窗口切得更细" class="headerlink" title="缓解方式：把窗口切得更细"></a>缓解方式：把窗口切得更细</h3><p>比如你要实现 1000/s，可以把 1 秒切成 100 个 10ms 小窗口，每个 10ms 允许 10 次。</p><p>窗口越细，越平滑，但实现也更复杂、开销也更高。</p><h3 id="Java-简化实现"><a href="#Java-简化实现" class="headerlink" title="Java 简化实现"></a>Java 简化实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FixedWindowRateLimiter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> capacity;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> windowSizeInMillis;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> windowStart;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> usedCapacity;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FixedWindowRateLimiter</span><span class="params">(<span class="keyword">long</span> capacity, <span class="keyword">long</span> windowSizeInMillis)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        <span class="keyword">this</span>.windowSizeInMillis = windowSizeInMillis;</span><br><span class="line">        <span class="keyword">this</span>.windowStart = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">this</span>.usedCapacity = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">if</span> (now - windowStart &gt; windowSizeInMillis) &#123;</span><br><span class="line">            windowStart = now;</span><br><span class="line">            usedCapacity = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (usedCapacity &lt; capacity) &#123;</span><br><span class="line">            usedCapacity++;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-滑动窗口（Sliding-Floating-Window）"><a href="#2-滑动窗口（Sliding-Floating-Window）" class="headerlink" title="2. 滑动窗口（Sliding / Floating Window）"></a>2. 滑动窗口（Sliding / Floating Window）</h2><h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><p>滑动窗口的目标是解决固定窗口的“边界突刺”。</p><p>做法之一是：记录最近一个窗口大小（比如最近 1 秒）内的请求时间戳；每次请求进来时，把超过窗口的记录丢掉，然后看当前窗口内数量是否超过阈值。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>优点：更平滑，更符合“过去 1 秒最多 N 次”的直觉</li><li>缺点：需要存储时间戳（内存/CPU 开销），高并发下需要更好的数据结构与锁策略</li></ul><h3 id="Java-简化实现-1"><a href="#Java-简化实现-1" class="headerlink" title="Java 简化实现"></a>Java 简化实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Deque;</span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SlidingWindowRateLimiter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> windowSizeInMillis;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxRequests;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Deque&lt;Long&gt; timestamps = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SlidingWindowRateLimiter</span><span class="params">(<span class="keyword">long</span> windowSizeInMillis, <span class="keyword">int</span> maxRequests)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.windowSizeInMillis = windowSizeInMillis;</span><br><span class="line">        <span class="keyword">this</span>.maxRequests = maxRequests;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">allowRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!timestamps.isEmpty() &amp;&amp; now - timestamps.peekFirst() &gt; windowSizeInMillis) &#123;</span><br><span class="line">            timestamps.pollFirst();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (timestamps.size() &lt; maxRequests) &#123;</span><br><span class="line">            timestamps.addLast(now);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-漏桶（Leaky-Bucket）"><a href="#3-漏桶（Leaky-Bucket）" class="headerlink" title="3. 漏桶（Leaky Bucket）"></a>3. 漏桶（Leaky Bucket）</h2><h3 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h3><p>把请求想象成倒进桶里的水。</p><ul><li>桶有容量上限（满了就拒绝）</li><li>桶底部以一个<strong>固定速率</strong>漏水（对应“以恒定速率处理请求”）</li></ul><p>这会把突发流量“削峰填谷”，让输出更平滑。</p><h3 id="直觉"><a href="#直觉" class="headerlink" title="直觉"></a>直觉</h3><p>漏桶更像是：你限制的是“处理速率”，而不是“允许速率”。</p><h3 id="Java-简化实现（以-token-水量表示）"><a href="#Java-简化实现（以-token-水量表示）" class="headerlink" title="Java 简化实现（以 token/水量表示）"></a>Java 简化实现（以 token/水量表示）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeakyBucketRateLimiter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> capacity;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> leakRateInMillis;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> available;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastLeakTs;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LeakyBucketRateLimiter</span><span class="params">(<span class="keyword">long</span> capacity, <span class="keyword">long</span> leakRateInMillis)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        <span class="keyword">this</span>.leakRateInMillis = leakRateInMillis;</span><br><span class="line">        <span class="keyword">this</span>.available = capacity;</span><br><span class="line">        <span class="keyword">this</span>.lastLeakTs = System.currentTimeMillis();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">allowRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">long</span> elapsed = now - lastLeakTs;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> leaked = elapsed / leakRateInMillis;</span><br><span class="line">        <span class="keyword">if</span> (leaked &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            available = Math.min(capacity, available + leaked);</span><br><span class="line">            lastLeakTs = now;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (available &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            available--;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：这里的“available”实现只是为了方便理解。生产实现里你可能会用更精确的方式表示速率与时间。</p></blockquote><h2 id="4-令牌桶（Token-Bucket）"><a href="#4-令牌桶（Token-Bucket）" class="headerlink" title="4. 令牌桶（Token Bucket）"></a>4. 令牌桶（Token Bucket）</h2><h3 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h3><p>令牌桶是工程上最常用的一种：</p><ul><li>桶里有令牌，最大容量为 <code>burst</code></li><li>系统按固定速率往桶里“补令牌”</li><li>每个请求消耗一个令牌</li><li>没令牌就拒绝（或排队等待）</li></ul><p>它和漏桶的区别在于：<strong>令牌桶天然允许一定程度的突发（burst）</strong>：只要桶里之前攒了令牌，就可以瞬间放行一批。</p><h3 id="常见实现技巧"><a href="#常见实现技巧" class="headerlink" title="常见实现技巧"></a>常见实现技巧</h3><p>很多人一开始会用一个“补令牌线程”。但实际上可以不用线程：</p><ul><li>记录上次补给时间 <code>lastSupply</code></li><li>每次请求到来时，按 <code>now - lastSupply</code> 计算应该补多少令牌</li></ul><p>这样更简单，也更省资源。</p><h3 id="Java-简化实现（接近原思路）"><a href="#Java-简化实现（接近原思路）" class="headerlink" title="Java 简化实现（接近原思路）"></a>Java 简化实现（接近原思路）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicLong;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenBucketRateLimiter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> latestSupplySecond = <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong tokenCount = <span class="keyword">new</span> AtomicLong();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> permitsPerSecond;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TokenBucketRateLimiter</span><span class="params">(<span class="keyword">int</span> permitsPerSecond)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.permitsPerSecond = permitsPerSecond;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canDo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> nowSec = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());</span><br><span class="line">        <span class="keyword">long</span> diff = nowSec - latestSupplySecond;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (diff == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> tokenCount.decrementAndGet() &gt;= <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            <span class="keyword">long</span> nowSec2 = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());</span><br><span class="line">            <span class="keyword">long</span> diff2 = nowSec2 - latestSupplySecond;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (diff2 &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> tokenCount.decrementAndGet() &gt;= <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 距离上次补给过了 diff2 秒</span></span><br><span class="line">            <span class="keyword">long</span> shouldSupply = diff2 * (<span class="keyword">long</span>) permitsPerSecond;</span><br><span class="line">            latestSupplySecond = nowSec2;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 这里没做“桶上限”（burst）限制，是为了示例简化</span></span><br><span class="line">            <span class="keyword">return</span> tokenCount.addAndGet(shouldSupply - <span class="number">1</span>) &gt;= <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="什么时候该用哪种？（快速建议）"><a href="#什么时候该用哪种？（快速建议）" class="headerlink" title="什么时候该用哪种？（快速建议）"></a>什么时候该用哪种？（快速建议）</h2><ul><li>想要实现简单、开销低：固定窗口（但要接受边界突刺）</li><li>想要更严格的“过去 1 秒最多 N 次”：滑动窗口</li><li>想要输出速率尽量平滑：漏桶</li><li>既要限速，又要允许一定突发（最常见）：令牌桶</li></ul><p>如果你后面要做“分布式限流”（多实例共享额度），那就需要把状态放到 Redis/网关/Sidecar 等地方，算法本身也需要考虑一致性与性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;高 QPS 的后端服务，几乎一定需要“限流”（Rate Limiting）。&lt;/p&gt;
&lt;p&gt;这里讨论的是&lt;strong&gt;本地限流器（local rate limiter）&lt;/strong&gt;：限流状态只存在于&lt;strong&gt;单个进程/单个实例&lt;/strong&gt;中，不和其他实例
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Java" scheme="https://xhinliang.github.io/tags/Java/"/>
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="Chinese" scheme="https://xhinliang.github.io/tags/Chinese/"/>
    
      <category term="Concurrent" scheme="https://xhinliang.github.io/tags/Concurrent/"/>
    
      <category term="RateLimiter" scheme="https://xhinliang.github.io/tags/RateLimiter/"/>
    
  </entry>
  
  <entry>
    <title>Rate Limiter In Action</title>
    <link href="https://xhinliang.github.io/2023/12/backend/rate-limiter-in-action/"/>
    <id>https://xhinliang.github.io/2023/12/backend/rate-limiter-in-action/</id>
    <published>2023-12-29T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p>Backend systems that handle high QPS often need a local rate limiter to protect themselves.</p><p>Here, “local” means the limiter runs within a single process and does not coordinate state across instances.</p><p>Below are a few classic local rate-limiting strategies that show up in production code:</p><ul><li>Fixed Window</li><li>Floating Window</li><li>Leaky Bucket</li><li>Token Bucket</li></ul><p>We’ll go through them one by one.</p><h2 id="Fixed-Window"><a href="#Fixed-Window" class="headerlink" title="Fixed Window"></a>Fixed Window</h2><p>Fixed Window rate limiting splits time into discrete windows and allows up to <code>N</code> actions per window.</p><p>The downside is that throughput can be uneven around window boundaries. Imagine we set a limit of 1000 requests per second:</p><ul><li>No requests arrive in the first 999 ms.</li><li>Then 1000 requests arrive within 1 ms and all are allowed.</li><li>Immediately after that, another 1000 requests arrive within 1 ms and are also allowed because they land in the next window.</li></ul><p>One common mitigation is to make the window smaller (at the cost of a little more bookkeeping). For example, for “1000 per second”, you can split one second into 100 × 10 ms windows and limit 10 actions per 10 ms. The smaller the window, the smoother the limiter behaves.</p><p>Here is a minimal Java implementation:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FixedWindowRateLimiter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> capacity;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> windowSizeInMillis;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> windowStart;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> usedCapacity;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FixedWindowRateLimiter</span><span class="params">(<span class="keyword">long</span> capacity, <span class="keyword">long</span> windowSizeInMillis)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        <span class="keyword">this</span>.windowSizeInMillis = windowSizeInMillis;</span><br><span class="line">        <span class="keyword">this</span>.windowStart = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">this</span>.usedCapacity = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> currentTime = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">if</span> (currentTime - windowStart &gt; windowSizeInMillis) &#123;</span><br><span class="line">            windowStart = currentTime;</span><br><span class="line">            usedCapacity = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (usedCapacity &lt; capacity) &#123;</span><br><span class="line">            usedCapacity++;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This implementation tracks a window start time plus a counter. When the window expires it resets the counter; otherwise it allows the request until the capacity is reached.</p><h2 id="Floating-Window"><a href="#Floating-Window" class="headerlink" title="Floating Window"></a>Floating Window</h2><p>The Floating Window rate limiter smooths out Fixed Window’s boundary bursts by evaluating the request count over a moving time range.</p><p>It’s more expensive to implement because it typically requires tracking individual request timestamps (or maintaining buckets that approximate them).</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FloatingWindowRateLimiter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> windowSizeInMillis;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxRequests;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Deque&lt;Long&gt; requestTimestamps = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FloatingWindowRateLimiter</span><span class="params">(<span class="keyword">long</span> windowSizeInMillis, <span class="keyword">int</span> maxRequests)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.windowSizeInMillis = windowSizeInMillis;</span><br><span class="line">        <span class="keyword">this</span>.maxRequests = maxRequests;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">allowRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> currentTime = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">while</span> (!requestTimestamps.isEmpty() &amp;&amp; currentTime - requestTimestamps.peek() &gt; windowSizeInMillis) &#123;</span><br><span class="line">            requestTimestamps.poll();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (requestTimestamps.size() &lt; maxRequests) &#123;</span><br><span class="line">            requestTimestamps.add(currentTime);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Leaky-Bucket"><a href="#Leaky-Bucket" class="headerlink" title="Leaky Bucket"></a>Leaky Bucket</h2><p>The Leaky Bucket algorithm models rate limiting as a bucket that “leaks” at a constant rate. Bursts can be absorbed up to the bucket capacity, but the outflow rate stays stable.</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeakyBucketRateLimiter</span> &#123;</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> capacity;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> leakRateInMillis;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> availableTokens;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastLeakTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LeakyBucketRateLimiter</span><span class="params">(<span class="keyword">long</span> capacity, <span class="keyword">long</span> leakRateInMillis)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        <span class="keyword">this</span>.leakRateInMillis = leakRateInMillis;</span><br><span class="line">        <span class="keyword">this</span>.availableTokens = capacity;</span><br><span class="line">        <span class="keyword">this</span>.lastLeakTimestamp = System.currentTimeMillis();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> synchronized <span class="keyword">boolean</span> <span class="title">allowRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> currentTime = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">long</span> timeSinceLastLeak = currentTime - lastLeakTimestamp;</span><br><span class="line">        <span class="keyword">long</span> tokensToLeak = timeSinceLastLeak / leakRateInMillis;</span><br><span class="line">        <span class="keyword">if</span> (tokensToLeak &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            availableTokens = Math.<span class="built_in">min</span>(capacity, availableTokens + tokensToLeak);</span><br><span class="line">            lastLeakTimestamp = currentTime;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (availableTokens &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            availableTokens--;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Token-Bucket"><a href="#Token-Bucket" class="headerlink" title="Token Bucket"></a>Token Bucket</h2><p>Token Bucket uses a bucket of tokens. Tokens are replenished at a fixed rate.</p><p>The backend system consumes one token per request. When the bucket runs out of tokens, requests are rejected or delayed.</p><p>Compared with Leaky Bucket, Token Bucket is often more flexible because it allows short bursts (up to the bucket size) while still enforcing an average rate. It’s also convenient to implement by tracking only the token count and the last refill timestamp, without a dedicated refill thread.</p><p>Here is a simple implementation:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RateLimiter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> latestSupplyTimestamp = <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong tokenCount = <span class="keyword">new</span> AtomicLong();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> performPerSecond;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RateLimiter</span><span class="params">(<span class="keyword">int</span> performPerSecond)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.performPerSecond = performPerSecond;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canDo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> now = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());</span><br><span class="line">        <span class="keyword">long</span> diff = now - latestSupplyTimestamp;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (diff == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> tokenCount.decrementAndGet() &gt; <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (diff &gt;= <span class="number">1000L</span>) &#123;</span><br><span class="line">                latestSupplyTimestamp = now;</span><br><span class="line">                tokenCount.set(performPerSecond - <span class="number">1L</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 0 &lt; diff &lt; 1000</span></span><br><span class="line">            <span class="keyword">long</span> shouldSupply = diff * performPerSecond - <span class="number">1L</span>;</span><br><span class="line">            latestSupplyTimestamp = now;</span><br><span class="line">            <span class="keyword">return</span> tokenCount.addAndGet(shouldSupply) &gt;= <span class="number">0L</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Backend systems that handle high QPS often need a local rate limiter to protect themselves.&lt;/p&gt;
&lt;p&gt;Here, “local” means the limiter runs w
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Java" scheme="https://xhinliang.github.io/tags/Java/"/>
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="Concurrent" scheme="https://xhinliang.github.io/tags/Concurrent/"/>
    
      <category term="Distributed" scheme="https://xhinliang.github.io/tags/Distributed/"/>
    
  </entry>
  
  <entry>
    <title>How to Build a Scalable Live Streaming Interactive Service - Part II</title>
    <link href="https://xhinliang.github.io/2022/03/backend/livestreaming/scalable-interactive-service-2/"/>
    <id>https://xhinliang.github.io/2022/03/backend/livestreaming/scalable-interactive-service-2/</id>
    <published>2022-03-27T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>With faster networks and the push from COVID-era remote life, live streaming has become popular again.</p><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0obi03jucj20yv0u0dkg.jpg" alt></p><p>In Part I, we discussed signal types and interaction/connection modeling. In Part II, we’ll focus on scaling methods for interactive services.</p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>In backend systems, services are often described as either stateless or stateful.</p><p>A stateless service can process a request using only the information carried with that request and does not rely on state from earlier requests.</p><p>A stateful service relies on state accumulated from earlier requests (in memory or in storage) in addition to the data carried with the current request.</p><p>This distinction matters because stateless workloads can usually be load-balanced freely, while stateful workloads often require affinity (sticky sessions, consistent hashing, sharding) and are harder to scale out.</p><p>As a result, bottlenecks often come from stateful components.</p><p>The interactive service of live streaming is composed of several parts:</p><ul><li>Storage<ul><li>Relational databases</li></ul></li><li>Caches<ul><li>Structured collections</li><li>Key-value caches</li></ul></li><li>Compute Services<ul><li>Scheduled tasks</li></ul></li><li>Payment Service<ul><li>Account transfers</li></ul></li><li>HTTP Servers</li><li>Signaling Servers</li></ul><p>The scaling methods discussed below largely target these components.</p><h2 id="Scaling-Methods"><a href="#Scaling-Methods" class="headerlink" title="Scaling Methods"></a>Scaling Methods</h2><p>Distributed systems are usually built from many small instances. To increase capacity and throughput, we scale out by adding instances—but that only works if each component is designed to scale.</p><h3 id="Relational-Databases"><a href="#Relational-Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h3><p>As an internet service, relational databases are usually the source of truth. The typical scaling approach is sharding: split data into partitions and store them in different databases.</p><p>Live streaming data has a few useful characteristics that make sharding more approachable:</p><ul><li>Timeliness: live streams have a bounded lifetime. The longest sessions are under 480 hours, and the average duration is under 5 minutes. For online traffic, we mostly care about ongoing rooms and recent history, so we can keep only the most recent ~4 weeks in the “hot” dataset.</li><li>Geography: users often consume streams that are geographically close or at least within the same region. That makes it natural to shard records by the host’s region.</li></ul><p>One practical routing scheme is:</p><ol><li>Generate the live streaming room ID based on the host (anchor) ID, and generate the host ID with a regional prefix (based on where the account was registered).</li><li>Insert the room record into the database shard that the room ID maps to.</li><li>When querying an ongoing room by room ID, route to the owning shard directly.</li><li>When querying an offline (ended) room by room ID, route to an append-only archive database that can be scaled independently by adding nodes.</li></ol><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0oertdxcbj216h0u0tc7.jpg" alt></p><h3 id="Structured-Collections"><a href="#Structured-Collections" class="headerlink" title="Structured Collections"></a>Structured Collections</h3><p>We use Redis to implement structured collections (lists, sets, sorted sets, hashes). Scaling this layer mostly means scaling Redis itself: add more shards and spread keys across them.</p><p>In our setup, we don’t use the official <code>Redis Cluster</code> protocol. Instead, we use Twemproxy to shard reads/writes across a fleet of Redis master/replica nodes managed by Redis Sentinel. So the “Redis cluster” below means “Twemproxy + Redis + Sentinel”, not the built-in Redis Cluster feature.</p><p>We also build two identical Redis clusters in different AZs (Availability Zones) and designate one as the primary via configuration.</p><p>Writes to the primary cluster are replicated to the secondary cluster through Kafka.</p><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0ofdhqbrcj215g0u078o.jpg" alt></p><p>In the diagram above, notice that <code>Twemproxy</code> is stateless. You can scale it horizontally, and it shouldn’t be the bottleneck of the Redis layer.</p><p>In practice, Redis keys are often derived from the room ID, which distributes load naturally once you have enough shards. The remaining work is to avoid creating big keys and hot keys.</p><p>Big keys often come from writes to large sorted sets and hashes. In live streaming, two practical ways to avoid big keys are:</p><ul><li>Trim collections to a fixed size once they exceed a threshold.</li><li>Limit write rate with a rate limiter (or other backpressure mechanism).</li></ul><p>Hot keys usually come from reads. To mitigate hot keys, you can:</p><ul><li>Add another cache layer for derived results (for example, a short-lived per-process local cache).</li><li>Store the same logical value under multiple physical keys and choose a key randomly when reading. In most sharding schemes, those keys land on different Redis nodes, so the read load spreads out.</li></ul><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0olq9yfdqj21aw0u00wc.jpg" alt></p><p>On large social-style platforms, hotspot detection plus special-case policies (rate limits, circuit breakers, pre-warming) are also common tools.</p><h3 id="K-V-Caches"><a href="#K-V-Caches" class="headerlink" title="K-V Caches"></a>K-V Caches</h3><p>We use Memcached as our key-value cache. Most values cached in Memcached are derived from the relational databases.</p><p>Unlike Redis, we don’t use a proxy in front of Memcached. Reads are routed by the client library (typically via consistent hashing).</p><p>Memcached has no built-in primary/replica role. To improve availability, we run multiple equivalent clusters for the same purpose, and treat them as peers.</p><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0ogzrpxaej20xn0u077z.jpg" alt></p><p>When a client tries to read a value from Memcached, it follows a flow like this:</p><ol><li>Randomly pick one cluster in the same AZ, then hash the key and route to a Memcached node.</li><li>Read from that node; if it’s a hit, return immediately.</li><li>On a miss, randomly pick another cluster in the same AZ and route to its node.</li><li>Read again; if it’s a hit, write the value back to the node from step 2 (to heal the miss) and then return.</li><li>If it’s still a miss, call a service like <code>DbReader</code> to read from the database, then write back to both nodes (step 4 and step 2).</li></ol><p>One more detail: different clusters use different hash salts. That way, even if two clusters have the same number of nodes, “node 0” in cluster A does not tend to store the same set of keys as “node 0” in cluster B.</p><p>With this approach, availability is high:</p><ul><li>If one Memcached node is down, clients can still read most keys from another cluster and heal the missing copy via write-back.</li><li>If two nodes in different clusters are down, the expected lost portion is roughly <code>1/M * 1/N</code> (where <code>M</code> and <code>N</code> are the node counts of the two clusters).</li></ul><h3 id="Scheduled-Tasks"><a href="#Scheduled-Tasks" class="headerlink" title="Scheduled Tasks"></a>Scheduled Tasks</h3><p>Running periodic business jobs is common in live streaming backends. Many of these jobs share two properties:</p><ul><li>They should know which rooms are ongoing;</li><li>Their logic can be partitioned by live room.</li></ul><p>Ongoing live rooms are stored in the online databases, but the naïve way to find them is scanning tables. That is expensive—especially when multiple jobs and services scan concurrently.</p><p>We can use a service named “OngoingQuery” to protect the databases.</p><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0om5ilpczj21fl0u0dka.jpg" alt></p><p>The OngoingQuery service stores the room IDs of ongoing live streams.</p><p>It periodically scans the database for a full refresh, and tails database binlogs for near-real-time updates. With this setup, its cache eventually converges to the database state.</p><p>When we deploy a sharded job (multiple worker processes), workers register themselves in ZooKeeper and then run periodically:</p><ul><li>Query ZooKeeper to learn how many worker instances are currently running.</li><li>Determine the worker’s own index among the instances.</li><li>Request OngoingQuery for the worker’s partition of the ongoing room list.</li><li>Process only that partition.</li></ul><p>With this partitioning abstraction, one logical job can be split across multiple processes and run concurrently.</p><h3 id="Account-Transporting"><a href="#Account-Transporting" class="headerlink" title="Account Transporting"></a>Account Transporting</h3><p>Almost every live streaming platform will support gift features.</p><p>The core of gifting is money movement: when a viewer sends a gift, balance is transferred from the viewer’s account to the host (anchor)’s account.</p><p>For scalability, user balances are often sharded across databases. A direct viewer → host transfer therefore tends to become a distributed transaction, which is expensive.</p><p>One way to reduce the cost is to introduce a virtual “room account”.</p><p><img src="/uploads/persister-scalable-interactive-service-2--e6c9d24ely1h0omsn1ul4j21570u0jw6.jpg" alt></p><p>When a live room begins, create a virtual room account in each balance database shard.</p><p>During the live stream, gifts become transfers between the viewer’s account and the virtual room account in the same shard (a local transaction).</p><p>When the live room ends, a service like “Settler” collects all virtual room accounts and performs settlement to the host’s account (the cross-shard part happens once, in batch).</p><h3 id="HTTP-Service"><a href="#HTTP-Service" class="headerlink" title="HTTP Service"></a>HTTP Service</h3><p>There are many load-balancing strategies for HTTP services, so I won’t spend much time on this topic.</p><p>One practical trick is to use an Nginx routing policy that sends the same room ID to the same service group, which can improve local-cache hit rate.</p><h3 id="Signaling-Servers"><a href="#Signaling-Servers" class="headerlink" title="Signaling Servers"></a>Signaling Servers</h3><p>We have talked about this in the <a href="https://xhinliang.win/2022/01/backend/livestreaming/scalable-interactive-service-1/" target="_blank" rel="noopener">How to Build a Scalable Live Streaming Interactive Service - Part I</a>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this article, we discussed scaling strategies for the stateful parts of a live streaming interactive service. Many people refer to these ideas simply as “sharding strategies”.</p><p>The common theme is simple: split a big thing into smaller, independent pieces.</p><p>In the next post, I’ll share my thoughts on building a multi-region (or cross-region) live streaming platform.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://en.wikipedia.org/wiki/Service_statelessness_principle#:~:text=Service%20statelessness%20is%20a%20design,their%20state%20data%20whenever%20possible" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Service_statelessness_principle#:~:text=Service%20statelessness%20is%20a%20design,their%20state%20data%20whenever%20possible</a>.</li><li><a href="https://www.proud2becloud.com/stateful-vs-stateless-the-good-the-bad-and-the-ugly/" target="_blank" rel="noopener">https://www.proud2becloud.com/stateful-vs-stateless-the-good-the-bad-and-the-ugly/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;With faster networks and the push from
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="LiveStreaming" scheme="https://xhinliang.github.io/tags/LiveStreaming/"/>
    
      <category term="Live" scheme="https://xhinliang.github.io/tags/Live/"/>
    
  </entry>
  
  <entry>
    <title>How to Build a Scalable Live Streaming Interactive Service - Part I</title>
    <link href="https://xhinliang.github.io/2022/01/backend/livestreaming/scalable-interactive-service-1/"/>
    <id>https://xhinliang.github.io/2022/01/backend/livestreaming/scalable-interactive-service-1/</id>
    <published>2022-01-15T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>With faster networks and the push from COVID-era remote life, live streaming has become popular again.</p><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0obi03jucj20yv0u0dkg.jpg" alt></p><p>I’ve been involved in building a live streaming platform, and the part I’ve spent the most time on is the “Interactive Service”. This series is a write-up of how I think about that component.</p><p>First, let’s define what “Interactive Service” means. A useful mental model is to split a live streaming platform into two subsystems: “Video Streaming” and “Interactive Service”.</p><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0oblby6l5j20z60u0tbv.jpg" alt></p><p>“Video Streaming” is the audio/video pipeline that viewers watch and listen to in (near) real time. It’s the foundation of live streaming—without it, there’s nothing to watch, and the room degenerates into a plain group chat.</p><p>“Interactive Service” is everything else: comments, gifts, e-commerce notifications, moderation actions, presence, and so on. It turns a stream from a one-way broadcast into a shared room where the host and audience can interact.</p><h2 id="Interactive-Service"><a href="#Interactive-Service" class="headerlink" title="Interactive Service"></a>Interactive Service</h2><p>I think of an “Interactive Service” as a full-featured ecosystem.</p><p>Both hosts and viewers generate signals (events/messages) to it, and it delivers those signals to other participants in the room. For example, a viewer <code>foo</code> can send a comment like “You look good”, and the interactive service will broadcast that comment to everyone else shortly after.</p><p>The interactive service can also generate events on its own. For example, it might periodically broadcast the current online user count so everyone can see how many viewers are in the room.</p><p>The core topic of this article is how to build an interactive service that delivers signals safely, quickly, and cost-effectively:</p><ul><li>Safe<ul><li>The interactive service should authenticate and authorize requests/connections.</li><li>Clients should not receive any signals they are not allowed to see.</li></ul></li><li>Quick<ul><li>The interactive service should deliver signals within a bounded, low latency—regardless of room size.</li></ul></li><li>Economic<ul><li>The interactive service should support very large rooms with huge audiences.</li><li>It should use resources efficiently: CPU, memory, disk, and network bandwidth.</li></ul></li></ul><h2 id="Signal-Types"><a href="#Signal-Types" class="headerlink" title="Signal Types"></a>Signal Types</h2><p>Before we discuss different ways to model an interactive service, it helps to classify the signals you need to deliver. In my experience, there are three types, and each tends to want a different implementation.</p><h3 id="State-Sync-Signal"><a href="#State-Sync-Signal" class="headerlink" title="State Sync Signal"></a>State Sync Signal</h3><p>Right after entering a room, clients need an initial snapshot of state (room metadata, configuration, pinned content, and so on). When that state changes, clients should be notified and update their local view.</p><p>A common implementation uses a “total + partial versions” scheme to describe business state changes.</p><p>In this scheme, both servers and clients track a total version plus multiple partial versions. Clients poll the total version periodically; if it differs from the local total version, the client fetches only the partial versions it is missing.</p><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0oc8bkeruj21d90u078y.jpg" alt></p><p>We call this model <code>SS Signal</code> for short.</p><h3 id="Time-Series-Signal"><a href="#Time-Series-Signal" class="headerlink" title="Time Series Signal"></a>Time Series Signal</h3><p>A Time Series Signal is a temporary event in a room. Events that happened before a client joined are usually not replayed, and missing a few time-series events typically should not break the experience.</p><p>There are multiple ways to implement time-series signals, but two common approaches are:</p><ul><li>Use a messaging system, such as Redis Pub/Sub or Kafka.</li><li>Store events in a time-series database and have clients fetch them periodically.</li></ul><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0oc8tju01j219g0u0djh.jpg" alt></p><p>We call this model <code>TS Signal</code> for short.</p><h3 id="Peer-Delivery-Signal"><a href="#Peer-Delivery-Signal" class="headerlink" title="Peer Delivery Signal"></a>Peer Delivery Signal</h3><p>SS signals and TS signals are designed to reach everyone in the same live room. When state changes or an action happens, these signals are delivered to all clients in that room. Peer delivery signals, in contrast, target only a subset of participants.</p><p>You can implement peer delivery by broadcasting and letting clients filter, but it tends to create performance issues (especially bandwidth waste) as rooms grow.</p><p>A more efficient approach is to treat peer delivery like instant messaging. With a typical IM system, you can locate which server the target client is connected to, so sending peer signals is both direct and efficient.</p><h2 id="Connection-Modeling"><a href="#Connection-Modeling" class="headerlink" title="Connection Modeling"></a>Connection Modeling</h2><p>The interactive service also needs a way for servers and clients to communicate. There are several common connection models; I’ll briefly outline a few.</p><h3 id="C-S-Modeling"><a href="#C-S-Modeling" class="headerlink" title="C/S Modeling"></a>C/S Modeling</h3><p>Traditional request/response is easy to scale and optimize, so many systems start with an HTTP-based client/server (C/S) model.</p><p>Can we use this model to implement an interactive service? Sure—especially for pull-based delivery.</p><p>A familiar example is WeChat: each account has a message list, and clients request unread messages via a cursor. After pulling messages, the client stores the latest cursor locally and uses it for the next request.</p><p>We can apply the same idea to an interactive service, with a few differences:</p><ul><li>Split servers into groups, classify clients by the live room they are watching, and route them to the correct server group.</li><li>Handle large rooms differently from small ones. Some rooms can have huge audiences, and a single group may become a bottleneck; those rooms often need extra fan-out capacity across multiple groups.</li></ul><h3 id="CDN-Modeling"><a href="#CDN-Modeling" class="headerlink" title="CDN Modeling"></a>CDN Modeling</h3><p>Once you have a pull-based C/S interactive service, it starts to resemble a CDN: lots of stateless nodes accepting requests, caching aggressively, and serving responses close to users.</p><p>CDN-style architectures can improve latency and throughput, so it can be useful to borrow the same ideas (or even reuse CDN infrastructure) to accept requests and deliver some classes of signals.</p><h3 id="Server-Push-Modeling"><a href="#Server-Push-Modeling" class="headerlink" title="Server-Push Modeling"></a>Server-Push Modeling</h3><p>When a room updates rapidly, server-push is often a better fit than repeated polling.</p><p>In this model, you might choose WebSocket for compatibility, QUIC for efficiency, or plain TCP for simplicity in a controlled environment.</p><p>The transport is relatively straightforward; the difficult part is load balancing stateful connections.</p><p>With classic HTTP request/response, each request is mostly independent, which makes load balancing simpler.</p><p>The “server grouping” idea from the C/S model also applies to server-push. Common approaches include:</p><ul><li>Split servers into groups and give each group its own endpoint. Clients choose the endpoint based on the live room ID.</li><li>Use (or implement) an application-level load balancing algorithm to assign and migrate connections.</li></ul><p><img src="/uploads/persister-how-to-build-a-scalable-live-streaming-interactive-service--e6c9d24ely1h0oc72dn9yj21fc0u0afn.jpg" alt></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Building a live streaming platform is a large project, so I can’t capture all of my thoughts in a single post.</p><p>In Part I, we discussed a few core concepts that will be useful later:</p><ul><li>The boundary between the Interactive Service and the Video Streaming service;</li><li>How live streaming signals differ from instant messages;</li><li>Different types of live streaming signals;</li><li>Different connection models for an interactive service.</li></ul><p>In Part II, I’ll share my thoughts on scaling methods for interactive services.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;With faster networks and the push from
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="LiveStreaming" scheme="https://xhinliang.github.io/tags/LiveStreaming/"/>
    
      <category term="Live" scheme="https://xhinliang.github.io/tags/Live/"/>
    
  </entry>
  
  <entry>
    <title>Me and My Game Life</title>
    <link href="https://xhinliang.github.io/2021/10/uncategorized/me-and-my-game-life/"/>
    <id>https://xhinliang.github.io/2021/10/uncategorized/me-and-my-game-life/</id>
    <published>2021-10-02T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.609Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/uploads/persister-me-and-my-game-life-Witcher-2020-05-19_5ec3eb24973b8_vedmak-3-dikaia-okhota-geralt-tsiri-triss-iennifer-ciri-gera-1056x594.jpg" alt="Witcher"></p><p><strong>Video Games: A Lifelong Journey</strong></p><p>Video games have been a significant part of my life. Recently, I finally finished “The Witcher 3: Wild Hunt”, a modern classic RPG.</p><p>Even though I love video games, I haven’t always had much time for them. I’ve played everything from classics like Contra and Ninja Gaiden to iconic PC games like Counter-Strike and Warcraft. I also spent a lot of time on handhelds such as the Game Boy Advance and PSP, getting lost in games like Pokémon and God of War.</p><p>Games have always given me hope, happiness, and room to imagine. But they also require time—which I didn’t have much of during school. I would fall in love with a game and then put it down after a few days to get back to studying.</p><p>As I grew older, homework turned into work deadlines. After a long day, I rarely had the energy for competitive games like Dota 2 or PUBG. Over time I drifted toward mobile games—partly because building a high-end PC has become prohibitively expensive.</p><p>“The Witcher 3: Wild Hunt” broke that pattern. It pulled me in enough that I spent nearly three months finishing it—my biggest gaming commitment since university.</p><p>That summer, I bought a 4K TV and started looking for a console that could make good use of it. I settled on a used Xbox One X, which turned out to be a great fit. I also picked up a few games known to run well at 4K/60 FPS, including “NBA 2K21,” “Dirt Rally 2.0,” “Titanfall 2,” and “Battlefield 5.”</p><p>Over the next three months, I spent most of my gaming time on “Battlefield 5” and “The Witcher 3.” “Battlefield 5” looked great, but the campaign was short and aiming in an FPS with a controller still isn’t my strength, so I moved on quickly. “The Witcher 3”, on the other hand, felt right at home on the Xbox: it’s an action RPG, and the controller experience is comfortable.</p><p>It may not reinvent the genre, but “The Witcher 3” is still one of the best fantasy games I’ve played. It let me cross vast grasslands and snowy fields, fight monsters with magic and swords, and wander the cobbled streets of medieval cities.</p><p>After sinking so many hours into it, I get why people call it a “AAA” game: it’s huge, polished, and clearly built with a lot of time and resources. It asked a lot of me, but it also gave a lot back.</p><p>In today’s busy and competitive world, it’s easy to feel guilty about spending time on games. Still, I’m grateful for my Xbox and the games I’ve played on it. They make my life richer, and they’re an important part of how I relax and recharge.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/uploads/persister-me-and-my-game-life-Witcher-2020-05-19_5ec3eb24973b8_vedmak-3-dikaia-okhota-geralt-tsiri-triss-iennifer-ciri
      
    
    </summary>
    
      <category term="Uncategorized" scheme="https://xhinliang.github.io/categories/Uncategorized/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL InnoDB Locks</title>
    <link href="https://xhinliang.github.io/2021/09/backend/innodb-locks/"/>
    <id>https://xhinliang.github.io/2021/09/backend/innodb-locks/</id>
    <published>2021-09-13T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/uploads/persister-innodb-locks-MySQL-1200px-MySQL.svg.png" alt="MySQL"></p><p>InnoDB is MySQL’s transactional storage engine. After more than a decade of development, it has become the default choice for most production workloads.</p><p>There are plenty of good deep-dives on InnoDB locks already; this post is a short refresher focused on how to inspect locks in a running database.</p><h2 id="HOW-TO-VIEW-LOCKS-IN-MYSQL"><a href="#HOW-TO-VIEW-LOCKS-IN-MYSQL" class="headerlink" title="HOW TO VIEW LOCKS IN MYSQL"></a>HOW TO VIEW LOCKS IN MYSQL</h2><p>To understand locking behavior, start by learning how to inspect the locks currently held in your database.</p><p>Run the following in the MySQL client to list all locks InnoDB is currently holding:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from performance_schema.data_locks\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               ENGINE: INNODB</span><br><span class="line">       ENGINE_LOCK_ID: 140616303283528:1157:140616462868960</span><br><span class="line">ENGINE_TRANSACTION_ID: 4492</span><br><span class="line">            THREAD_ID: 58</span><br><span class="line">             EVENT_ID: 75</span><br><span class="line">        OBJECT_SCHEMA: xhinliang_test <span class="comment">--- the database you used</span></span><br><span class="line">          OBJECT_NAME: locking_test <span class="comment">--- the table which of the lock occur</span></span><br><span class="line">       PARTITION_NAME: NULL</span><br><span class="line">    SUBPARTITION_NAME: NULL</span><br><span class="line">           INDEX_NAME: NULL</span><br><span class="line">OBJECT_INSTANCE_BEGIN: 140616462868960</span><br><span class="line">            LOCK_TYPE: TABLE <span class="comment">--- TABLE or RECORD, we will explain it below</span></span><br><span class="line">            LOCK_MODE: IX <span class="comment">--- several options, we will explain it below</span></span><br><span class="line">          LOCK_STATUS: GRANTED <span class="comment">--- whether the lock is granted or waiting?</span></span><br><span class="line">            LOCK_DATA: NULL <span class="comment">-- index of the lock using</span></span><br></pre></td></tr></table></figure><p>As shown above, you can inspect the current locks in the database. There are several key fields worth focusing on:</p><ul><li><code>INDEX_NAME</code>: The index involved in the lock. For table locks, this can be <code>NULL</code>.</li><li><code>LOCK_TYPE</code>: Whether the lock is a <code>TABLE</code> lock or a <code>RECORD</code> lock.</li><li><code>LOCK_MODE</code>: The specific lock mode (e.g., intention locks, record locks, gap locks).</li><li><code>LOCK_STATUS</code>: Whether the lock is <code>GRANTED</code> or <code>WAITING</code>.</li><li><code>LOCK_DATA</code>: For record locks, what record (or boundary) the lock refers to.</li></ul><p>From these fields you can usually tell what is locked, what kind of lock it is, and why a transaction is blocked.</p><h3 id="LOCK-TYPE"><a href="#LOCK-TYPE" class="headerlink" title="LOCK_TYPE"></a>LOCK_TYPE</h3><p>LOCK_TYPE is the easiest field to interpret. It is either <code>TABLE</code> or <code>RECORD</code> and tells you the scope of the lock.</p><h3 id="LOCK-MODE"><a href="#LOCK-MODE" class="headerlink" title="LOCK_MODE"></a>LOCK_MODE</h3><p>LOCK_MODE is the trickiest field in this post; it’s also the one people most often confuse with LOCK_TYPE.</p><p>LOCK_MODE has several common values:</p><ul><li>IX -&gt; Intention Exclusive Lock</li><li>IS -&gt; Intention Share Lock</li><li>X,REC_NOT_GAP -&gt; Exclusive Record Lock</li><li>X,GAP -&gt; Exclusive Gap Lock</li><li>X -&gt; Exclusive Next-Key Lock</li><li>S,REC_NOT_GAP -&gt; Share Record Lock</li><li>S,GAP -&gt; Share Gap Lock</li><li>S -&gt; Share Next-Key Lock</li></ul><h3 id="LOCK-STATUS"><a href="#LOCK-STATUS" class="headerlink" title="LOCK_STATUS"></a>LOCK_STATUS</h3><p>LOCK_STATUS shows whether the lock is currently held or still being waited on:</p><ul><li><code>GRANTED</code>: the session has acquired the lock.</li><li><code>WAITING</code>: the session is blocked, waiting to acquire it.</li></ul><h3 id="LOCK-DATA"><a href="#LOCK-DATA" class="headerlink" title="LOCK_DATA"></a>LOCK_DATA</h3><p>LOCK_DATA indicates what data this lock refers to.</p><p>For example, suppose we have a table named <code>child</code> with the following rows:</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; desc child;</span><br><span class="line"><span class="code">+-------+</span>------<span class="code">+------+</span>-----<span class="code">+---------+</span>-------+</span><br><span class="line">| Field | Type | Null | Key | Default | Extra |</span><br><span class="line"><span class="code">+-------+</span>------<span class="code">+------+</span>-----<span class="code">+---------+</span>-------+</span><br><span class="line">| id    | int  | NO   | PRI | NULL    |       |</span><br><span class="line"><span class="code">+-------+</span>------<span class="code">+------+</span>-----<span class="code">+---------+</span>-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from child</span><br><span class="line"><span class="code">    -&gt; ;</span></span><br><span class="line"><span class="code">+-----+</span></span><br><span class="line">| id  |</span><br><span class="line"><span class="code">+-----+</span></span><br><span class="line">|  89 |</span><br><span class="line">|  90 |</span><br><span class="line">| 102 |</span><br><span class="line">| 151 |</span><br><span class="line"><span class="code">+-----+</span></span><br><span class="line">4 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>Now, in one session, try to lock a row that doesn’t exist:<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="keyword">begin</span>;</span><br><span class="line">Query OK, <span class="number">0</span> rows affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">select</span> * <span class="keyword">from</span> child <span class="keyword">where</span> id = <span class="number">100</span> <span class="keyword">for</span> update;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></p><p>If we inspect <code>performance_schema.data_locks</code> now, we can see that the gap between <code>(90, 102)</code> is locked as an exclusive gap lock.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">mysql&gt;</span> <span class="string">select</span> <span class="string">*</span> <span class="string">from</span> <span class="string">performance_schema.data_locks\G</span></span><br><span class="line"><span class="string">***************************</span> <span class="number">1</span><span class="string">.</span> <span class="string">row</span> <span class="string">***************************</span></span><br><span class="line">               <span class="attr">ENGINE:</span> <span class="string">INNODB</span></span><br><span class="line">       <span class="attr">ENGINE_LOCK_ID:</span> <span class="number">140459538615624</span><span class="string">:1158:140458948850848</span></span><br><span class="line"><span class="attr">ENGINE_TRANSACTION_ID:</span> <span class="number">5640</span></span><br><span class="line">            <span class="attr">THREAD_ID:</span> <span class="number">49</span></span><br><span class="line">             <span class="attr">EVENT_ID:</span> <span class="number">25</span></span><br><span class="line">        <span class="attr">OBJECT_SCHEMA:</span> <span class="string">xhinliang_test</span></span><br><span class="line">          <span class="attr">OBJECT_NAME:</span> <span class="string">child</span></span><br><span class="line">       <span class="attr">PARTITION_NAME:</span> <span class="literal">NULL</span></span><br><span class="line">    <span class="attr">SUBPARTITION_NAME:</span> <span class="literal">NULL</span></span><br><span class="line">           <span class="attr">INDEX_NAME:</span> <span class="literal">NULL</span></span><br><span class="line"><span class="attr">OBJECT_INSTANCE_BEGIN:</span> <span class="number">140458948850848</span></span><br><span class="line">            <span class="attr">LOCK_TYPE:</span> <span class="string">TABLE</span></span><br><span class="line">            <span class="attr">LOCK_MODE:</span> <span class="string">IX</span></span><br><span class="line">          <span class="attr">LOCK_STATUS:</span> <span class="string">GRANTED</span></span><br><span class="line">            <span class="attr">LOCK_DATA:</span> <span class="literal">NULL</span></span><br><span class="line"><span class="string">***************************</span> <span class="number">2</span><span class="string">.</span> <span class="string">row</span> <span class="string">***************************</span></span><br><span class="line">               <span class="attr">ENGINE:</span> <span class="string">INNODB</span></span><br><span class="line">       <span class="attr">ENGINE_LOCK_ID:</span> <span class="number">140459538615624</span><span class="string">:3:4:3:140459473245216</span></span><br><span class="line"><span class="attr">ENGINE_TRANSACTION_ID:</span> <span class="number">5640</span></span><br><span class="line">            <span class="attr">THREAD_ID:</span> <span class="number">49</span></span><br><span class="line">             <span class="attr">EVENT_ID:</span> <span class="number">25</span></span><br><span class="line">        <span class="attr">OBJECT_SCHEMA:</span> <span class="string">xhinliang_test</span></span><br><span class="line">          <span class="attr">OBJECT_NAME:</span> <span class="string">child</span></span><br><span class="line">       <span class="attr">PARTITION_NAME:</span> <span class="literal">NULL</span></span><br><span class="line">    <span class="attr">SUBPARTITION_NAME:</span> <span class="literal">NULL</span></span><br><span class="line">           <span class="attr">INDEX_NAME:</span> <span class="string">PRIMARY</span></span><br><span class="line"><span class="attr">OBJECT_INSTANCE_BEGIN:</span> <span class="number">140459473245216</span></span><br><span class="line">            <span class="attr">LOCK_TYPE:</span> <span class="string">RECORD</span></span><br><span class="line">            <span class="attr">LOCK_MODE:</span> <span class="string">X,GAP</span></span><br><span class="line">          <span class="attr">LOCK_STATUS:</span> <span class="string">GRANTED</span></span><br><span class="line">            <span class="attr">LOCK_DATA:</span> <span class="number">102</span></span><br><span class="line"><span class="number">2</span> <span class="string">rows</span> <span class="string">in</span> <span class="string">set</span> <span class="string">(0.01</span> <span class="string">sec)</span></span><br></pre></td></tr></table></figure><p>Once you get used to reading these fields, debugging lock contention becomes much more mechanical: take a snapshot of <code>performance_schema.data_locks</code>, identify <code>WAITING</code> locks, and then find the corresponding <code>GRANTED</code> locks on the same object/index.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/uploads/persister-innodb-locks-MySQL-1200px-MySQL.svg.png&quot; alt=&quot;MySQL&quot;&gt;&lt;/p&gt;
&lt;p&gt;InnoDB is MySQL’s transactional storage engine.
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="MySQL" scheme="https://xhinliang.github.io/tags/MySQL/"/>
    
      <category term="InnoDB" scheme="https://xhinliang.github.io/tags/InnoDB/"/>
    
      <category term="Lock" scheme="https://xhinliang.github.io/tags/Lock/"/>
    
  </entry>
  
  <entry>
    <title>给 Markdown 程序员的写作能力提升指南</title>
    <link href="https://xhinliang.github.io/2020/04/uncategorized/writing-guide-for-markdown-programer/"/>
    <id>https://xhinliang.github.io/2020/04/uncategorized/writing-guide-for-markdown-programer/</id>
    <published>2020-04-09T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.609Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近完成了一次反复打磨的技术分享。在准备和进行分享的过程中，我遇到了不少问题，而这些问题的根源在于自己日渐下降的写作水平和表达能力。</p><p>在本文中，我会先从自己的角度分析写作能力对程序员的作用；再结合自己的参考和实践，总结一套相对标准的写作套路；最后分享一些常用的写作辅助软件，并分析这些软件的特点。</p><h2 id="写作能力真的有用吗"><a href="#写作能力真的有用吗" class="headerlink" title="写作能力真的有用吗"></a>写作能力真的有用吗</h2><p>不少工科学生认为写作能力对个人水平的提升没什么实际作用，甚至会调侃：“代码写得好的，不如 PPT 讲得好的”。在这几年的工作中，我越来越不赞同这种看法。</p><p>首先，提升写作能力有助于增强结构化思考能力。<br>写作主要是写给人看，写代码则同时写给人和机器看。一份好的代码首先要可读；如果可读性很差，甚至已经无法维护，那么这份代码的价值就会大打折扣。<br>一篇好文章应该层级清晰、文笔顺畅；同样，一份好代码也应该命名统一、流程明确。如果文章层级不清、章节衔接生硬，读者阅读时就很容易一头雾水。<br>如果程序员对某段逻辑已经想得很清楚，写出来的代码往往也会更顺畅。类比到写作：当你对要表达的内容了然于胸，写出来的文章自然不会晦涩难懂。</p><p>其次，提升写作能力也有助于提升言语表达能力。<br>我经常会提笔忘字，也常常在和别人讨论问题时语无伦次。这时我可能会说：等我在聊天工具上把语言组织好，再用文字发给你。<br>当我在聊天工具上组织语言时，我会发现：经过几轮修改和校正，我终于能把自己要表达的意思完整、准确地表达出来。<br>当然，言语表达不清楚时，用文字也未必就能表达清楚。但由于文字表达对即时性的要求通常低于言语表达，我们可以先通过锻炼文字表达能力，间接提升自己的言语表达能力。</p><p>最后，提升写作能力有助于程序员把学到的知识真正落地。<br>不少优秀的程序员都有写博客的习惯。写博客能加深对知识的理解，并让知识更好地沉淀。俗话说“好记性不如烂笔头”，而博客往往就是程序员最好的“烂笔头”。<br>但也有不少程序员没有写博客的习惯，或者坚持不下去。我认为最主要的原因是：不想让别人读到这么“烂”的文章，从而影响自己在别人心中的形象。<br>如果因为害怕曝光而不写作，写作能力就会在恶性循环中不断下降。反过来，如果能逐步提升写作能力，写出来的博客质量自然会慢慢提高，写博客的习惯也更容易建立起来。</p><h2 id="程序员写作的套路"><a href="#程序员写作的套路" class="headerlink" title="程序员写作的套路"></a>程序员写作的套路</h2><p>如今 Markdown 在程序员群体里已经非常普及，很多人的文档和笔记都用 Markdown 来写。我自然也是 Markdown 的重度使用者。多年写作下来，我也总结了一些经验和教训。</p><p>第一，创作时别太在意排版。<br>当灵感涌现时，最好先把文字尽量输出；排版这些琐碎的小事，可以等草稿完成后再处理。创作阶段如果把太多时间花在排版上，很容易打断思路。</p><p>第二，别滥用太多 Markdown 语法。<br>我以前特别喜欢各种语法，例如 <code>code</code> <em>斜体</em>  <strong>强调</strong>，还有各种大大小小的标题。后来回过头阅读自己写过的 Markdown 文章时，我发现：标题层级越复杂，行文往往越不流畅。标题的插入在某种程度上会让文章的上下文衔接变得不自然。</p><p>第三，写作要多练。<br>对于很多高中毕业已经五六年的人来说，重拾写作这件事本来难度就不低。如果让我把写出来的所有文字、所有笔记都分享出来，估计会有人笑掉大牙。但因为害怕曝光就不写作是不对的：越是惧怕写作，写作能力下降得越快。我现在会每周强迫自己写至少 2000 字，这些文字大多只保存在自己的网盘里；如果有特别值得分享的，或者写得特别好的，我才会分享出来。</p><h2 id="工具分享"><a href="#工具分享" class="headerlink" title="工具分享"></a>工具分享</h2><p>Markdown 写作工具层出不穷，选择也很多。每个人可能都有自己偏好的工具。在这里，我结合自己的使用经验以及他人的推荐，分享一些常用的软件和工具。</p><h3 id="VS-Code-Markdown-Preview-Enhanced"><a href="#VS-Code-Markdown-Preview-Enhanced" class="headerlink" title="VS Code + Markdown Preview Enhanced"></a>VS Code + Markdown Preview Enhanced</h3><p>VS Code 大概不用介绍了，它是一个文本编辑器，同时也是一个轻量级的 IDE。平常我会用它进行 Markdown 文本创作。写作时我不会使用太多插件，一般就把它当作纯文本编辑器使用。在检索资料时，我也经常使用 VS Code 的文本搜索功能，很方便。</p><p>Markdown Preview Enhanced 是一个很常用的 Markdown 插件，我会用它在 VS Code 中渲染 Markdown，以便预览效果。当然，它也支持很多原生 Markdown 不支持的功能，例如 puml 渲染、甘特图渲染等。不过我最近没有在使用这些功能了，因为觉得使用文本画图的效率不高。</p><p><img src="/uploads/persister-writing-guide-for-markdown-programer-MPE-28227953-eb6eefa4-68a1-11e7-8769-96ea83facf3b.png" alt="MPE"></p><h3 id="坚果云"><a href="#坚果云" class="headerlink" title="坚果云"></a>坚果云</h3><p>坚果云为我提供了个人的云盘服务。Markdown 文件很小，免费版坚果云的流量限额完全够用。我自己有多台设备，会在各个设备上安装坚果云并开启同步，基本能达到无缝切换的效果。如果我需要在手机上阅读和查找，坚果云手机客户端也完全能满足我的需求。</p><p><img src="/uploads/persister-writing-guide-for-markdown-programer-nutcloud-normal.jpg" alt="nutcloud"></p><h3 id="iPic-persister"><a href="#iPic-persister" class="headerlink" title="iPic + persister"></a>iPic + persister</h3><p>iPic 是一个图床管理器，支持将截图上传到图床，并直接生成 Markdown 的图片链接。免费版的 iPic 只支持微博图床，而微博图床经常出现 403，所以我自己写了一个程序，用于将 Markdown 文件里的图片下载到本地，然后替换 Markdown 的图片链接，具体可以参考 <a href="https://github.com/XhinLiang/persister" target="_blank" rel="noopener">persister</a>。</p><h3 id="Draw-io"><a href="#Draw-io" class="headerlink" title="Draw.io"></a>Draw.io</h3><p>Draw.io 是一个开源的画图工具，基本功能跟 processon 类似，但它免费且支持多种存储。我习惯使用它的桌面版客户端，并使用坚果云提供的本地存储。画图结束后，我习惯输出 svg 格式的图片，然后在 Markdown 中引用。</p><p><img src="/uploads/persister-writing-guide-for-markdown-programer-drawio-v2-45382597c219dca9114d65900d5c868a_1200x500.jpg" alt="drawio"></p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>本篇文章算是我这个月给自己设定的目标之一。<br>写完这篇文章，希望能让自己养成坚持写作的好习惯，顺带提升自己的表达能力和结构化思考能力。<br>前路漫漫，未来可期。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://thewriter.dev/perface/" target="_blank" rel="noopener">给程序员的写作课</a></li><li><a href="https://studyidea.cn/write_tools" target="_blank" rel="noopener">坚持写作快两年了，有些私藏工具跟你们分享</a></li><li><a href="http://zhangtielei.com/posts/blog-kungfu-flow-well-being.html" target="_blank" rel="noopener">心流：写作、编程和修炼武功的共同法门</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近完成了一次反复打磨的技术分享。在准备和进行分享的过程中，我遇到了不少问题，而这些问题的根源在于自己日渐下降的写作水平和表达能力。&lt;/p&gt;
      
    
    </summary>
    
      <category term="Uncategorized" scheme="https://xhinliang.github.io/categories/Uncategorized/"/>
    
    
  </entry>
  
  <entry>
    <title>自我时间管理</title>
    <link href="https://xhinliang.github.io/2020/03/uncategorized/self-time-managment/"/>
    <id>https://xhinliang.github.io/2020/03/uncategorized/self-time-managment/</id>
    <published>2020-03-08T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.609Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>现在开始尝试一套更规范的自我时间管理机制。<br>这套机制主要由三个工具支撑：番茄土豆、坚果云、VSCode。<br>其中，番茄土豆用于管理碎片化时间和待办事项；坚果云用于存储更宏观的计划与日志；VSCode 用于管理坚果云中的文件。</p><p><img src="/uploads/manage-self-time/pomodoro.png" alt></p><h1 id="制定-OKR"><a href="#制定-OKR" class="headerlink" title="制定 OKR"></a>制定 OKR</h1><p>每两个月制定一次个人 OKR，并落实到 okr.md 文件中。<br>为便于聚焦，Objective 通常控制在三个以内。<br>每个 Objective 需要配套 Key Result，且 Key Result 应当可量化。<br>制定好 OKR 后，在坚果云中为这两个月建立文件夹，例如 2020-march-aprill，然后将 okr.md 文件放到该文件夹中。<br>接下来两个月的个人时间管理会围绕这份 OKR 进行。</p><h1 id="灵感"><a href="#灵感" class="headerlink" title="灵感"></a>灵感</h1><p>OKR 制定完成后，随着时间的推移，往往会产生一些关于如何完成 OKR 的想法。<br>这些想法分为大想法和小想法。大想法一般需要更多时间完成，而小想法通常只需要一到两个番茄时间即可。</p><p>对于大想法，记录到 okr.md 中，等到下个 sprint 再安排时间。<br>对于小想法，如果本周有充足的时间可以完成，则加到番茄土豆的待办事项中；如果没有，也放到 okr.md 中。</p><p>如果有一些无关 OKR 的想法，也可以先记录到 okr.md 中，等到下个 OKR 再进行统一安排。</p><h1 id="制定-Sprint"><a href="#制定-Sprint" class="headerlink" title="制定 Sprint"></a>制定 Sprint</h1><p>每周需要创建一个 sprint 文件，例如 sprint-march-1.md。<br>此文件用于记录本周个人时间管理的结果，应尽量保证可读性。</p><p>sprint 文件必须包含的内容有：<br>本周目标。本周目标通常是相对比较大的工作，一般由 OKR 拆解而来。例如 <code>系统学习 ZooKeeper 相关的组件及实现原理</code><br>每日回顾。每天应该回顾一下工作和学习中值得记录的事情，学习了某篇很有意义的文章，可以将文章概要写下来；解决了某个 bug，可以将 bug 解决的过程记录下来。俗话说“好记性不如烂笔头”，希望这样一个机制能够让自己将知识更好地沉淀下来。<br>本周总结。将本周完成的目标和感触写下来，以达到复习的目的。</p><h1 id="初始化待办事项"><a href="#初始化待办事项" class="headerlink" title="初始化待办事项"></a>初始化待办事项</h1><p>每周日，将本周需要完成的目标写到 sprint 文件中，称为本周目标。</p><p>本周目标的制定与本周可用时间有关。<br>每个目标最好能够细化出完成目标预计需要花费的时间，并与本周可用时间一起配置。如果本周的可用时间较多，那么本周目标可以设定得更多一些。</p><h1 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h1><p>在每日的执行工作中，强依赖番茄土豆这个工具，主要包括两方面：</p><ol><li>每日使用番茄钟来计算本日使用的时间。我手动将番茄钟的时间调整为 30 分钟，这是为了便于计算每日花费时间。</li><li>将每天做的事情使用番茄土豆的待办事项进行管理。</li></ol><h2 id="温习"><a href="#温习" class="headerlink" title="温习"></a>温习</h2><p>每日工作完成后，需要对本日的学习和工作做一份总结，总结的来源一般是 git log 和番茄土豆的待办事项完成记录。<br>对于本日完成的有意义的事情，最好能够做一些概要性描述。</p><p>每周工作完成后，回顾一遍 sprint.md，并根据每日目标的完成情况编写本周总结。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;现在开始尝试一套更规范的自我时间管理机制。&lt;br&gt;这套机制主要由三个工具支撑：番茄土豆、坚果
      
    
    </summary>
    
      <category term="Uncategorized" scheme="https://xhinliang.github.io/categories/Uncategorized/"/>
    
    
  </entry>
  
  <entry>
    <title>在 macOS 中使用命令行打开 VSCode</title>
    <link href="https://xhinliang.github.io/2019/02/computer/code-tools/open-vscode-via-terminal-in-macos/"/>
    <id>https://xhinliang.github.io/2019/02/computer/code-tools/open-vscode-via-terminal-in-macos/</id>
    <published>2019-02-15T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/uploads/new--006tKfTcgy1g07apvz6gfj31660u07h1.jpg" alt><br>VSCode 相信已经是大家的必备编辑器了，轻量，免费。<br>在 Linux 环境中， VSCode 可以通过图标启动，也可以通过命令行启动。<br>例如，我想在 VSCode 中打开这个文件夹，可以这样：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ code someCodeProject</span><br></pre></td></tr></table></figure></p><p>但是在 macOS 下默认是不能操作的，因为没有 <code>code</code> 这个程序。<br>我们可以伪造一个：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat code</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">TARGET_DIR=<span class="string">"."</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$1</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">TARGET_DIR=<span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">nohup /Applications/Visual\ Studio\ Code.app/Contents/MacOS/Electron <span class="variable">$TARGET_DIR</span> &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>然后，把 <code>code</code> 所在的目录加入到 <code>PATH</code> 环境变量中，即可。</p><p>给个示例：<br><img src="/uploads/new--006tKfTcgy1g07arzgl7ij30u604djt7.jpg" alt><br><img src="/uploads/new--006tKfTcgy1g07asgll76j30u20mggrj.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/uploads/new--006tKfTcgy1g07apvz6gfj31660u07h1.jpg&quot; alt&gt;&lt;br&gt;VSCode 相信已经是大家的必备编辑器了，轻量，免费。&lt;br&gt;在 Linux 环境中， VSCode 可以通过图标启动，也可以通过命
      
    
    </summary>
    
      <category term="Computer" scheme="https://xhinliang.github.io/categories/Computer/"/>
    
    
      <category term="macOS" scheme="https://xhinliang.github.io/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>安装 Ubuntu Workstation 之后要做的事儿</title>
    <link href="https://xhinliang.github.io/2019/01/computer/code-tools/init-ubuntu/"/>
    <id>https://xhinliang.github.io/2019/01/computer/code-tools/init-ubuntu/</id>
    <published>2019-01-11T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/uploads/new--2000px-Former_Ubuntu_logo.png" alt="logo"></p><p>最近自己心爱的 Dell Latitude E6430s 频繁死机，以为是硬盘问题，格式化硬盘重装系统问题依旧。<br>硬件检查才发现是一根内存有了问题，取出问题的内存后一切正常。</p><p>但删掉的系统是需要重装的，经过一些实践后，我选择了 Ubuntu 16.04 LTS 作为我新的操作系统。</p><p>都 2019 年了，为什么还要选择老旧的 Ubuntu 16.04 呢？<br>其实我之前试过了几个 Linux 发行版，都有一些问题：</p><ul><li>Ubuntu 14.04 LTS，这也是我之前一直使用的发行版，稳定性尚可，但很快就结束维护了。</li><li>Manjaro Xfce/i3wm，可能我的电脑硬件太老旧了，pacman -SYy 之后基本就滚挂了，试了几遍都这样，无奈放弃。</li><li>Fedora，官方貌似只维护 Gnome 版本，不太喜欢，放弃。</li><li>Ubuntu 18.04 LTS，桌面也换成了 Gnome，非常别扭别扭，放弃。</li></ul><p>最后试了下 Ubuntu 16.04 LTS，安装非常顺利，基本配置以后也非常顺手，所以就硬定他了。</p><h2 id="双系统安装"><a href="#双系统安装" class="headerlink" title="双系统安装"></a>双系统安装</h2><p>本次安装还是 Windows 10 + Ubuntu 16.04 双系统。<br>我的主硬盘是一块 480G 的固态硬盘，所以我还是依照原来的方案，把两个系统都装在这个硬盘。</p><h3 id="Step-0"><a href="#Step-0" class="headerlink" title="Step 0"></a>Step 0</h3><p>Windows 10 安装，先格式化整个硬盘，并将分区表修改为 GPT 格式<br>使用配置好的 UEFI U盘启动后，在第一个界面 Shift + F10 进入命令行环境：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">diskpart</span><br><span class="line">clean</span><br><span class="line">convert gpt</span><br></pre></td></tr></table></figure></p><p>然后分出 200G 给 Windows 10，Windows 安装程序会自动再分出三个小分区，我们只需要关注其中 100M 的那个 EFI 分区，此分区的文件系统是 FAT32，应该非常好辨认。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>Windows 10 顺利安装完成，用 UltraISO（还可以使用别的工具，随意了） 把 Ubuntu 的 ISO 文件写入U盘。<br>然后重启进入 Ubuntu Live 系统。</p><p>选择安装。<br>手动分区。<br>分出一个 EXT4 文件系统的分区，大小是 12G，分区类型是 <code>SWAP</code>（事实上就是虚拟内存）。<br>剩下的 250G 再分出一个 EXT4 的分区，没有分区类型，此分区直接挂载到 <code>/</code>（根目录）。<br>然后最下方的启动磁盘直接选择整个固态硬盘（我这边好像是 SDA ）</p><p>确认安装，应该没问题了。</p><h2 id="初始化系统"><a href="#初始化系统" class="headerlink" title="初始化系统"></a>初始化系统</h2><h3 id="修改-home-目录下的文件夹目录为英文"><a href="#修改-home-目录下的文件夹目录为英文" class="headerlink" title="修改 home 目录下的文件夹目录为英文"></a>修改 home 目录下的文件夹目录为英文</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LANG=en_US</span><br><span class="line">xdg-user-dirs-gtk-update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时会弹出弹框，确认即可</span></span><br><span class="line"><span class="comment"># 再把语言设回来</span></span><br><span class="line"><span class="built_in">export</span> LANG=zh_CN</span><br></pre></td></tr></table></figure><h3 id="安装必备的软件"><a href="#安装必备的软件" class="headerlink" title="安装必备的软件"></a>安装必备的软件</h3><p>以下软件手动寻找 deb 安装文件，使用 dpkg 安装即可：</p><ul><li>VS Code</li><li>Chrome</li><li>5hadow5ocks-QT5</li><li>Nutcloud</li><li>网易云音乐</li></ul><h3 id="解决输入法问题"><a href="#解决输入法问题" class="headerlink" title="解决输入法问题"></a>解决输入法问题</h3><p>官网下载 deb 格式的安装包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i sogoupinyin_2.2.0.0108_amd64.deb</span><br><span class="line">sudo apt-get -f install <span class="comment"># 解决依赖冲突问题</span></span><br></pre></td></tr></table></figure></p><h3 id="删掉亚马逊"><a href="#删掉亚马逊" class="headerlink" title="删掉亚马逊"></a>删掉亚马逊</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove unity-webapps-common</span><br></pre></td></tr></table></figure><h2 id="美化命令行"><a href="#美化命令行" class="headerlink" title="美化命令行"></a>美化命令行</h2><p>以下步骤按对应的说明安装即可<br>安装 oh-my-zsh<br>安装 ZSH 主题 <a href="https://github.com/bhilburn/powerlevel9k" target="_blank" rel="noopener">https://github.com/bhilburn/powerlevel9k</a><br>安装 Powerline 字体 <a href="https://github.com/powerline/fonts" target="_blank" rel="noopener">https://github.com/powerline/fonts</a> 直接 clone 代码库，install 完事儿</p><h3 id="安装几个-ZSH-插件"><a href="#安装几个-ZSH-插件" class="headerlink" title="安装几个 ZSH 插件"></a>安装几个 ZSH 插件</h3><p>自行安装</p><ul><li>zsh-autosuggestions</li><li>zsh-syntax-highlighting</li><li><a href="https://github.com/romkatv/powerlevel10k" target="_blank" rel="noopener">https://github.com/romkatv/powerlevel10k</a></li></ul><h2 id="安装必备开发工具"><a href="#安装必备开发工具" class="headerlink" title="安装必备开发工具"></a>安装必备开发工具</h2><p>安装 <code>n</code> &amp;&amp; <code>node</code><br>k-vim <a href="https://github.com/wklken/k-vim" target="_blank" rel="noopener">https://github.com/wklken/k-vim</a></p><p>安装 <code>golang</code><br><a href="https://xhinliang.win/2018/10/30/2018/nsq/nsq-part1-set-up-env/" target="_blank" rel="noopener">https://xhinliang.win/2018/10/30/2018/nsq/nsq-part1-set-up-env/</a></p><p>安装 Oracle JDK 11<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk11-downloads-5066655.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk11-downloads-5066655.html</a></p><h3 id="设置-Golang-的安装目录"><a href="#设置-Golang-的安装目录" class="headerlink" title="设置 Golang 的安装目录"></a>设置 Golang 的安装目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> GOROOT=<span class="string">"<span class="variable">$HOME</span>/cli-utils/golang/go"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$GOROOT</span>/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 设置 GOPATH</span></span><br><span class="line"><span class="built_in">export</span> GOPATH=<span class="string">"<span class="variable">$HOME</span>/go"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$GOPATH</span>/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Java</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/lib/jvm/jdk-11.0.1"</span></span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="string">"<span class="variable">$JAVA_HOME</span>/jre"</span></span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="string">".:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JRE_HOME</span>/lib:<span class="variable">$CLASSPATH</span>"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></table></figure><h3 id="配置小飞机"><a href="#配置小飞机" class="headerlink" title="配置小飞机"></a>配置小飞机</h3><p>地址忽略</p><p>小飞机的本地代理可以直接使用 HTTP 形式，所以可以直接用 HTTP 代理暴露出来，然后使用<br><code>google-chrome --proxy-server=&quot;http://localhost:1080&quot;</code><br>这个命令给 chrome 配置 proxy 并启动，然后登录原来的账号，就可以安装上各种插件，包括 SwitchyOmega</p><p>安装 proxychains 并配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install proxychains</span><br><span class="line">sudo vim /etc/proxychains.conf</span><br></pre></td></tr></table></figure></p><h3 id="autojump"><a href="#autojump" class="headerlink" title="autojump"></a>autojump</h3><p><a href="https://github.com/wting/autojump" target="_blank" rel="noopener">https://github.com/wting/autojump</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/wting/autojump.git</span><br><span class="line"><span class="built_in">cd</span> autojump</span><br><span class="line">./install.py</span><br></pre></td></tr></table></figure><p>~/.zshrc 文件加上 plugin，并加上 autojump 提示的那两行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">  git</span><br><span class="line">  zsh-autosuggestions</span><br><span class="line">  zsh-syntax-highlighting</span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  autojump</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># autojump</span></span><br><span class="line">[[ -s /home/xhinliang/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; <span class="built_in">source</span> /home/xhinliang/.autojump/etc/profile.d/autojump.sh</span><br><span class="line"><span class="built_in">autoload</span> -U compinit &amp;&amp; compinit -u</span><br></pre></td></tr></table></figure></p><h3 id="配置舒适的日志记录环境"><a href="#配置舒适的日志记录环境" class="headerlink" title="配置舒适的日志记录环境"></a>配置舒适的日志记录环境</h3><p>我习惯使用 VSCode + 坚果云 + Markdown 记日志。<br>但默认的 Markdown 不支持 PlantUML 的绘制，我们加上插件让他更完美些。</p><ol><li>在 VSCode 中安装 <code>markdown preview enhanced</code> 插件</li><li><code>sudo apt-get install graphviz</code></li><li><p>下载 plantuml.jar 放到 <code>$HOME/cli-utils/jars</code> 文件夹并把这个文件夹加到 PATH 环境变量中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PLANTUML_JAR=<span class="string">"<span class="variable">$HOME</span>/cli-utils/jars"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PLANTUML_JAR</span>:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></table></figure></li><li><p>安装图床上传工具 <a href="https://github.com/klesh/fu" target="_blank" rel="noopener">fu</a></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">tar</span> 包编译失败了</span><br><span class="line">源代码编译成功，但是运行失败。。</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/uploads/new--2000px-Former_Ubuntu_logo.png&quot; alt=&quot;logo&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近自己心爱的 Dell Latitude E6430s 频繁死机，以为是硬盘问题，格式化硬盘重装系统问题依旧。&lt;br&gt;硬件检
      
    
    </summary>
    
      <category term="Computer" scheme="https://xhinliang.github.io/categories/Computer/"/>
    
    
      <category term="Ubuntu" scheme="https://xhinliang.github.io/tags/Ubuntu/"/>
    
      <category term="Linux" scheme="https://xhinliang.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>那些年，我们一起改过的配置文件 -- 常见配置文件类型简介</title>
    <link href="https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-conf-intro/"/>
    <id>https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-conf-intro/</id>
    <published>2018-11-25T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="/uploads/persister-frequent-conf-intro-overview-maxresdefault.jpg" alt="overview"></p><p>在程序员日常开发中，配置文件是一个非常常见的需求。</p><p>配置文件可以定制程序的逻辑，一段代码生成的程序可以灵活地适应多个需求。<br>几乎所有的程序都需要或者隐形需要配置文件，因为它们需要根据配置文件来决定在代码中使用怎样的逻辑来运行。</p><p>对于大部分的程序来说，他们需要的配置文件常常是一个 K-V 类型的结构，可以理解为一个 Key 为字符串， Value 也为字符串的一个 Map。字符串可以被转化成大部分通用的数据结构，只需要程序自己做好解析就可以了。</p><p>但随着程序的复杂度的增加，配置文件用纯字符串 K-V 来表示的局限性就越来越明显。<br>例如，我们如果需要一个 List，我们可以自定义分隔符。但如果一个 List 里我需要嵌套，那么用纯字符串 K-V 来表示就非常吃力了。</p><p>计算机编程的本质是抽象。因为配置文件是可以被抽象出来的，所以，各个程序员根据自己对配置文件的理解，定义了不同的对配置文件的抽象，这就是我们今天看到的五花八门的配置文件类型。</p><p>今天我就来侃侃常见的几种配置文件类型。</p><h1 id="Body"><a href="#Body" class="headerlink" title="Body"></a>Body</h1><h2 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h2><p>毫无疑问，JSON 是目前使用最广泛的一个数据交换格式。大量的 Web 前后端交互使用 JSON 作为数据载体，同时也触发了 JSON 在数据传输之外的用途 – 配置文件。</p><p>我们用一个 empty 的 npm project 的配置文件作为例子：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"dev_2018_11"</span>,</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"1.0.0"</span>,</span><br><span class="line">  <span class="attr">"description"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="attr">"main"</span>: <span class="string">"index.js"</span>,</span><br><span class="line">  <span class="attr">"scripts"</span>: &#123;</span><br><span class="line">    <span class="attr">"test"</span>: <span class="string">"echo \"Error: no test specified\" &amp;&amp; exit 1"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"author"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="attr">"license"</span>: <span class="string">"ISC"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>JSON 是 JavaScript 的亲儿子，所以在 JavaScript 相关的世界中被广泛用作配置文件。<br>例如：</p><ul><li>npm 依赖描述文件是 JSON</li><li>PHP composer 的依赖描述文件是 JSON</li><li>大量的 npm 第三方包的配置文件是 JSON</li></ul><p>优点：</p><ul><li>抽象程度高。JSON 能表示大部分的数据结构，对于嵌套的 List，Map 等需求，也能支持得非常好</li><li>通用性好。绝大部分的编程语言都内置了 JSON 解析器，所以 JSON 的通用性也非常好</li><li>合法性校验方便，有现成的 JSON-Schema 校验工具</li></ul><p>缺点：</p><ul><li>可读性较差。<ul><li>JSON 不支持注释，一些复杂配置的可读性非常差</li><li>灵活性欠佳</li><li>不支持嵌套配置文件（不能在一个配置文件中 import 另外一个配置文件）</li><li>JSON 严格的格式校验（Array 的最后一个元素后面不能加逗号），导致修改起来不够方便</li><li>Key 必须被双引号包裹，编写起来也不太方便</li></ul></li></ul><h2 id="JSON5"><a href="#JSON5" class="headerlink" title="JSON5"></a>JSON5</h2><p>JSON 的缺点非常致命，优点又非常明显，所以一些程序员希望在 JSON 的基础上新定义一个数据类型，对 JSON 扬长补短，这就是我们看到的 JSON5 结构。</p><p>举个官方的例子：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attribute">name</span>: <span class="string">'ManerFan'</span>,</span><br><span class="line">    // address</span><br><span class="line">    addr: <span class="string">'KunMing Road,\</span></span><br><span class="line"><span class="string">        ShannXi,\</span></span><br><span class="line"><span class="string">        China'</span>,</span><br><span class="line">    nickname: <span class="string">'\u5c0f\u5e08\u59b9'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>JSON5 完全兼容 JSON，而且对 JSON 做了一些扩展：</p><ul><li>支持注释</li><li>属性key可以不使用引号包含，而且可以使用单引号包含</li><li>可以在尾部有多余逗号</li><li>支持多行字符串</li><li>etc..</li></ul><p>JSON5 解决了 JSON 大部分的问题，但是依然不支持嵌套的配置文件。</p><h2 id="XML"><a href="#XML" class="headerlink" title="XML"></a>XML</h2><p>XML 在 <code>Java</code> 和 <code>Spring</code> 中使用非常广泛，他能描述大部分的数据结构，但是缺点是太罗嗦了，罗嗦到我现在都不想贴示例出来。</p><p>好吧，还是随便贴一个：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.xhinliang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>xcall<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>$&#123;project.artifactId&#125;<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>我只想说，XML 的「罗嗦」这个缺点，成功地掩盖了它的其它缺点，因为已经罗嗦到不能忍了…</p><h2 id="INI"><a href="#INI" class="headerlink" title="INI"></a>INI</h2><p>INI 配置文件在一些 Windows 程序中用得比较多，它事实上是一种非常朴素的字符串 K-V 的配置文件。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[section1]</span>  </span><br><span class="line"><span class="comment">; this is comment</span></span><br><span class="line"><span class="attr">key</span>=value</span><br><span class="line"><span class="attr">key2</span>=value2</span><br><span class="line"></span><br><span class="line"><span class="section">[section2]</span></span><br></pre></td></tr></table></figure><p>ini 配置文件现在用得不多了，优点也不多说了，几乎没有。<br>缺点也不多说，关键是注释长得很丑。</p><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><p>Properties 跟 INI 类似，事实上是一个简单的字符串 K-V。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># comment</span><br><span class="line">db=xxxxxx</span><br></pre></td></tr></table></figure><p>特点也跟 INI 差不多吧，抽象描述能力有限。</p><h2 id="PHP"><a href="#PHP" class="headerlink" title="PHP"></a>PHP</h2><p>没错，PHP 也是一种配置文件格式，在 80% 的网站 PHP 覆盖率面前颤抖吧！<br>PHP 的数组（array）即能表示普通的数组，也能表示Map，甚至能同时包含（这个特性能逼疯很多人…）</p><p>在 PHP 程序中， <code>config.php</code> 文件非常常见，事实上，大部分的 PHP 框架都使用 PHP 文件作为他们的配置文件。<br>举个 Yii 框架的配置文件作为例子：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"></span><br><span class="line">$config= [</span><br><span class="line">    <span class="string">'components'</span> =&gt; [</span><br><span class="line">        <span class="string">'db'</span> =&gt; [</span><br><span class="line">            <span class="string">'class'</span> =&gt; <span class="string">'yii\db\Connection'</span>,</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">'mailer'</span> =&gt; [</span><br><span class="line">            <span class="string">'class'</span> =&gt; <span class="string">'yii\swiftmailer\Mailer'</span>,</span><br><span class="line">            <span class="string">'viewPath'</span> =&gt; <span class="string">'@common/mail'</span>,</span><br><span class="line">            <span class="comment">// send all mails to a file by default. </span></span><br><span class="line">            <span class="string">'useFileTransport'</span> =&gt; <span class="keyword">true</span>,</span><br><span class="line">        ],</span><br><span class="line">    ],</span><br><span class="line">];</span><br><span class="line"><span class="keyword">if</span> (YII_ENV_DEV) &#123;</span><br><span class="line">    <span class="comment">// configuration adjustments for 'dev' environment</span></span><br><span class="line">    $config[<span class="string">'bootstrap'</span>][] = <span class="string">'debug'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>PHP 文件作为配置文件，有以下优点：</p><ul><li>灵活性非常好，能更灵活地定制程序的逻辑，因为你能在 PHP 文件里干任何事情，在一个配置文件中 require 另外一个配置文件，是一个非常常见，非常 easy 的事情</li><li>抽象性非常棒，几乎没有用 PHP 配置文件描述不出来的数据结构</li></ul><p>但缺点也很明显：</p><ul><li>跨语言通用性几乎为 0</li><li>太过灵活导致安全性欠佳（你见过 import 之后会删除本地文件的配置文件吗…）</li></ul><h2 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h2><p>YAML 不是一种标记语言。他是一种数据描述语言（DDL）。有一些程序支持使用 YAML 作为配置文件，例如 <code>Spring Boot</code> 和 <code>Hexo</code>，<code>Ruby on Rails</code>。</p><p>典型的 YAML 配置格式如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">house:</span></span><br><span class="line">  <span class="attr">family:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">Doe</span></span><br><span class="line">    <span class="attr">parents:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">John</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">Jane</span></span><br><span class="line">    <span class="attr">children:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">Paul</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">Mark</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">Simone</span></span><br><span class="line">  <span class="attr">address:</span></span><br><span class="line">    <span class="attr">number:</span> <span class="number">34</span></span><br><span class="line">    <span class="attr">street:</span> <span class="string">Main</span> <span class="string">Street</span></span><br><span class="line">    <span class="attr">city:</span> <span class="string">Nowheretown</span></span><br><span class="line">    <span class="attr">zipcode:</span> <span class="number">12345</span></span><br></pre></td></tr></table></figure></p><p>可以看到 YAML 的可读性和易修改性都非常好，目前大部分语言都直接支持了。</p><h2 id="Toml"><a href="#Toml" class="headerlink" title="Toml"></a>Toml</h2><p>TOML 是前 GitHub CEO 于2013年创建的语言，其目标是成为一个小规模的易于使用的语义化配置文件格式。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个TOML文件</span></span><br><span class="line"></span><br><span class="line"><span class="attr">title</span> = <span class="string">"TOML Example"</span></span><br><span class="line"></span><br><span class="line"><span class="section">[owner]</span></span><br><span class="line"><span class="attr">name</span> = <span class="string">"Lance Uppercut"</span></span><br><span class="line"><span class="attr">dob</span> = <span class="number">1979</span>-<span class="number">05</span>-<span class="number">27</span>T07:<span class="number">32</span>:<span class="number">00</span>-<span class="number">08</span>:<span class="number">00</span> <span class="comment"># 日期是一等公民</span></span><br><span class="line"></span><br><span class="line"><span class="section">[database]</span></span><br><span class="line"><span class="attr">server</span> = <span class="string">"192.168.1.1"</span></span><br><span class="line"><span class="attr">ports</span> = [ <span class="number">8001</span>, <span class="number">8001</span>, <span class="number">8002</span> ]</span><br><span class="line"><span class="attr">connection_max</span> = <span class="number">5000</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="section">[servers]</span></span><br><span class="line">  <span class="comment"># 可以使用空格、制表符进行缩进，或者根本不缩进。TOML不关心缩进。</span></span><br><span class="line">  <span class="section">[servers.alpha]</span></span><br><span class="line">  ip = "10.0.0.1"</span><br><span class="line">  dc = "eqdc10"</span><br></pre></td></tr></table></figure><p>乍看和 INI 差不多，但事实上扩展性比 INI 强多了，基本上能完整描述大部分的数据结构了。</p><h2 id="自定义-Conf-配置文件"><a href="#自定义-Conf-配置文件" class="headerlink" title="自定义 Conf 配置文件"></a>自定义 Conf 配置文件</h2><p>有一些程序会自己定义配置文件格式，例如 <code>Redis</code> <code>Nginx</code> <code>Supervisor</code>。<br>举个 Nginx 的配置文件作为例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user  www www;</span><br><span class="line">worker_processes  2;</span><br><span class="line">error_log  logs/error.log;</span><br><span class="line">#error_log  logs/error.log  notice;</span><br><span class="line">#error_log  logs/error.log  info;</span><br><span class="line">pid        logs/nginx.pid;</span><br><span class="line">events &#123;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  2048;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自定义配置文件一般来说基本不需要跨语言访问了（甚至同语言也不需要访问，只需要他自己一个程序访问就够了）<br>Nginx 自定义的配置文件可读性应该比用 JSON 描述会高很多，但合法性的校验目前没有太多的标准。</p><h1 id="PostView"><a href="#PostView" class="headerlink" title="PostView"></a>PostView</h1><p>配置文件的发展和变革，事实上也是编程语言的发展和变革，我们知道，在计算机领域通常「没有银弹」。但事实上有一些更优秀的配置文件格式正在渐渐替代老旧的格式。</p><p>但目前来看，并不存在一种配置文件格式能通杀所有的需求。所以，长期来看，还是会出现多种配置文件并存的现象的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/uploads/persister-frequent-conf-int
      
    
    </summary>
    
      <category term="Computer" scheme="https://xhinliang.github.io/categories/Computer/"/>
    
    
      <category term="Computer" scheme="https://xhinliang.github.io/tags/Computer/"/>
    
  </entry>
  
  <entry>
    <title>常见数据库简介</title>
    <link href="https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-db-intro/"/>
    <id>https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-db-intro/</id>
    <published>2018-11-18T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="/uploads/persister-frequent-db-intro-overview-548706-637199619492944531-16x9.jpg" alt="overview"></p><p>数据库大概是后端程序员最常用的中间件之一，今天我们来聊聊常见的数据库。</p><h2 id="MySQL-派系"><a href="#MySQL-派系" class="headerlink" title="MySQL 派系"></a>MySQL 派系</h2><p>MySQL 无疑是世界上最热门的数据库之一。也正因为足够热门，它衍生出了不少分支，各自都有一些不同的特点。</p><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><p>在国内，MySQL 是最常见的数据库之一，也是 MySQL 派系中最主流的分支，目前由 Oracle 公司维护。</p><p>特点：</p><ol><li>源代码使用 C 和 C++ 编写，性能稳定。</li><li>支持多种数据库引擎（MyISAM、InnoDB、Memory、MyRocks 等），可以满足不同场景下的需要。</li><li>InnoDB 支持事务，但 MyISAM 不支持。</li><li>索引使用 B+ 树实现，很多操作以顺序读取为主，对 HDD 更友好。</li><li>MyISAM 中每个索引都是一级索引；而 InnoDB 中除主键外的索引都是二级索引（先通过二级索引找到主键，再由主键定位数据）。</li><li>分为社区版（免费）和商业版（收费）两种授权模式。</li><li>支持主从配置：主库读写，从库只读。</li></ol><h3 id="MariaDB"><a href="#MariaDB" class="headerlink" title="MariaDB"></a>MariaDB</h3><p>MariaDB 是 MySQL 的一个开源分支，目标是实现对 MySQL 100% 的兼容。<br>特点：</p><ol><li>它的存储引擎与 MySQL 并不完全一致。</li><li>它独特的存储引擎叫 Maria，是 InnoDB 的变体，支持事务；此外还支持 FederatedX、XtraDB 等其他存储引擎。</li><li>与 MySQL 100% 兼容：<ol><li>数据和表定义文件（.frm）是二进制兼容的。</li><li>所有客户端 API、协议和结构都是完全一致的。</li><li>所有文件名、二进制、路径、端口等都是一致的。</li></ol></li><li>整体性能与 MySQL 类似。</li></ol><h3 id="TiDB"><a href="#TiDB" class="headerlink" title="TiDB"></a>TiDB</h3><p>TiDB 是 PingCAP 公司推出的开源分布式关系型数据库。<br>简介可参考官网：<a href="https://pingcap.com/docs-cn/" target="_blank" rel="noopener">https://pingcap.com/docs-cn/</a></p><p>特点：</p><ul><li>基本兼容 MySQL 协议（但不是 100% 兼容）。</li><li>支持分布式事务（这一点很强）。</li><li>支持在线 DDL。</li><li>100% 支持标准 ACID 事务。</li><li>不同于 MySQL 的主从复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证；且在不丢失大多数副本的前提下，还能实现故障自动恢复（auto-failover），无需人工介入（这点也很强）。</li><li>也支持大多数 OLAP 场景（OLAP：联机分析处理）。</li></ul><h3 id="AliSQL"><a href="#AliSQL" class="headerlink" title="AliSQL"></a>AliSQL</h3><p>AliSQL 是另一个 MySQL 的分支版本，目前由 Alibaba 维护。<a href="https://github.com/alibaba/AliSQL" target="_blank" rel="noopener">GitHub</a></p><p>官方描述是：「在通用基准测试场景下，AliSQL 版本比 MySQL 官方版本有着 70% 的性能提升；在秒杀场景下，性能提升 100 倍。」</p><p>特点：</p><ul><li>100% 兼容 MySQL。</li><li>也是另一套存储引擎。</li><li>虽然由 Alibaba 维护，但国内相关资料极少。</li><li>开源声势很大，但源代码已经很久没有更新（估计内部仍在开发，对外开放的版本相对滞后）。</li></ul><h2 id="PostgreSQL"><a href="#PostgreSQL" class="headerlink" title="PostgreSQL"></a>PostgreSQL</h2><p>PostgreSQL 和 MySQL 类似，也是关系型数据库；但它还有一个特点：它是对象关系数据库管理系统（ORDBMS）。</p><p>提到 PostgreSQL，就不得不把它和 MySQL 对比一下，<a href="https://blog.csdn.net/tiandao2009/article/details/79839037" target="_blank" rel="noopener">这里有简单的对比</a>。</p><p>特点：</p><ul><li>更偏学院派（这是个哲学问题，这里不展开）。</li><li>多进程架构；相比之下，MySQL 是多线程的。</li><li>支持同步、异步、半同步的 replica，属于物理复制。</li><li>JOIN 性能相比 MySQL 有明显优势。</li></ul><h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><p>MongoDB 不是关系型数据库，而是一种 NoSQL。<br>特点：</p><ul><li>天生支持分布式，对扩展友好。</li><li>不支持 ACID 事务，但作为替代，有个新的概念：BASE。基本可用（Basically Available）、软状态/柔性事务（Soft state）、最终一致性（Eventual consistency）。</li><li>在一些场景下，基本可以替代 MySQL 使用。</li></ul><h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>HBase 是建立在 HDFS 之上的分布式面向列数据库，也是 Google 著名论文《BigTable》的开源实现。</p><p><a href="https://blog.csdn.net/nosqlnotes/article/details/79647096" target="_blank" rel="noopener">这篇文章讲得不错</a>。</p><p>HBase 是面向列的数据库，在表中按行排序。表模式定义只能列族，也就是键值对。一个表可以有多个列族，每个列族又可以有任意数量的列。后续列的值会连续地存储在磁盘上。表中的每个单元格值都具有时间戳。</p><p>HBase 常被用来存放结构简单但数据量非常大的数据。</p><p>特点：</p><ul><li>较大的表中能快速查找。</li><li>数十亿条记录低延迟访问单个行记录（随机存取）。</li><li>面向列的数据库。</li><li>不具有固定列模式的概念，仅定义列族，每个列族可以包含多个列。</li><li>大宽表，列值是稀疏的，而且是半结构化的数据。</li><li>不支持任何事务。</li><li>索引能力有限。</li><li>支持自动故障恢复。</li></ul><h2 id="InfluxDB"><a href="#InfluxDB" class="headerlink" title="InfluxDB"></a>InfluxDB</h2><p>InfluxDB 是一个开源的时序数据库，使用 Golang 开发，特别适合处理和分析资源监控数据这类时序相关数据。</p><p>InfluxDB 自带多种特殊函数，如求标准差、随机取样数据、统计数据变化比等，使数据统计和实时分析变得更方便。</p><p>特点：</p><ul><li>数据可以被标记，从而支持非常灵活的查询。</li><li>支持一部分 SQL 语句。</li><li>适合 OLAP 或监控需求。</li></ul><h2 id="OpenTSDB"><a href="#OpenTSDB" class="headerlink" title="OpenTSDB"></a>OpenTSDB</h2><p>也是一种时序数据库，但和 InfluxDB 不同的是，它依赖 HBase 实现。</p><h2 id="GpDB-–-Greenplum"><a href="#GpDB-–-Greenplum" class="headerlink" title="GpDB – Greenplum"></a>GpDB – Greenplum</h2><p>Greenplum 数据库（也叫 GPDB）是一个分布式数据库，也是数据仓库的快速查询工具。</p><p>特点：</p><ul><li>支持 SQL。</li><li>支持分布式事务。</li><li>支持线性扩展。</li></ul><h2 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h2><p>近年来兴起的一个 OLAP 数据库，由俄罗斯公司 Yandex 开发，性能强劲。</p><h2 id="LevelDb"><a href="#LevelDb" class="headerlink" title="LevelDb"></a>LevelDb</h2><p>LevelDb 是 Google 实现的高效 KV 数据库，能够支持 billion 级别的数据量。在这个数量级别下仍能保持很高的性能，主要归功于它良好的设计，尤其是 LSM 算法。</p><h2 id="RocksDb"><a href="#RocksDb" class="headerlink" title="RocksDb"></a>RocksDb</h2><p>RocksDB 是 Facebook 基于 C++ 编写的嵌入式 KV 存储引擎，其键和值都允许使用二进制流，并提供向后兼容的 LevelDB API。</p><p>RocksDB 针对 Flash 存储进行优化，延迟极小。RocksDB 使用 LSM 存储引擎，纯 C++ 编写。Java 版本 RocksJava 正在开发中。</p><p>我理解 RocksDb 应该是 LevelDb 的另一种实现，从功能上算是超集吧。</p><h2 id="MyRocks"><a href="#MyRocks" class="headerlink" title="MyRocks"></a>MyRocks</h2><p>MySQL 兼容的 RocksDb，底层实现基本与 RocksDB 一致，但作为一种存储引擎在 MySQL 中使用。国内有一些公司在使用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/uploads/persister-frequent-db-intro
      
    
    </summary>
    
      <category term="Computer" scheme="https://xhinliang.github.io/categories/Computer/"/>
    
    
      <category term="Computer" scheme="https://xhinliang.github.io/tags/Computer/"/>
    
  </entry>
  
  <entry>
    <title>常用的文件系统简介</title>
    <link href="https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-fs-intro/"/>
    <id>https://xhinliang.github.io/2018/11/computer/frequent-series/frequent-fs-intro/</id>
    <published>2018-11-10T00:57:29.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="/uploads/fs-intro-overview.jpg" alt="overview"></p><p>文件系统算是计算机软件中比较底层的一部分，可能很多程序员都不会直接接触到。<br>今天简单学习了一下，做个笔记。</p><h2 id="FAT-文件系统"><a href="#FAT-文件系统" class="headerlink" title="FAT 文件系统"></a>FAT 文件系统</h2><p>FAT 文件系统起源于二十世纪七十年代，最初用于 DOS 系统。早期设计比较简单，后来随着存储介质的发展，逐渐出现了一些增强版本。<br>所以现在主要包含这几个版本：FAT8、FAT12、FAT16、FAT32。<br>这些版本名后面的数字是什么意思呢？其实就是「簇」地址编码占用的位宽。例如，FAT12 的意思是簇编址采用 12 bit 宽度，因此这种文件系统最多只能寻址 2 ** 12 = 4096 个簇。其他 FAT 变种类似，就不赘述了。</p><p>这里需要注意：FAT 不是一种日志文件系统。至于「日志文件系统」，稍后会简单介绍一下。</p><h2 id="NTFS-文件系统"><a href="#NTFS-文件系统" class="headerlink" title="NTFS 文件系统"></a>NTFS 文件系统</h2><p>NTFS（New Technology File System）是微软公司推出的、用于大型存储介质的 <strong>日志</strong> 文件系统，从 Windows 2000 左右的版本开始支持。<br>从技术角度看，NTFS 相对比较先进，主要特点如下：</p><ul><li>安全性高：NTFS 支持基于文件或目录的 ACL，并支持加密文件系统（EFS）。</li><li>可恢复性：NTFS 支持基于原子事务概念的文件恢复，更符合服务器文件系统的要求。</li><li>文件压缩：NTFS 支持基于文件或目录的文件压缩，可以更方便地节省磁盘空间。</li><li>磁盘配额：NTFS 支持磁盘配额，可针对系统中每个用户分配磁盘资源。</li></ul><h2 id="日志文件系统"><a href="#日志文件系统" class="headerlink" title="日志文件系统"></a>日志文件系统</h2><p>日志文件系统不是一种具体的文件系统，这里主要突出「日志」这两个字。<br>日志式文件系统起源于 Oracle、Sybase 等大型数据库。数据库操作往往由多个相关、相互依赖的子操作组成，任何一个子操作的失败都意味着整个操作的无效性，因此对数据库数据的任何修改都要恢复到操作以前的状态。Linux 日志式文件系统就是由此发展而来的。日志文件系统通过增加一个叫做日志的、新的数据结构来解决这个“fsck”问题。这个日志是位于磁盘上的结构：在对元数据做任何改变以前，文件系统驱动程序会向日志中写入一个条目，这个条目描述了它将要做些什么，所以日志文件具有可伸缩性和健壮性。在分区中保存日志记录文件的好处是：文件系统写操作首先会对记录文件进行操作；若整个写操作由于某种原因（如系统掉电）而中断，则在下次系统启动时就会读日志记录文件的内容，恢复未完成的写操作，这个过程一般只需要两三分钟。</p><p>我们可以看到，日志文件系统是大型数据库系统的基础。现在 Linux 上的文件系统基本上都是日志文件系统。</p><h2 id="Ext"><a href="#Ext" class="headerlink" title="Ext"></a>Ext</h2><p>ext 是第一种专门为 Linux 开发的文件系统类型，叫做扩展文件系统。它于 1992 年 4 月完成，对 Linux 早期的发展产生了重要作用。但是，由于其在稳定性、速度和兼容性上存在许多缺陷，现在已经很少使用了。</p><p>Ext 不是一种日志文件系统，它不支持日志功能。</p><h2 id="Ext2"><a href="#Ext2" class="headerlink" title="Ext2"></a>Ext2</h2><p>ext2 是为解决 ext 文件系统的缺陷而设计的可扩展、高性能文件系统，又被称为二级扩展文件系统。ext2 于 1993 年发布，设计者是 Rey Card。它是 Linux 文件系统类型中使用最多的格式，并且在速度和 CPU 利用率上较为突出，是 GNU/Linux 系统中标准的文件系统。它存取文件的性能极好，对于中、小型文件更显示出优势，这主要得益于其簇快取层的优良设计。ext2 可以支持 256 字节的长文件名；其单一文件大小和文件系统本身的容量上限，与文件系统本身的簇大小有关。在常见的 Intel x86 兼容处理器系统中，簇最大为 4KB，单一文件大小上限为 2048GB，而文件系统的容量上限为 6384GB。尽管 Linux 可以支持种类繁多的文件系统，但是 2000 年以前几乎所有的 Linux 发行版都使用 ext2 作为默认的文件系统。</p><p>ext2 也有一些问题。由于它在设计时主要考虑的是文件系统性能，而在写入文件内容的同时，并没有写入文件的 meta-data（和文件有关的信息，例如权限、所有者及创建和访问时间）。换句话说，Linux 先写入文件内容，然后等到有空的时候才写入文件的 meta-data。如果出现文件内容写入完成后、但在写入 meta-data 之前系统突然断电，就可能造成文件系统处于不一致的状态。在一个有大量文件操作的系统中，出现这种情况会导致很严重的后果。另外，由于目前 Linux 的 2.4 内核所能使用的单一分割区最大只有 2048GB，尽管文件系统的容量上限为 6384G，但是实际上能使用的文件系统容量最多也只有 2048GB。</p><p>Ext2 也不是一种日志文件系统，它也不支持日志功能。</p><h2 id="Ext3"><a href="#Ext3" class="headerlink" title="Ext3"></a>Ext3</h2><p>ext3 是由开源社区开发的日志文件系统，早期主要开发人员是 Stephen Tweedie。ext3 被设计成 ext2 的升级版本，尽可能方便用户从 ext2 向 ext3 迁移。ext3 在 ext2 的基础上加入了记录元数据的日志功能，努力保持向前和向后的兼容性，也就是在保有目前 ext2 格式的前提下再加上日志功能。和 ext2 相比，ext3 提供了更佳的安全性，这就是数据日志和元数据日志之间的不同。ext3 是一种日志式文件系统，日志式文件系统的优越性在于：由于文件系统都有快取层参与运作，如不使用时必须将文件系统卸下，以便将快取层的资料写回磁盘中。因此，每当系统要关机时，必须将其所有的文件系统全部卸下后才能进行关机。如果在文件系统尚未卸下前就关机（如停电），那么重开机后就会造成文件系统的资料不一致，故这时必须做文件系统的重整工作，将不一致与错误的地方修复。然而，这个过程相当耗时，特别是容量大的文件系统，不能百分之百保证所有的资料都不会流失，在大型服务器上可能会出现问题。除了与 ext2 兼容之外，ext3 还通过共享 ext2 的元数据格式继承了 ext2 的其它优点。比如，ext3 用户可以使用一个稳固的 fsck 工具。由于 ext3 基于 ext2 的代码，所以它的磁盘格式和 ext2 相同，这意味着一个干净卸载的 ext3 文件系统可以作为 ext2 文件系统毫无问题地重新挂载。如果现在使用的是 ext2 文件系统，并且对数据安全性要求很高，这里建议考虑升级使用 ext3。</p><h2 id="Ext4"><a href="#Ext4" class="headerlink" title="Ext4"></a>Ext4</h2><p>第四扩展日志文件系统（fourth extended journaling file system，ext4fs）是由 ext3fs 演化而来。Ext4 文件系统被设计为具有向前和向后兼容性，但它具有许多新的高级特性（其中的一些特性破坏了兼容性）。这就意味着您可以将 ext4fs 的一部分作为 ext3fs 挂载，反之亦然。</p><p>首先，ext4fs 是 64 位文件系统，并被设计为可以支持很大的容量（1 exabyte）。它还可以使用分区，但是这样做将失去与 ext3fs 的兼容性。像 XFS 和 Reiser4 一样，ext4fs 还支持在必要时采取延时分配方式分配块（这样可以减少磁盘碎片）。日志的内容也已经执行过校验和（checksum）检查，使日志更加可靠。ext4fs 并没有采用标准的 B+ 或者 B* 树，取而代之的是 B 树的一种变体，叫做 H 树，它支持更大的子目录（ext3 的上限为 32KB）。</p><p>虽然延时分配的方法可以减少磁盘碎片，但时间久了，一个大的文件系统仍可能产生碎片。为解决这个问题，开发了在线磁盘碎片整理工具（e4defrag）。您可以使用这个工具来整理单个文件或者整个文件系统。</p><p>ext3fs 与 ext4fs 间的另一个有趣区别就在于文件的时间分辨率。在 ext3 中，时间戳的最小分辨率为 1 秒。而 Ext4fs 是面向未来的：那时处理器和接口的速度会持续加快，需要更高的分辨率。因此，ext4 中时间戳的最小分辨率为 1 纳秒。</p><p>Ext4 已经在 Linux 2.6 以后的版本中应用非常广泛了。</p><h2 id="ZFS"><a href="#ZFS" class="headerlink" title="ZFS"></a>ZFS</h2><p>ZFS 是一种比 Ext4 更先进的文件系统，是一种比日志文件系统更先进的「事务性文件系统」，起源于 Sun 公司的 Solaris 系统。主要有以下特点：</p><ul><li>引入「存储池」的概念，理论上没有存储容量限制。</li><li>支持写时拷贝。</li><li>支持事务性语义。</li><li>支持校验和自我恢复。</li><li>支持快照。</li></ul><h2 id="HFS"><a href="#HFS" class="headerlink" title="HFS"></a>HFS</h2><p>分层文件系统（Hierarchical File System，HFS）是一种由苹果计算机开发，并使用在 macOS 上的文件系统。最初被设计用于软盘和硬盘，同时也可以在只读媒体（如 CD-ROM）上见到。</p><h2 id="HFS-1"><a href="#HFS-1" class="headerlink" title="HFS+"></a>HFS+</h2><p>HFS+ 是一个 HFS 的改进版本，支持更大的文件，并用 Unicode 来命名文件或文件夹，代替了 Mac OS Roman 或其他一些字符集。和 HFS 一样，HFS+ 也使用 B 树来存储大部分分卷元数据。<br>尽管 HFS+ 比现有的先进文件系统（NTFS、ZFS）落后许多，但现在的 macOS 依然使用 HFS+。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/uploads/fs-intro-overview.jpg&quot; alt=
      
    
    </summary>
    
      <category term="Computer" scheme="https://xhinliang.github.io/categories/Computer/"/>
    
    
      <category term="Linux" scheme="https://xhinliang.github.io/tags/Linux/"/>
    
      <category term="macOS" scheme="https://xhinliang.github.io/tags/macOS/"/>
    
      <category term="FileSystem" scheme="https://xhinliang.github.io/tags/FileSystem/"/>
    
  </entry>
  
  <entry>
    <title>多级缓存的设计与实现</title>
    <link href="https://xhinliang.github.io/2018/10/backend/multi-level-cache/"/>
    <id>https://xhinliang.github.io/2018/10/backend/multi-level-cache/</id>
    <published>2018-10-30T01:43:29.000Z</published>
    <updated>2026-02-25T07:09:26.608Z</updated>
    
    <content type="html"><![CDATA[<p>在高并发的后端业务中，多级缓存是一种非常常见的设计。<br>事实上，多级缓存策略在计算机硬件中也普遍存在。</p><p><img src="/uploads/persister-multi-level-cache-overview-cache-memory-4113.jpg" alt="overview"></p><p>为什么会有多级缓存？<br>通常缓存都比原数据要小。设立缓存的目的很简单：某些数据经常被访问，我们不想每次都去最深的地方（这里一般是指数据库）查找，所以会在更方便取到数据的地方把这些数据保存起来。</p><p>那么，为什么要用多级缓存呢？<br>对所有数据来说，访问频率并不一致。对于访问频率非常高的数据，我们称为热点数据。<br>例如，某个大 V 发了一条微博告诉大家他结婚了，那么这一条微博就是一个热点数据。热到不行时，我们的服务器平均每毫秒要访问这条数据一万次（不夸张……）。<br>我在上一篇文章里也提到过，分布式缓存 100us 的读取已经算是正常成绩了，但这个成绩完全不够支撑。我们必须有更快的缓存来保存这条数据，以减轻分布式缓存的压力。</p><p>对业务开发者来说，显式使用 CPU 的 L1、L2、L3 显然不现实。于是，我们只能指望主板上的内存条了。</p><p>现在的内存性能已经很强了，10000 mb/s 的读取速度也很普遍，至于延迟我还没有太清晰的概念。<br>既然内存这么强，我们就要尽量利用好它，所以内存通常会作为多级缓存的最顶层。</p><p>这篇文章中，我就尝试用 Java 来实现一下多级缓存。</p><p>各门语言的实现思路都大同小异。我这一年里基本都在写 Java，所以就直接用 Java 来写了。</p><h2 id="Preconditions-预设计"><a href="#Preconditions-预设计" class="headerlink" title="Preconditions 预设计"></a>Preconditions 预设计</h2><p>对一个合格的后端服务来说，多级缓存的设计至少应包括以下几个功能：</p><ul><li>支持过期清理</li><li>支持容量限制及逐出策略</li><li>支持回源及回写机制（后面我会解释回源和回写这两个概念）</li></ul><h3 id="Back-to-Source-回源"><a href="#Back-to-Source-回源" class="headerlink" title="Back-to-Source 回源"></a>Back-to-Source 回源</h3><p>通常说的回源，是指 CDN 层面上的回源：在这个 CDN 节点上没找到相关资源，就回到「源站」去获取该资源。<br>放到我们的缓存设计中，在缓存层面上，回源也是类似的意思：在这一层缓存中没找到这个数据，就到下一层「数据源」去获取资源。</p><h3 id="Write-Back-回写"><a href="#Write-Back-回写" class="headerlink" title="Write-Back 回写"></a>Write-Back 回写</h3><p>回写建立在回源的基础上。<br>当某一层缓存没找到资源时，就到下一层去找；如果找到了，这一层就把这个数据缓存起来。<br>这样下次再取这个数据的时候，就能马上取到了。</p><h2 id="Code-搞起"><a href="#Code-搞起" class="headerlink" title="Code 搞起"></a>Code 搞起</h2><p>简单写了一版，轻喷： <a href="https://github.com/XhinLiang/multi-level-cache" target="_blank" rel="noopener">https://github.com/XhinLiang/multi-level-cache</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在高并发的后端业务中，多级缓存是一种非常常见的设计。&lt;br&gt;事实上，多级缓存策略在计算机硬件中也普遍存在。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/uploads/persister-multi-level-cache-overview-cache-memory-4113.jp
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Java" scheme="https://xhinliang.github.io/tags/Java/"/>
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="Concurrent" scheme="https://xhinliang.github.io/tags/Concurrent/"/>
    
  </entry>
  
  <entry>
    <title>为什么不建议在 Redis 使用大 Key</title>
    <link href="https://xhinliang.github.io/2018/10/backend/big-key-in-redis/"/>
    <id>https://xhinliang.github.io/2018/10/backend/big-key-in-redis/</id>
    <published>2018-10-30T00:00:00.000Z</published>
    <updated>2026-02-25T07:09:26.607Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Preview"><a href="#Preview" class="headerlink" title="Preview"></a>Preview</h2><p><img src="/uploads/persister-big-key-in-redis-overview-Redis.png" alt="overview"></p><p>公司里某位工程师小斌发现在一个 Redis 集群中的 some_big_list 经常出现慢查询，而且 QPS 特别高。初步定位是出现了一个热点的 Key。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">newexplore&gt; llen some_big_list</span><br><span class="line">500000</span><br></pre></td></tr></table></figure></p><p>上面的命令发现，这个 some_big_list 是一个大 Key，导致 Redis Server 的服务器 CPU idle 很低，结果出现了慢查询。</p><p>当公司里富有经验的工程师磊哥介入调查的时候，发现这个 Redis Server 的所有响应在某一个时刻出现了 block。<br>磊哥是很牛逼的，怀疑是不是小斌在线上直接执行了 <code>del some_big_list</code> 操作。</p><p>结果当然是啦！</p><p>小斌 <code>del some_big_list</code> 的时候， Redis 的单线程模型只顾着删数据了（<code>del</code> 的时间复杂度是 O(N)），没有时间响应请求，直接导致出现一大堆请求超时。<br>而线上的 Redis 请求超时，又会让 Redis 的线程池打满，从而线上的 API Server 的 CPU 也会直接飙升。</p><p>不过还好，这个不是太大的 Key，这个业务也不是很核心的业务，所以没有造成什么影响。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>上面的例子告诉我们，Redis 大 Key 是一件很容易造成线上事故的事情，我们在业务上要尽量避免大 Key 的产生。<br>当不小心产生大 Key 的时候，我们也不能直接把整个 key 直接 del 掉，会影响其他的业务（不读取这个 key 的业务也会收到影响）。</p><p>那么，我们在出现 Redis 大 Key 的时候，应该怎么处理呢？</p><ul><li>业务层面，可以使用更小粒度的单位（时间单位或者地点单位，what ever），先换一个 keyPrefix。</li><li>换了 keyPrefix 之后，需要对老的 keyPrefix 作清理<ul><li>如果是 Redis 4.0 以后的版本，可以把 <code>del</code> 命令换成 <code>unlink</code> 命令，使用这个命令的时候，Redis 会另起一个线程进行删除，不会影响别的业务请求。其实也很好理解，既然把这个 key 删除了，那么也很容易搞定多线程不一致的问题了。</li><li>如果是更老的版本，可以设立一个准则：慢慢删…<ul><li>如果是 <code>list</code>，可以使用 <code>ltrim</code> 慢慢删</li><li>如果是 <code>zset</code>, 可以使用 <code>zremrangebyscore</code> 之类的命令慢慢删…</li><li>如果是 <code>hash</code> 或者 <code>set</code>，比较麻烦，请自己慢慢找合适的命令吧。。</li></ul></li></ul></li></ul><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>好了，说了基础知识，我们来实地演练一下。</p><h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>首先说下我对分布式系统的 “响应时间” 的认知：</p><ul><li><code>t&lt;0.1ms</code>，非常快，没有优化的必要</li><li><code>0.1ms&lt;t&lt;1ms</code>，挺快的，基本不会对系统造成瓶颈</li><li><code>1ms&lt;t&lt;10ms</code>，凑活用，如果不是核心链路，基本 OK</li><li><code>10ms&lt;t&lt;100ms</code>，有点慢了，需要找时间优化</li><li><code>100ms&lt;t&lt;1000ms</code>，太慢了，赶紧查下原因吧！</li><li><code>t&gt;1000ms</code>，这他妈怎么用啊，下线算了</li></ul><p>注意这个是分布式系统的响应时间，而 Redis 作为一个最基础的缓存中间件，我认为对他的要求要更高一些（上述的响应时间 / 10 吧）</p><h3 id="Platform"><a href="#Platform" class="headerlink" title="Platform"></a>Platform</h3><p>这场测试在我自己的电脑上做，先来了解下我的爱机：</p><p>CPU：<a href="mailto:i5-3380m@3.6Ghz" target="_blank" rel="noopener">i5-3380m@3.6Ghz</a>，双核四线程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/cpuinfo</span><br><span class="line">processor: 0</span><br><span class="line">vendor_id: GenuineIntel</span><br><span class="line">cpu family: 6</span><br><span class="line">model: 58</span><br><span class="line">model name: Intel(R) Core(TM) i5-3380M CPU @ 2.90GHz</span><br><span class="line">stepping: 9</span><br><span class="line">microcode: 0x20</span><br><span class="line">cpu MHz: 1272.714</span><br><span class="line">cache size: 3072 KB</span><br><span class="line">physical id: 0</span><br><span class="line">siblings: 4</span><br><span class="line">core id: 0</span><br><span class="line">cpu cores: 2</span><br><span class="line">apicid: 0</span><br><span class="line">initial apicid: 0</span><br><span class="line">fpu: yes</span><br><span class="line">fpu_exception: yes</span><br><span class="line">cpuid level: 13</span><br><span class="line">wp: yes</span><br><span class="line">flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm epb ssbd ibrs ibpb stibp kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flush_l1d</span><br><span class="line">bugs: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf</span><br><span class="line">bogomips: 5780.76</span><br><span class="line">clflush size: 64</span><br><span class="line">cache_alignment: 64</span><br><span class="line">address sizes: 36 bits physical, 48 bits virtual</span><br><span class="line">power management:</span><br></pre></td></tr></table></figure></p><p>内存： 2×8G DDR3L 1600Mhz<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:         15951      13847       2103        445       2331       6722</span><br><span class="line">-/+ buffers/cache:       4794      11156</span><br><span class="line">Swap:        15624          0      15624</span><br></pre></td></tr></table></figure></p><p>系统版本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/os-release</span><br><span class="line">NAME=<span class="string">"Ubuntu"</span></span><br><span class="line">VERSION=<span class="string">"14.04.5 LTS, Trusty Tahr"</span></span><br><span class="line">ID=ubuntu</span><br><span class="line">ID_LIKE=debian</span><br><span class="line">PRETTY_NAME=<span class="string">"Ubuntu 14.04.5 LTS"</span></span><br><span class="line">VERSION_ID=<span class="string">"14.04"</span></span><br><span class="line">HOME_URL=<span class="string">"http://www.ubuntu.com/"</span></span><br><span class="line">SUPPORT_URL=<span class="string">"http://help.ubuntu.com/"</span></span><br><span class="line">BUG_REPORT_URL=<span class="string">"http://bugs.launchpad.net/ubuntu/"</span></span><br></pre></td></tr></table></figure></p><p>系统限制<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ulimit</span> -a</span><br><span class="line">-t: cpu time (seconds)              unlimited</span><br><span class="line">-f: file size (blocks)              unlimited</span><br><span class="line">-d: data seg size (kbytes)          unlimited</span><br><span class="line">-s: stack size (kbytes)             8192</span><br><span class="line">-c: core file size (blocks)         0</span><br><span class="line">-m: resident <span class="built_in">set</span> size (kbytes)      unlimited</span><br><span class="line">-u: processes                       63332</span><br><span class="line">-n: file descriptors                65535</span><br><span class="line">-l: locked-in-memory size (kbytes)  64</span><br><span class="line">-v: address space (kbytes)          unlimited</span><br><span class="line">-x: file locks                      unlimited</span><br><span class="line">-i: pending signals                 63332</span><br><span class="line">-q: bytes <span class="keyword">in</span> POSIX msg queues       819200</span><br><span class="line">-e: max nice                        0</span><br><span class="line">-r: max rt priority                 0</span><br><span class="line">-N 15:                              unlimited</span><br></pre></td></tr></table></figure></p><p>硬盘：美光 M500,480G<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=<span class="built_in">test</span> bs=64k count=512 oflag=dsync</span><br><span class="line">记录了512+0 的读入</span><br><span class="line">记录了512+0 的写出</span><br><span class="line">33554432字节(34 MB)已复制，5.16719 秒，6.5 MB/秒</span><br></pre></td></tr></table></figure></p><p>Redis版本：2.8，很老的版本了，连 Redis-Cluster 都不支持的版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ redis-server -v</span><br><span class="line">Redis server v=2.8.4 sha=00000000:0 malloc=jemalloc-3.4.1 bits=64 build=a44a05d76f06a5d9</span><br></pre></td></tr></table></figure></p><h3 id="Upgrade-Redis"><a href="#Upgrade-Redis" class="headerlink" title="Upgrade Redis"></a>Upgrade Redis</h3><p>非常尴尬，我的机器上的 Redis 居然是 2.8 这个上古世纪的版本，既然这样我就把它先删了吧（笑）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove redis-server</span><br></pre></td></tr></table></figure><p>一顿操作猛如虎，编译源代码走起！<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-5.0.0.tar.gz</span><br><span class="line">$ tar xzf redis-5.0.0.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> redis-5.0.0</span><br><span class="line">$ make <span class="comment"># 编译走起</span></span><br><span class="line">$ make <span class="built_in">test</span> <span class="comment"># 编译完了跑个测试吧</span></span><br></pre></td></tr></table></figure></p><p>Redis 是很轻量的，按理说编译不会有什么坑，果然顺滑无比，编译加测试总共只花了三分钟。</p><p>那么现在来试下新版本的 Redis 吧！<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-server -v</span><br><span class="line">Redis server v=5.0.0 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=792f3c7998732f3c</span><br><span class="line"></span><br><span class="line">$ src/redis-server</span><br><span class="line">21176:C 29 Oct 2018 00:24:32.717 <span class="comment"># oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span></span><br><span class="line">21176:C 29 Oct 2018 00:24:32.717 <span class="comment"># Redis version=5.0.0, bits=64, commit=00000000, modified=0, pid=21176, just started</span></span><br><span class="line">21176:C 29 Oct 2018 00:24:32.717 <span class="comment"># Warning: no config file specified, using the default config. In order to specify a config file use src/redis-server /path/to/redis.conf</span></span><br><span class="line">                _._                                                  </span><br><span class="line">           _.-``__ <span class="string">''</span>-._                                             </span><br><span class="line">      _.-``    `.  `_.  <span class="string">''</span>-._           Redis 5.0.0 (00000000/0) 64 bit</span><br><span class="line">  .-`` .-```.  ```\/    _.,_ <span class="string">''</span>-._                                   </span><br><span class="line"> (    <span class="string">'      ,       .-`  | `,    )     Running in standalone mode</span></span><br><span class="line"><span class="string"> |`-._`-...-` __...-.``-._|'</span>` _.-<span class="string">'|     Port: 6379</span></span><br><span class="line"><span class="string"> |    `-._   `._    /     _.-'</span>    |     PID: 21176</span><br><span class="line">  `-._    `-._  `-./  _.-<span class="string">'    _.-'</span>                                   </span><br><span class="line"> |`-._`-._    `-.__.-<span class="string">'    _.-'</span>_.-<span class="string">'|                                  </span></span><br><span class="line"><span class="string"> |    `-._`-._        _.-'</span>_.-<span class="string">'    |           http://redis.io        </span></span><br><span class="line"><span class="string">  `-._    `-._`-.__.-'</span>_.-<span class="string">'    _.-'</span>                                   </span><br><span class="line"> |`-._`-._    `-.__.-<span class="string">'    _.-'</span>_.-<span class="string">'|                                  </span></span><br><span class="line"><span class="string"> |    `-._`-._        _.-'</span>_.-<span class="string">'    |                                  </span></span><br><span class="line"><span class="string">  `-._    `-._`-.__.-'</span>_.-<span class="string">'    _.-'</span>                                   </span><br><span class="line">      `-._    `-.__.-<span class="string">'    _.-'</span>                                       </span><br><span class="line">          `-._        _.-<span class="string">'                                           </span></span><br><span class="line"><span class="string">              `-.__.-'</span>                                               </span><br><span class="line"></span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 <span class="comment"># WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.</span></span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 <span class="comment"># Server initialized</span></span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 <span class="comment"># WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.</span></span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 <span class="comment"># WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span></span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 * DB loaded from disk: 0.000 seconds</span><br><span class="line">21176:M 29 Oct 2018 00:24:32.720 * Ready to accept connections</span><br></pre></td></tr></table></figure></p><p>Server 成功跑起来了，我们来开启另一个 Shell 窗口来启动 Client：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-cli</span><br><span class="line"><span class="number">127.0.0.1:6379</span>&gt;</span><br></pre></td></tr></table></figure></p><p>So Easy～<br>先看看现在 Redis 占用了多少内存吧<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps aef -o <span class="built_in">command</span>,vsize,rss,%mem,size | grep redis-server </span><br><span class="line">src/redis-server *:6379  63004  5532  0.0 40964</span><br></pre></td></tr></table></figure></p><p>好吧，0%，先忽略。。</p><p>准备一个 lua 脚本，我们保存为 add-test-big-key.lua<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">local</span> bulk = <span class="number">1000</span></span><br><span class="line"><span class="keyword">local</span> fvs = &#123;&#125;</span><br><span class="line"><span class="keyword">local</span> j</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>, ARGV[<span class="number">1</span>] <span class="keyword">do</span></span><br><span class="line">  j = i % bulk</span><br><span class="line">  <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">then</span></span><br><span class="line">    fvs[<span class="number">2</span> * bulk - <span class="number">1</span>] = <span class="string">"field"</span> .. i</span><br><span class="line">    fvs[<span class="number">2</span> * bulk] = <span class="string">"value"</span> .. i</span><br><span class="line">    redis.call(<span class="string">"HMSET"</span>, KEYS[<span class="number">1</span>], <span class="built_in">unpack</span>(fvs))</span><br><span class="line">    fvs = &#123;&#125;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    fvs[<span class="number">2</span> * j - <span class="number">1</span>] = <span class="string">"field"</span> .. i</span><br><span class="line">    fvs[<span class="number">2</span> * j] = <span class="string">"value"</span> .. i</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> #fvs &gt; <span class="number">0</span> <span class="keyword">then</span></span><br><span class="line">  redis.call(<span class="string">"HMSET"</span>, KEYS[<span class="number">1</span>], <span class="built_in">unpack</span>(fvs))</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"OK"</span></span><br></pre></td></tr></table></figure></p><p>此脚本参考 <a href="https://groups.google.com/d/msg/redis-db/0UzLhSkAziQ/H-35GJfqtisJ" target="_blank" rel="noopener">how to load lua script from file for redis</a></p><p>灌数据，然后看下内存占用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-cli --<span class="built_in">eval</span> add-test-big-key.lua big_hash1 , 1000000</span><br><span class="line"><span class="string">"OK"</span></span><br><span class="line">$ ps aef -o <span class="built_in">command</span>,vsize,rss,%mem,size | grep redis-server</span><br><span class="line">src/redis-server *:6379 134660 74044  0.4 112620</span><br></pre></td></tr></table></figure></p><p>可以看到虽然这个大Key里有100w条数据，但内存占用依然很低。<br>我们现在尝试把这个 key 删除，并查看 SLOWLOG。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; del big_hash1</span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line">127.0.0.1:6379&gt; SLOWLOG GET 2</span><br><span class="line">1) 1) (<span class="built_in">integer</span>) 2009</span><br><span class="line">   2) (<span class="built_in">integer</span>) 1540777173</span><br><span class="line">   3) (<span class="built_in">integer</span>) 373285</span><br><span class="line">   4) 1) <span class="string">"del"</span></span><br><span class="line">      2) <span class="string">"big_hash1"</span></span><br><span class="line">   5) <span class="string">"127.0.0.1:57110"</span></span><br><span class="line">   6) <span class="string">""</span></span><br><span class="line">2) 1) (<span class="built_in">integer</span>) 2008</span><br><span class="line">   2) (<span class="built_in">integer</span>) 1540777157</span><br><span class="line">   3) (<span class="built_in">integer</span>) 2325250</span><br><span class="line">   4) 1) <span class="string">"EVAL"</span></span><br><span class="line">      2) <span class="string">"local bulk = 1000\nlocal fvs = &#123;&#125;\nlocal j\nfor i = 1, ARGV[1] do\n  j = i % bulk\n  if j == 0 then\n    fvs[2 * bulk - 1] = \"field\" .... (254 more bytes)"</span></span><br><span class="line">      3) <span class="string">"1"</span></span><br><span class="line">      4) <span class="string">"big_hash1"</span></span><br><span class="line">      5) <span class="string">"1000000"</span></span><br><span class="line">   5) <span class="string">"127.0.0.1:57234"</span></span><br><span class="line">   6) <span class="string">""</span></span><br></pre></td></tr></table></figure><p>可以看到 del 操作持续了 373ms，在线上环境中，这个影响应该不算大。</p><p>我们把数据量增大 100 倍看下吧，预期 redis-server 会占用 40% 左右的内存（redis好像没有这方面的优化。。）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-cli --<span class="built_in">eval</span> add-test-big-key.lua big_hash1 , 100000000 </span><br><span class="line"><span class="string">"OK"</span></span><br><span class="line">$ ps aef -o <span class="built_in">command</span>,vsize,rss,%mem,size | grep redis-server    </span><br><span class="line">src/redis-server *:6379 7804420 6656092 40.7 7782380</span><br></pre></td></tr></table></figure><p>果不其然。。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; del big_hash1</span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line">(62.95s)</span><br><span class="line">127.0.0.1:6379&gt; SLOWLOG GET 2</span><br><span class="line">1) 1) (<span class="built_in">integer</span>) 102012</span><br><span class="line">   2) (<span class="built_in">integer</span>) 1540777822</span><br><span class="line">   3) (<span class="built_in">integer</span>) 62952411</span><br><span class="line">   4) 1) <span class="string">"del"</span></span><br><span class="line">      2) <span class="string">"big_hash1"</span></span><br><span class="line">   5) <span class="string">"127.0.0.1:57110"</span></span><br><span class="line">   6) <span class="string">""</span></span><br><span class="line">2) 1) (<span class="built_in">integer</span>) 102011</span><br><span class="line">   2) (<span class="built_in">integer</span>) 1540777634</span><br><span class="line">   3) (<span class="built_in">integer</span>) 279820829</span><br><span class="line">   4) 1) <span class="string">"EVAL"</span></span><br><span class="line">      2) <span class="string">"local bulk = 1000\nlocal fvs = &#123;&#125;\nlocal j\nfor i = 1, ARGV[1] do\n  j = i % bulk\n  if j == 0 then\n    fvs[2 * bulk - 1] = \"field\" .... (254 more bytes)"</span></span><br><span class="line">      3) <span class="string">"1"</span></span><br><span class="line">      4) <span class="string">"big_hash1"</span></span><br><span class="line">      5) <span class="string">"100000000"</span></span><br><span class="line">   5) <span class="string">"127.0.0.1:57324"</span></span><br><span class="line">   6) <span class="string">""</span></span><br></pre></td></tr></table></figure><p>牛逼了，删这个 key 花了一分钟，在线上肯定爆炸了。。（其实灌数据的脚本花了更久，这个忽略吧。。）<br>这个数据量是 100m 条数据（简单数据），对于线上还是有参考意义的。</p><p>接下来我们是下 <code>unlink</code> 的性能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-cli --<span class="built_in">eval</span> add-test-big-key.lua big_hash3 , 100000000 &amp;</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; unlink big_hash2</span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line"></span><br><span class="line">$ top</span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                         </span><br><span class="line"> 3512 xhinlia+  20   0 7806468 6.349g   3720 S 102.9 40.8  11:24.49 redis-server</span><br></pre></td></tr></table></figure><p>可以看到， <code>unlink</code> 立刻就返回了，但是 redis-server 还是会消耗很多 CPU。</p><p>接下来我们看一下大 key 自然过期的时候会发生什么事情：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ src/redis-cli --<span class="built_in">eval</span> add-test-big-key.lua big_hash3 , 100000000 </span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; expire big_hash3 5</span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; <span class="built_in">set</span> abc xxx EX 10</span><br><span class="line">（卡住了。。。）</span><br></pre></td></tr></table></figure></p><p>可以看到，自然过期的大 key 也出现了阻塞。</p><h2 id="Conclution"><a href="#Conclution" class="headerlink" title="Conclution"></a>Conclution</h2><p>总结一下：</p><ul><li>尽量从业务上避免 Redis 大 Key，无论从性能角度（hash成本）还是过期删除成本角度，都会比较高</li><li>尽量使用 <code>unlink</code> 代替 <code>del</code> 删除大 key</li><li>key 的自然过期和手动删除，都会阻塞 Redis</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Preview&quot;&gt;&lt;a href=&quot;#Preview&quot; class=&quot;headerlink&quot; title=&quot;Preview&quot;&gt;&lt;/a&gt;Preview&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/uploads/persister-big-key-in-redis-over
      
    
    </summary>
    
      <category term="Backend" scheme="https://xhinliang.github.io/categories/Backend/"/>
    
    
      <category term="Backend" scheme="https://xhinliang.github.io/tags/Backend/"/>
    
      <category term="Cache" scheme="https://xhinliang.github.io/tags/Cache/"/>
    
      <category term="Redis" scheme="https://xhinliang.github.io/tags/Redis/"/>
    
      <category term="Lua" scheme="https://xhinliang.github.io/tags/Lua/"/>
    
      <category term="DevOps" scheme="https://xhinliang.github.io/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>求一个数组的波峰</title>
    <link href="https://xhinliang.github.io/2018/03/algorithm/find-peak-in-array/"/>
    <id>https://xhinliang.github.io/2018/03/algorithm/find-peak-in-array/</id>
    <published>2018-03-27T23:36:29.000Z</published>
    <updated>2026-02-25T07:09:26.606Z</updated>
    
    <content type="html"><![CDATA[<p>三个月前有人问我一道有趣的算法题，当时想了很久才想出来，现在写篇博客记录一下。</p><blockquote><p>给定一个很长的数组 arr，已知数组长度为 length 且 length &gt;= 3，数组的第一个元素不比第二个元素大，最后一个元素不比倒数第二个元素大。求这个数组中 <strong>任意一个</strong> 波峰的数组下标。PS：不比前一个元素小且不比后一个元素小的元素称为波峰。</p></blockquote><p>既然 <code>arr[1] &gt;= arr[0]</code>，那么只要再满足 <code>arr[1] &gt;= arr[2]</code>，下标 1 就是一个波峰。</p><p>如果 <code>arr[1] &lt; arr[2]</code>，那么只要 <code>arr[2] &gt;= arr[3]</code>，下标 2 就是一个波峰。</p><p>……</p><p>这其实可以抽象成一个动态规划问题，但没必要；而且很明显，抽象之后还是得遍历……</p><p>等等……如果除去第一个元素和最后一个元素后，剩下的数组是有序递增的怎么办？这种情况下我们需要扫遍整个数组，从而付出 O(N) 的复杂度。有没有觉得这有点像连环诈骗：指针每次移动到的元素都满足波峰的前一部分条件（不比前一个元素小），我们满心欢喜地期待它满足下半部分条件，但有时并不能如愿。</p><blockquote><p>有没有那么一瞬间想到二分搜索？</p></blockquote><p>确实，这很像二分搜索能解决的问题。但是 <em>数组并没有说明自己有序啊</em>，所以根本不满足二分搜索的基本条件。</p><blockquote><p>死马当活马医试试？</p></blockquote><p>好吧，那我们试一下。先在数组的正中间取一个元素，记为 <code>arr[length/2]</code>。<br>取到之后，我们是不是应该满怀希望地迫不及待地将它和它的左右两个元素进行比较呢？因为这个比较发生在数组上，所以开销可以忽略不计。<br>如果它比左右都大，那很明显它就是一个波峰，此时返回它的下标 <code>length/2</code> 就可以了。<br>那如果它比左边小、比右边小，或者比左右都小呢？<br>那么波峰是不是可以在左边或者右边找一找？又如何确定左边或者右边一定有波峰呢？</p><p>这时，我们应该注意到两个问题。</p><ol><li>如果它比左边小，那么 <code>arr[:length/2+1]</code> 这个子数组也符合题设条件；如果它比右边小，那么 <code>arr[length/2:]</code> 这个子数组也符合题设条件。</li><li>在左边找或者在右边找，一定能找到一个波峰吗？换句话说，符合题设条件的数组是否一定存在至少一个波峰？</li></ol><p>如果把这两个问题想清楚，那么这道题就迎刃而解了。<br>给出示例代码（纯手写……）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_peak</span><span class="params">(arr, length)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> length &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    middle = length / <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> length &lt; <span class="number">4</span>:</span><br><span class="line">        <span class="keyword">return</span> middle </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> arr[middle] &gt; arr[middle - <span class="number">1</span>] <span class="keyword">and</span> arr[middle] &gt; arr[middle + <span class="number">1</span>]:</span><br><span class="line">        <span class="keyword">return</span> middle</span><br><span class="line">    <span class="keyword">if</span> arr[middle] &lt; arr[middle - <span class="number">1</span>]:</span><br><span class="line">        <span class="keyword">return</span> find_peak(arr[:middle + <span class="number">1</span>], middle + <span class="number">1</span>)</span><br><span class="line">    sub_length = middle + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> length % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        sub_length = middle</span><br><span class="line">    <span class="keyword">return</span> find_peak(arr[middle:], sub_length)</span><br></pre></td></tr></table></figure></p><p>这个实现需要考虑很多边界情况，也许我的代码还没覆盖到；如果有问题，欢迎拍砖。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;三个月前有人问我一道有趣的算法题，当时想了很久才想出来，现在写篇博客记录一下。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给定一个很长的数组 arr，已知数组长度为 length 且 length &amp;gt;= 3，数组的第一个元素不比第二个元素大，最后一个元素不比倒数第二个元
      
    
    </summary>
    
      <category term="Algorithm" scheme="https://xhinliang.github.io/categories/Algorithm/"/>
    
    
      <category term="Algorithm" scheme="https://xhinliang.github.io/tags/Algorithm/"/>
    
  </entry>
  
</feed>
